
"co_ai\agents\mixins\prompt_evolver_mixin.py"
---START-OF-FILE---
from co_ai.compiler.prompt_evolver import PromptEvolver

class PromptEvolverMixin:
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.prompt_evolver = None  # Will be initialized on first use

    def init_evolver(self, llm, logger=None):
        self.prompt_evolver = PromptEvolver(llm, logger=logger)

    def evolve_prompts(self, examples: list[dict], context: dict = {}, sample_size: int = 10) -> list[str]:
        return self.prompt_evolver.evolve(examples, context=context, sample_size=sample_size)
---END-OF-FILE---


"co_ai\agents\mixins\proximity_scoring_mixin.py"
---START-OF-FILE---
from co_ai.constants import PROXIMITY
from co_ai.models import ScoreORM
from co_ai.scoring.proximity import ProximityScore


class ProximityScoringMixin:
    """
    A mixin that provides proximity scoring functionality to any agent.
    Can be used in ProximityAgent, MetaReviewAgent, SharpeningAgent, etc.
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.proximity_scorer = None  # Will be initialized on first use

    def get_proximity_scorer(self) -> ProximityScore:
        """
        Lazily initialize the ProximityScore instance.
        """
        if not self.proximity_scorer:
            self.proximity_scorer = ProximityScore(self.cfg, memory=self.memory, logger=self.logger)
        return self.proximity_scorer

    def score_hypothesis_with_proximity(self, hyp: dict, context: dict) -> dict:
        """
        Score a hypothesis using proximity analysis.

        Args:
            hyp (dict): Hypothesis dictionary containing "text" and optionally "id".
            context (dict): Execution context including goal, pipeline_run_id, etc.

        Returns:
            float: The computed proximity score.
        """
        hyp_text = hyp.get("text")
        hyp_id = self.get_hypothesis_id(hyp)

        # Get goal from context
        goal = context.get("goal")

        # Build prompt context for summary generation
        prompt_context = {
            "goal": goal,
            "goal_text": goal.get("goal_text"),
            "hypothesis": hyp_text
        }

        # Load prompt and generate proximity summary
        summary_prompt = self.prompt_loader.load_prompt(self.cfg, prompt_context)
        summary_output = self.call_llm(summary_prompt, prompt_context)

        # Use ProximityScore to evaluate the summary
        scorer = self.get_proximity_scorer()
        score = scorer.get_score(hyp, context)

        # Log the scoring event
        if self.logger:
            self.logger.log("ProximityScoreComputed", {
                "hypothesis_id": hyp_id,
                "score": score,
                "analysis_snippet": summary_output[:300]
            })

        # Save score to DB
        self._save_proximity_score(hyp, context, score)
        return {"id": hyp_id, "score": score, PROXIMITY: summary_output, "scores": scorer.scores}


    def _save_proximity_score(self, hyp: dict, context: dict, score: float):
        """
        Save the proximity score to the database.
        """
        goal = context.get("goal")
        score_obj = ScoreORM(
            agent_name=self.name,
            model_name=self.model_name,
            goal_id=goal.get("id"),
            hypothesis_id=hyp.get("id"),
            score_type="proximity",
            evaluator_name=self.name,
            score=score,
            extra_data={"score": score},
            pipeline_run_id=context.get("pipeline_run_id"),
        )
        self.memory.scores.insert(score_obj)

    def _store_score(self, hypothesis: dict, context: dict, dimension: str, data: dict):
        score_obj = ScoreORM(
            goal_id=hypothesis.get("goal_id"),
            hypothesis_id=hypothesis.get("id"),
            agent_name=self.agent_name,
            model_name=self.model_name,
            evaluator_name="ProximityScore",
            score_type=dimension,
            score=data["score"],
            rationale=data.get("rationale", ""),
            pipeline_run_id=context.get("pipeline_run_id"),
            metadata={"source": "structured_reflection"},
        )
        self.memory.scores.insert(score_obj)

---END-OF-FILE---


"co_ai\agents\mixins\reflection_scoring_mixin.py"
---START-OF-FILE---
from co_ai.constants import GOAL, GOAL_TEXT, HYPOTHESES, REFLECTION
from co_ai.models import ScoreORM, ScoreRuleLinkORM
from co_ai.scoring import ReflectionScore


class ReflectionScoringMixin:
    """
    A mixin that provides reflection and scoring functionality.
    Can be used in agents like ReflectionAgent, MetaReviewAgent, SharpeningAgent, etc.
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.reflection_scorer = None

    def get_reflection_scorer(self):
        """Lazy initialization of reflection scorer"""
        if not self.reflection_scorer:
            self.reflection_scorer = ReflectionScore(
                self.cfg, memory=self.memory, logger=self.logger
            )
        return self.reflection_scorer

    def reflect_on_hypothesis(self, hyp: dict, context: dict) -> str:
        """
        Generate a reflection on a hypothesis using an LLM prompt.
        If reflection already exists, skips regeneration unless forced.
        """
        hyp_text = hyp.get("text")
        hyp_id = self.get_hypothesis_id(hyp)

        if REFLECTION in hyp and hyp[REFLECTION]:
            self.logger.log(
                "ReflectionSkipped",
                {
                    "reason": "already_exists",
                    "hypothesis_id": hyp_id,
                    "reflection_snippet": hyp[REFLECTION][:100],
                },
            )
            return hyp[REFLECTION]

        prompt = self.prompt_loader.load_prompt(
            self.cfg,
            {
                **context,
                **{HYPOTHESES: hyp_text, GOAL: context.get(GOAL).get(GOAL_TEXT)},
            },
        )

        reflection = self.call_llm(prompt, context).strip()
        self.memory.hypotheses.update_reflection(hyp_id, reflection)
        hyp[REFLECTION] = reflection

        self.logger.log(
            "ReflectionGenerated",
            {"hypothesis_id": hyp_id, "reflection_snippet": reflection[:200]},
        )

        return reflection

    def score_hypothesis_with_reflection(self, hyp: dict, context: dict) -> dict:
        """
        Score a hypothesis using its reflection content.
        Also saves the score to the database with optional rule linking.
        """
        hyp_id = self.get_hypothesis_id(hyp)
        goal = context.get(GOAL)

        # Ensure reflection is available
        reflection = self.reflect_on_hypothesis(hyp, context)

        # Score it
        scorer = self.get_reflection_scorer()
        reflection_score = scorer.get_score(hyp, context)

        # Save score
        score_obj = ScoreORM(
            goal_id=self.get_goal_id(goal),
            hypothesis_id=hyp_id,
            agent_name=self.name,
            model_name=self.model_name,
            evaluator_name="ReflectionAgent",
            score_type="self_reward",
            score=reflection_score,
            dimensions=scorer.scores,  
            extra_data=self.cfg,
        )
        score_id = self.memory.scores.insert(score_obj)

        # Link to rules if any
        rule_apps = self.memory.rule_applications.get_for_goal_and_hypothesis(
            goal_id=score_obj.goal_id,
            hypothesis_id=hyp_id,
        )

        for ra in rule_apps:
            link = ScoreRuleLinkORM(score_id=score_id, rule_application_id=ra.id)
            self.memory.session.add(link)

        self.memory.session.commit()

        hyp_text = hyp.get("text")
        self.logger.log(
            "ReflectionScoreStored",
            {
                "hypothesis": hyp_text[:60],
                "score": score_obj.to_dict(),
            },
        )

        return {
            "score": reflection_score,
            REFLECTION: reflection,
            "id": hyp_id,
            "scores": scorer.scores,
        }
---END-OF-FILE---


"co_ai\agents\mixins\scoring_mixin.py"
---START-OF-FILE---
from co_ai.analysis.score_evaluator import ScoreEvaluator

class ScoringMixin:
    """
    A generic scoring mixin that supports dynamic, stage-aware evaluation using ScoreEvaluator.

    Supports any configured scoring stage (e.g., review, reasoning, reflection).
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._scorers = {}  # Caches ScoreEvaluator instances per stage

    def get_scorer(self, stage: str) -> ScoreEvaluator:
        """
        Lazily loads and returns a ScoreEvaluator for the given stage.
        Config path is read from e.g., cfg['review_score_config'].
        """
        if stage not in self._scorers:
            config_key = f"{stage}_score_config"
            config_path = self.cfg.get(config_key, f"config/scoring/{stage}.yaml")
            self._scorers[stage] = ScoreEvaluator.from_file(
                filepath=config_path,
                prompt_loader=self.prompt_loader,
                cfg=self.cfg,
                logger=self.logger,
                memory=self.memory
            )
        return self._scorers[stage]

    def score_hypothesis(self, hypothesis: dict, context: dict, metrics: str = "review") -> dict:
        """
        Score a hypothesis for a given evaluation stage.

        Args:
            hyp (dict): Hypothesis object with a "text" key.
            context (dict): Pipeline context, must include 'goal'.
            metrics (str): Evaluation metrics (e.g., "review", "reasoning", "reflection").

        Returns:
            dict: {
                "id": hypothesis_id,
                "score": float,
                "scores": {dimension_name: {score, rationale, weight}, ...},
                "metrics": metrics
            }
        """
        scorer = self.get_scorer(metrics)
        dimension_scores = scorer.evaluate(
            hypothesis=hypothesis,
            context=context,
            llm_fn=self.call_llm
        )

        weighted_total = sum(
            s["score"] * s.get("weight", 1.0)
            for s in dimension_scores.values()
        )
        weight_sum = sum(s.get("weight", 1.0) for s in dimension_scores.values())
        final_score = round(weighted_total / weight_sum, 2) if weight_sum > 0 else 0.0

        self.logger.log("HypothesisScoreComputed", {
            "score": final_score,
            "dimension_scores": dimension_scores,
            "hypothesis": hypothesis,
            "metrics": metrics
        })

        return {
            "id": hypothesis.get("id"),
            "score": final_score,
            "scores": dimension_scores,
            "metrics": metrics
        }
---END-OF-FILE---


"co_ai\agents\__init__.py"
---START-OF-FILE---
"""
Agents responsible for core reasoning steps:
- base
- generation
- reflection
- ranking
- evolution
- meta review
- proximity
- debate
- literature
- generic
- refiner
"""
from .base import BaseAgent
from .debate import DebateAgent
from .dots_planner import DOTSPlannerAgent
from .evolution import EvolutionAgent
from .generation import GenerationAgent
from .generic import GenericAgent
from .judge import JudgeAgent
from .literature import LiteratureAgent
from .lookahead import LookaheadAgent
from .meta_review import MetaReviewAgent
from .proximity import ProximityAgent
from .ranking import RankingAgent
from .refiner import RefinerAgent
from .reflection import ReflectionAgent
from .sharpening import SharpeningAgent
---END-OF-FILE---


"co_ai\agents\adaptive_reasoner.py"
---START-OF-FILE---
from typing import Union

from co_ai.agents import BaseAgent
from co_ai.constants import GOAL
from co_ai.dataloaders import ARMDataLoader
from co_ai.evaluator import ARMReasoningSelfEvaluator, LLMJudgeEvaluator


class AdaptiveReasonerAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

        self.modes = ["adaptive", "instruction_guided", "consensus_guided"]
        self.mode = self.cfg.get("mode", "adaptive")
        self.format_list = self.cfg.get(
            "format_list", ["direct", "short_cot", "code", "long_cot"]
        )
        self.judge = self._init_judge()

    async def run(self, context: dict):
        goal = context.get(GOAL) 


        self.judge.train_from_database(goal.get("goal_text"), self.cfg)

        prompt = goal.get("goal_text")

        response = ""
        if self.mode == "instruction_guided":
            format_name = self.cfg.get("format", "long_cot")
            response = self._generate_with_format(format_name, context)
        elif self.mode == "consensus_guided":
            response = self._run_consensus_mode(context)
        else:  # default to adaptive
            response = self._run_adaptive_mode(prompt, context)

        self.logger.log("AdaptiveReasoningResponse", response)

        context[self.output_key] = response
        return context

    def _generate_with_format(self, fmt, context):
        prompt = self.prompt_loader.from_file(fmt, self.cfg, context)
        response = self.call_llm(prompt, context)
        return {
            "prompt": prompt,
            "response": response,
            "format_used": ARMDataLoader.detect_format(response) or fmt,
        }

    def _run_consensus_mode(self, context:dict):
        outputs = {}
        for fmt in ["direct", "short_cot", "code"]:
            outputs[fmt] = self._generate_with_format(fmt, context)["response"]

        responses = list(outputs.values())
        unique_responses = set(responses)

        if len(unique_responses) == 1:
            return {
                "response": responses[0],
                "format": "consensus-simple",
                "source_formats": list(outputs.keys()),
            }
        else:
            long_cot_response = self._generate_with_format("long_cot", context)
            return {
                "response": long_cot_response["response"],
                "format": "long_cot",
                "source_formats": list(outputs.keys()),
                "fallback_reason": "no_consensus",
            }

    def _run_adaptive_mode(self, prompt:str, context:dict) -> dict[str, Union[str, float]]:
        prioritized_formats = ["direct", "short_cot", "code", "long_cot"]

        scores = {}
        for fmt in prioritized_formats:
            dict_response = self._generate_with_format(fmt, context)
            response = dict_response["response"]
            base_score = self.judge.score(prompt, response)

            token_len = len(response.split())
            rarity_bonus = 1.0 / (1 + self.judge.format_freq.get(fmt, 0))

            final_score = base_score - 0.01 * token_len + rarity_bonus
            scores[fmt] = final_score
            self.judge._update_format_stats(fmt, final_score)

        best_format = max(scores, key=scores.get)
        chosen_response = self._generate_with_format(best_format, context)
        # Log decision
        self.logger.log(
            "AdaptiveModeDecision",
            {"goal": prompt, "scores": scores, "chosen": best_format},
        )

        return {
            "response": chosen_response,
            "format_used": best_format,
            "scores": scores,
        }

    def get_format_for_goal(self, goal: dict):
        if "preferred_format" in goal:
            return goal["preferred_format"]
        goal_type = goal.get("goal_type", "default")
        if goal_type == "math":
            return "code"
        elif goal_type == "commonsense":
            return "short_cot"
        else:
            return "long_cot"

    def _get_prioritized_formats(self, context:dict):
        if "preferred_format" in context:
            return [context["preferred_format"]]

        priority_map = self.cfg.get("format_priority_by_difficulty", {})
        difficulty = context.get("difficulty", "default").lower()
        return priority_map.get(difficulty, priority_map.get("default", ["long_cot"]))

    def _init_judge(self):
        judge_strategy = self.cfg.get("judge", "mrq")
        if judge_strategy == "llm":
            llm = self.cfg.get("judge_model", self.cfg.get("model"))
            prompt_file = self.cfg.get(
                "judge_prompt_file", "judge_pairwise_comparison.txt"
            )
            self.logger.log(
                "EvaluatorInit", {"strategy": "LLM", "prompt_file": prompt_file}
            )
            return LLMJudgeEvaluator(
                self.cfg, llm, prompt_file, self.call_llm, self.logger
            )
        else:
            self.logger.log("EvaluatorInit", {"strategy": "ARM"})
            return ARMReasoningSelfEvaluator(self.cfg, self.memory, self.logger)

---END-OF-FILE---


"co_ai\agents\auto_tuner.py"
---START-OF-FILE---
from datetime import datetime
from co_ai.agents.base import BaseAgent
from co_ai.models import SymbolicRuleORM
from co_ai.analysis.rule_effect_analyzer import RuleEffectAnalyzer
from co_ai.constants import PIPELINE_RUN_ID


class AutoTunerAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.eval_threshold = cfg.get("eval_threshold", 50)
        self.max_rules = cfg.get("max_rules_to_consider", 10)
        self.tune_mode = cfg.get("tune_mode", False)  # Only write changes if True

    async def run(self, context: dict) -> dict:
        self.logger.log("AutoTunerStart", {"tune_mode": self.tune_mode})

        analyzer = RuleEffectAnalyzer(self.memory.session, logger=self.logger)
        summary = analyzer.analyze(context.get(PIPELINE_RUN_ID))

        underperforming_rules = [
            (rule_id, data) for rule_id, data in summary.items()
            if data.get("avg_score", 100) < self.eval_threshold
        ]
        underperforming_rules = sorted(underperforming_rules, key=lambda x: x[1]["avg_score"])[:self.max_rules]

        for rule_id, data in underperforming_rules:
            rule = self.memory.symbolic_rules.get_by_id(rule_id)
            if not rule:
                continue

            self.logger.log("RuleUnderperforming", {
                "rule_id": rule_id,
                "avg_score": data["avg_score"],
                "context_hash": rule.context_hash,
                "attributes": rule.attributes
            })

            suggestions = self.suggest_rule_edits(rule, data)
            for suggestion in suggestions:
                self.logger.log("AutoRuleSuggestion", {
                    "rule_id": rule_id,
                    "suggested_attributes": suggestion,
                    "reason": "AutoTuner based on score analysis"
                })

                if self.tune_mode:
                    new_rule = SymbolicRuleORM(
                        target=rule.target,
                        filter=rule.filter,
                        attributes=suggestion,
                        source="auto_tuner",
                        created_at=datetime.utcnow(),
                        context_hash=SymbolicRuleORM.compute_context_hash(suggestion, rule.filter),
                        description=f"Auto-tuned from rule {rule_id}"
                    )
                    self.memory.symbolic_rules.insert(new_rule)
                    self.logger.log("AutoRuleInserted", {"new_rule_id": new_rule.id})

        self.logger.log("AutoTunerEnd", {"rules_checked": len(underperforming_rules)})
        return context

    def suggest_rule_edits(self, rule: SymbolicRuleORM, data: dict) -> list[dict]:
        """
        Heuristic placeholder: try tweaking `temperature`, `max_tokens`, `adapter`, etc.
        Could be replaced by LLM-based or learned tuner later.
        """
        original = rule.attributes or {}

        candidates = []

        if "temperature" in original:
            try:
                temp = float(original["temperature"])
                new_temp = round(min(temp + 0.2, 1.0), 2)
                candidates.append({**original, "temperature": new_temp})
            except:
                pass

        if "adapter" in original:
            candidates.append({**original, "adapter": "default"})

        if "max_tokens" in original:
            try:
                new_tokens = max(int(original["max_tokens"]) - 100, 100)
                candidates.append({**original, "max_tokens": new_tokens})
            except:
                pass

        # Default fallback: add a hint flag
        candidates.append({**original, "hint": "reviewed by tuner"})

        return candidates
---END-OF-FILE---


"co_ai\agents\base.py"
---START-OF-FILE---
# co_ai/agents/base.py
import random
import re
from abc import ABC, abstractmethod
from datetime import datetime, timezone

import litellm

from co_ai.constants import (
    AGENT,
    API_BASE,
    API_KEY,
    BATCH_SIZE,
    CONTEXT,
    GOAL,
    HYPOTHESES,
    INPUT_KEY,
    MODEL,
    NAME,
    OUTPUT_KEY,
    PROMPT_MATCH_RE,
    PROMPT_PATH,
    SAVE_CONTEXT,
    SAVE_PROMPT,
    SOURCE,
    STRATEGY,
)
from co_ai.logs import JSONLogger
from co_ai.prompts import PromptLoader
from co_ai.rules import SymbolicRuleApplier


def remove_think_blocks(text: str) -> str:
    return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()


class BaseAgent(ABC):
    def __init__(self, cfg, memory=None, logger=None):
        self.cfg = cfg
        agent_key = self.__class__.__name__.replace(AGENT, "").lower()
        self.name = cfg.get(NAME, agent_key)
        self.memory = memory
        self.logger = logger or JSONLogger()
        self.rule_applier = SymbolicRuleApplier(cfg, memory, logger)
        self.model_config = cfg.get(MODEL, {})
        self.prompt_loader = PromptLoader(memory=self.memory, logger=self.logger)
        self.prompt_match_re = cfg.get(PROMPT_MATCH_RE, "")
        self.llm = self.init_llm()  # TODO do we need to init here?
        self.strategy = cfg.get(STRATEGY, "default")
        self.model_name = self.llm.get(NAME, "")
        self.source = cfg.get(SOURCE, CONTEXT)
        self.batch_size = cfg.get(BATCH_SIZE, 6)
        self.save_context = cfg.get(SAVE_CONTEXT, False)
        self.input_key = cfg.get(INPUT_KEY, HYPOTHESES)
        self.preferences = cfg.get("preferences", {})
        self.remove_think = cfg.get("remove_think", True)
        self.output_key = cfg.get(OUTPUT_KEY, self.name)
        self._goal_id_cache = {}
        self._prompt_id_cache = {}
        self._hypothesis_id_cache = {}
        self.logger.log(
            "AgentInitialized",
            {
                "agent_key": agent_key,
                "class": self.__class__.__name__,
                "config": self.cfg,
            },
        )

    def init_llm(self, cfg=None):
        config = cfg or self.cfg
        model_cfg = config.get(MODEL, {})
        required_keys = [NAME, API_BASE]
        for key in required_keys:
            if key not in model_cfg:
                self.logger.log(
                    "MissingLLMConfig", {"agent": self.name, "missing_key": key}
                )
        return {
            NAME: model_cfg.get(NAME),
            API_BASE: model_cfg.get(API_BASE),
            API_KEY: model_cfg.get(API_KEY),
        }

    def call_llm(self, prompt: str, context: dict, llm_cfg: dict = None) -> str:
        updated_cfg = self.rule_applier.apply_to_prompt(self.cfg, context)
        if self.llm is None:
            # ðŸ” Apply rules here (now that goal is known)
            updated_cfg = self.rule_applier.apply_to_agent(self.cfg, context)
            self.llm = self.init_llm(cfg=updated_cfg)  # initialize with updated config

        """Call the default or custom LLM, log the prompt, and handle output."""
        props = llm_cfg or self.llm  # Use passed-in config or default

        agent_name = self.name

        strategy = updated_cfg.get(STRATEGY, "")
        prompt_key = updated_cfg.get(PROMPT_PATH, "")
        use_memory_for_fast_prompts = updated_cfg.get(
            "use_memory_for_fast_prompts", True
        )

        # ðŸ” Check cache
        if self.memory and use_memory_for_fast_prompts:
            previous = self.memory.prompt.find_matching(
                agent_name=agent_name, prompt_text=prompt, strategy=strategy
            )
            if previous:
                chosen = random.choice(previous)
                cached_response = chosen.get("response_text")
                self.logger.log(
                    "LLMCacheHit",
                    {
                        "agent": agent_name,
                        "strategy": strategy,
                        "prompt_key": prompt_key,
                        "cached": True,
                        "count": len(previous),
                        "emoji": "ðŸ“¦ðŸ”ðŸ’¬",
                    },
                )
                return cached_response

        messages = [{"role": "user", "content": prompt}]
        try:
            response = litellm.completion(
                model=props[NAME],
                messages=messages,
                api_base=props[API_BASE],
                api_key=props.get(API_KEY, ""),
            )
            output = response["choices"][0]["message"]["content"]

            # Save prompt and response if enabled
            if updated_cfg.get(SAVE_PROMPT, False) and self.memory:
                self.memory.prompt.save(
                    context.get("goal"),
                    agent_name=self.name,
                    prompt_key=updated_cfg.get(PROMPT_PATH, ""),
                    prompt_text=prompt,
                    response=output,
                    strategy=updated_cfg.get(STRATEGY, ""),
                    pipeline_run_id=context.get("pipeline_run_id"),
                    version=updated_cfg.get("version", 1),
                )

            # Remove [THINK] blocks if configured
            response_cleaned = (
                remove_think_blocks(output) if self.remove_think else output
            )

            # Optionally add to context history
            if updated_cfg.get("add_prompt_to_history", True):
                self.add_to_prompt_history(
                    context, prompt, {"response": response_cleaned}
                )

            return response_cleaned

        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            self.logger.log("LLMCallError", {"exception": str(e)})
            raise

    @abstractmethod
    async def run(self, context: dict) -> dict:
        pass

    def add_to_prompt_history(self, context: dict, prompt: str, metadata: dict = None):
        """
        Appends a prompt record to the context['prompt_history'] under the agent's name.

        Args:
            context (dict): The context dict to modify
            prompt (str): prompt to store
            metadata (dict): any extra info
        """
        if "prompt_history" not in context:
            context["prompt_history"] = {}
        if self.name not in context["prompt_history"]:
            context["prompt_history"][self.name] = []
        entry = {
            "prompt": prompt,
            "agent": self.name,
            "preferences": self.preferences,
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }
        if metadata:
            entry.update(metadata)
        context["prompt_history"][self.name].append(entry)

    def get_hypotheses(self, context: dict) -> list[dict]:
        try:
            if self.source == "context":
                hypothesis_dicts = context.get(self.input_key, [])
                if not hypothesis_dicts:
                    self.logger.log("NoHypothesesInContext", {"agent": self.name})
                return hypothesis_dicts

            elif self.source == "database":
                goal = context.get(GOAL)
                hypotheses = self.get_hypotheses_from_db(goal.get("goal_text"))
                if not hypotheses:
                    self.logger.log(
                        "NoHypothesesInDatabase", {"agent": self.name, "goal": goal}
                    )
                return [h.to_dict() for h in hypotheses] if hypotheses else []

            else:
                self.logger.log(
                    "InvalidSourceConfig", {"agent": self.name, "source": self.source}
                )
        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            self.logger.log(
                "HypothesisFetchError",
                {"agent": self.name, "source": self.source, "error": str(e)},
            )

        return []

    def get_hypotheses_from_db(self, goal_text: str):
        return self.memory.hypotheses.get_latest(goal_text, self.batch_size)

    @staticmethod
    def extract_goal_text(goal):
        return goal.get("goal_text") if isinstance(goal, dict) else goal

    def get_goal_id(self, goal: dict):
        if not isinstance(goal, dict):
            raise ValueError(
                f"Expected goal to be a dict, got {type(goal).__name__}: {goal}"
            )
        goal_text = goal.get("goal_text", "")
        if goal_text in self._goal_id_cache:
            return self._goal_id_cache[goal_text][0]
        goal = self.memory.goals.get_from_text(goal_text)
        self._goal_id_cache[goal_text] = (goal.id, goal)
        return goal.id

    def get_prompt_id(self, prompt_text: str):
        if prompt_text in self._prompt_id_cache:
            return self._prompt_id_cache[prompt_text][0]
        prompt = self.memory.prompt.get_from_text(prompt_text)
        self._prompt_id_cache[prompt_text] = (prompt.id, prompt)
        return prompt.id

    def get_hypothesis_id(self, hypothesis_dict: dict):
        if not isinstance(hypothesis_dict, dict):
            raise ValueError(
                f"Expected hypothesis_text to be a dict, got {type(hypothesis_dict).__name__}: {hypothesis_dict}"
            )
        text = hypothesis_dict.get("text")
        if text in self._hypothesis_id_cache:
            return self._hypothesis_id_cache[text][0]
        hypothesis = self.memory.hypotheses.get_from_text(text)
        self._hypothesis_id_cache[text] = (hypothesis.id, hypothesis)
        return hypothesis.id
---END-OF-FILE---


"co_ai\agents\compiler_optimizer.py"
---START-OF-FILE---
from co_ai.agents.base import BaseAgent
from co_ai.constants import PIPELINE_RUN_ID, GOAL
from collections import defaultdict
import statistics

class CompilerOptimizerAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.score_threshold = cfg.get("score_threshold", 5.0)  # optional tuning param

    async def run(self, context: dict) -> dict:
        pipeline_run_id = context.get(PIPELINE_RUN_ID)
        goal = context.get(GOAL)

        # Step 1: Fetch all hypotheses and their scores for this run
        hypotheses = self.memory.hypotheses.get_by_pipeline_run(pipeline_run_id)
        all_scores = self.memory.scores.get_by_pipeline_run(pipeline_run_id)

        # Group scores by prompt_id
        prompt_scores = defaultdict(list)
        for score in all_scores:
            if score.prompt_id:
                prompt_scores[score.prompt_id].append(score)

        summary = []

        # Step 2: Analyze performance per prompt
        for prompt_id, scores in prompt_scores.items():
            prompt = self.memory.prompt.get(prompt_id)
            raw_scores = [s.score for s in scores if s.score is not None]

            if not raw_scores:
                continue

            avg = statistics.mean(raw_scores)
            std = statistics.stdev(raw_scores) if len(raw_scores) > 1 else 0
            count = len(raw_scores)
            high_score_rate = sum(s >= self.score_threshold for s in raw_scores) / count

            summary.append({
                "prompt_id": prompt_id,
                "prompt_text": prompt.prompt_text[:100] if prompt else "<unknown>",
                "avg_score": avg,
                "std_dev": std,
                "count": count,
                "high_score_rate": round(high_score_rate * 100, 2)
            })

        # Step 3: Log or save insights
        top_prompts = sorted(summary, key=lambda x: x["avg_score"], reverse=True)[:5]
        self.logger.log("CompilerOptimizerSummary", {
            "goal": goal.get("goal_text"),
            "top_prompts": top_prompts,
            "pipeline_run_id": pipeline_run_id
        })

        print("\n=== Top Performing Compiled Prompts ===")
        for i, p in enumerate(top_prompts):
            print(f"[{i+1}] Avg Score: {p['avg_score']:.2f} | Used {p['count']}x | Prompt: {p['prompt_text']}")

        # Step 4 (future): Update strategy weights, rules, DSPy prior preferences

        context["compiler_optimization_summary"] = summary
        return context
---END-OF-FILE---


"co_ai\agents\cot_dspy_generator.py"
---START-OF-FILE---
from abc import ABC, abstractmethod

import dspy
from dspy import InputField, OutputField, Signature

from co_ai.agents.base import BaseAgent
from co_ai.constants import (GOAL, GOAL_TEXT, PIPELINE, PIPELINE_RUN_ID,
                             PROMPT_PATH, STRATEGY)
from co_ai.models import HypothesisORM


# DSPy signature for generating Chains of Thought
class CoTGenerationSignature(Signature):
    question = InputField(desc="A scientific or reasoning question")
    references = InputField(desc="Optional reference material to inform the reasoning")
    preferences = InputField(desc="Optional reasoning preferences or style constraints")
    answer = OutputField(desc="Chain-of-thought reasoning that addresses the question")


class CoTGeneratorModule(dspy.Module):
    def __init__(self):
        super().__init__()
        self.generator = dspy.Predict(CoTGenerationSignature)

    def forward(self, question, references="", preferences=""):
        return self.generator(
            question=question, references=references, preferences=preferences
        )


# Simple evaluation result class to return from evaluator
class EvaluationResult:
    def __init__(self, score: float, reason: str):
        self.score = score
        self.reason = reason


# Base evaluator interface (not used directly, but useful for future extensions)
class BaseEvaluator(ABC):
    @abstractmethod
    def evaluate(
        self, original: str, proposal: str, metadata: dict = None
    ) -> EvaluationResult:
        pass


# Main agent class responsible for training and tuning prompts using DSPy
class ChainOfThoughtDSPyGeneratorAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

        # Setup DSPy
        lm = dspy.LM(
            "ollama_chat/qwen3",
            api_base="http://localhost:11434",
            api_key="",
        )
        dspy.configure(lm=lm)

        self.module = CoTGeneratorModule()

    async def run(self, context: dict):
        goal = context.get(GOAL)
        references = context.get("references", "")
        preferences = context.get("preferences", "")

        result = self.module(
            question=goal.get("goal_text"), references=references, preferences=preferences
        )

        cot = result.answer.strip()
        self.logger.log("CoTGenerated", {"goal": goal, "cot": cot})

        prompt_text = goal.get(GOAL_TEXT)
        prompt = self.memory.prompt.get_from_text(prompt_text)
        if prompt is None:
            self.memory.prompt.save(
                context.get("goal"),
                agent_name=self.name,
                prompt_key=self.cfg.get(PROMPT_PATH, ""),
                prompt_text=prompt,
                strategy=self.cfg.get(STRATEGY, ""),
                pipeline_run_id=context.get("pipeline_run_id"),
                version=self.cfg.get("version", 1),
            )
            prompt = self.memory.prompt.get_from_text(prompt_text)

        goal_id = self.get_goal_id(goal)
        hyp = HypothesisORM(
            goal_id=goal_id,
            prompt_id=prompt.id,
            text=cot,
            features={"source": "cot_dspy"},
            pipeline_signature=context.get(PIPELINE),
            pipeline_run_id=context.get(PIPELINE_RUN_ID),
        )
        self.memory.hypotheses.insert(hyp)

        context[self.output_key] = [cot]
        return context
---END-OF-FILE---


"co_ai\agents\cot_generator.py"
---START-OF-FILE---
from co_ai.agents import BaseAgent
from co_ai.analysis.rubric_classifier import RubricClassifierMixin
from co_ai.constants import GOAL, PIPELINE, PIPELINE_RUN_ID
from co_ai.evaluator.llm_judge_evaluator import LLMJudgeEvaluator
from co_ai.evaluator.mrq_self_evaluator import MRQSelfEvaluator
from co_ai.models import HypothesisORM


class ChainOfThoughtGeneratorAgent(BaseAgent, RubricClassifierMixin): 
    def __init__(self, cfg, memory, logger):
        super().__init__(cfg, memory, logger)
        self.logger.log("AgentInit", {"agent": "ChainOfThoughtGeneratorAgent"})
        self.evaluator = self._init_evaluator()
        self.num_candidates = cfg.get("num_candidates", 2)

    async def run(self, context: dict):
        goal = context.get(GOAL)
        self.logger.log("AgentRunStarted", {"goal": goal})

        if isinstance(self.evaluator, MRQSelfEvaluator):
            self.logger.log("MRQTraining", {"type": "MRQ"})
            self.evaluator.train_from_database(goal=goal.goal_text, cfg=self.cfg)

        prompt = self.prompt_loader.load_prompt(self.cfg, context)
        self.logger.log("PromptGenerated", {"prompt": prompt[:200]})

        # Step 1: Generate candidates
        self.logger.log("GenerationStarted", {"num_candidates": self.num_candidates})
        candidates = [
            self.call_llm(prompt, context) for _ in range(self.num_candidates)
        ]
        self.logger.log(
            "GenerationCompleted", {"candidates": [c[:100] for c in candidates]}
        )

        # Step 2: Evaluate pairwise
        best = candidates[0]
        scores = {}
        for candidate in candidates[1:]:
            best, scores = self.evaluator.judge(
                prompt=prompt,
                output_a=best,
                output_b=candidate, 
                context=context
            )
        self.logger.log("EvaluationCompleted", {"best_output": best[:100], **scores})

        # Step 3: Store hypothesis and patterns
        value_a = scores.get("value_a", 0)
        value_b = scores.get("value_b", 0)
        score = max(value_a, value_b)
        features = {
            "prompt": prompt,
            "best_output": best,
            "candidates": candidates,
        }

        best_orm = HypothesisORM(
            goal_id=self.get_goal_id(goal),
            text=best,
            confidence=score,
            features=features,
            prompt_id=self.get_prompt_id(prompt),
            pipeline_signature=context.get(PIPELINE),
            pipeline_run_id=context.get(PIPELINE_RUN_ID),
        )
        self.memory.hypotheses.insert(best_orm)
        self.logger.log("HypothesisStored", {"text": best[:100], "confidence": score})

        self.classify_and_store_patterns(
            hypothesis=best_orm.to_dict(),
            context=context,
            prompt_loader=self.prompt_loader,
            cfg=self.cfg,
            memory=self.memory,
            logger=self.logger,
            agent_name=self.name,
            score=score,
        )

        context[self.output_key] = [best_orm.to_dict()]
        self.logger.log("AgentRunCompleted", {"output_key": self.output_key})
        return context

    def _init_evaluator(self):
        if self.cfg.get("evaluator", "mrq") == "llm":
            llm = self.cfg.get("evaluator_model", self.cfg.get("model"))
            prompt_file = self.cfg.get("evaluator_prompt_file", "evaluation.txt")
            self.logger.log(
                "EvaluatorInit", {"strategy": "LLM", "prompt_file": prompt_file}
            )
            return LLMJudgeEvaluator(
                self.cfg, llm, prompt_file, self.call_llm, self.logger
            )
        else:
            self.logger.log("EvaluatorInit", {"strategy": "MRQ"})
            return MRQSelfEvaluator(self.memory, self.logger)

    def _summarize_pattern(self, pattern: dict):
        stats = {}
        for dimension, label in pattern.items():
            if label not in stats:
                stats[label] = 0
            stats[label] += 1
        return stats
---END-OF-FILE---


"co_ai\agents\debate.py"
---START-OF-FILE---
# co_ai/agents/debate.py
from co_ai.agents.base import BaseAgent


class OptimistDebater(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

    async def run(self, context: dict) -> dict:
        hypotheses = context.get("hypotheses", [])
        reviews = []

        for h in hypotheses:
            prompt = (
                f"As an optimistic analyst, critique the following hypothesis:\n\n"
                f"{h}\n\n"
                f"Focus on strengths, positive implications, and reasons it might be valid."
            )
            review = self.call_llm(prompt, context)
            reviews.append({"hypotheses": h, "review": review, "persona": "Optimist"})

        return {"reviews": reviews}

class SkepticDebater(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

    async def run(self, context: dict) -> dict:
        hypotheses = context.get("hypotheses", [])
        reviews = []

        for h in hypotheses:
            prompt = (
                f"As a skeptical analyst, critique the following hypothesis:\n\n"
                f"{h}\n\n"
                f"Focus on weaknesses, uncertainties, or reasons it might be flawed."
            )
            review = self.call_llm(prompt, context)
            reviews.append({"hypotheses": h, "review": review, "persona": "Skeptic"})

        return {"reviews": reviews}

class BalancedDebater(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

    async def run(self, context: dict) -> dict:
        hypotheses = context.get("hypotheses", [])
        reviews = []

        for h in hypotheses:
            prompt = (
                f"As a balanced analyst, critique the following hypothesis:\n\n"
                f"{h}\n\n"
                f"Provide both positive and negative aspects."
            )
            review = self.call_llm(prompt, context)
            reviews.append({"hypotheses": h, "review": review, "persona": "Balanced"})

        return {"reviews": reviews}
    
class DebateAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.optimist = OptimistDebater(cfg, memory, logger)
        self.skeptic = SkepticDebater(cfg, memory, logger)
        self.balanced = BalancedDebater(cfg, memory, logger)

    async def run(self, context: dict) -> dict:
        hypotheses = context.get("hypotheses", [])
        optimist_reviews = await self.optimist.run({"hypotheses": hypotheses})
        skeptic_reviews = await self.skeptic.run({"hypotheses": hypotheses})
        balanced_reviews = await self.balanced.run({"hypotheses": hypotheses})

        return {
            "optimist_reviews": optimist_reviews,
            "skeptic_reviews": skeptic_reviews,
            "balanced_reviews": balanced_reviews
        }
    
---END-OF-FILE---


"co_ai\agents\dots_planner.py"
---START-OF-FILE---
from co_ai.agents.base import BaseAgent
from co_ai.constants import GOAL, STRATEGY
from co_ai.utils.goal_classifier import classify_goal_strategy  # See below


class DOTSPlannerAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.strategy_map = cfg.get("strategy_routes", {})
        self.default_strategy = cfg.get("default_strategy", "default")

    async def run(self, context: dict) -> dict:
        goal = context.get(GOAL)
        strategy = classify_goal_strategy(goal)

        pipeline = self.strategy_map.get(strategy, self.strategy_map[self.default_strategy])

        context["strategy"] = strategy
        context["suggested_pipeline"] = pipeline

        self.logger.log("DOTSPlanGenerated", {
            "strategy": strategy,
            "pipeline": pipeline
        })

        return context
---END-OF-FILE---


"co_ai\agents\evolution.py"
---START-OF-FILE---
# co_ai/agents/evolution.py
import itertools
import re

import numpy as np

from co_ai.agents.base import BaseAgent
from co_ai.constants import (EVOLVED, GOAL, HYPOTHESES, PIPELINE,
                             PIPELINE_RUN_ID, RANKING)
from co_ai.models import HypothesisORM
from co_ai.tools.embedding_tool import get_embedding


class EvolutionAgent(BaseAgent):
    """
    The Evolution Agent refines hypotheses iteratively using several strategies:

    - Grafting similar hypotheses into unified statements
    - Feasibility improvement through LLM reasoning
    - Out-of-the-box hypothesis generation
    - Inspiration from top-ranked ideas
    - Simplification and clarity enhancement

    These improvements are based on the paper:
    "The Evolution agent continuously refines and improves existing hypotheses..."
    """

    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.use_grafting = cfg.get("use_grafting", False)
        self.preferences = cfg.get("preferences", ["novelty", "feasibility"])

    async def run(self, context: dict) -> dict:
        """
        Evolve top-ranked hypotheses individually.

        Args:
            context: Dictionary with keys:
                - ranked: list of (hypotheses, score) tuples
                - hypotheses: list of unranked hypotheses (fallback)
                - preferences: override criteria for refinement
        """
        ranked = context.get(RANKING, [])
        fallback_hypotheses = context.get(HYPOTHESES, [])
        preferences = context.get("preferences", self.preferences)

        # Decide which hypotheses to evolve
        if ranked:
            top_texts = [hyp for hyp, _ in ranked[:3]]
        elif fallback_hypotheses:
            top_texts = fallback_hypotheses
        else:
            self.logger.log(
                "NoHypothesesToEvolve", {"reason": "no_ranked_or_unranked_input"}
            )
            context[EVOLVED] = []
            return context

        evolved = []
        for h in top_texts:
            try:
                prompt = self.prompt_loader.load_prompt(
                    {**self.cfg, **{HYPOTHESES: h}}, context
                )
                raw_output = self.call_llm(prompt, context)
                refined_list = self.extract_hypothesis(raw_output)
                self.logger.log(
                    "EvolvedParsedHypotheses",
                    {"raw_response_snippet": raw_output[:300], "parsed": refined_list},
                )

                if refined_list:
                    for r in refined_list:
                        goal = context.get(GOAL)
                        hyp = HypothesisORM(
                            goal=goal,
                            text=h,
                            pipeline_signature=context.get(PIPELINE),
                            pipeline_run_id=context.get(PIPELINE_RUN_ID),
                        )
                        self.memory.hypotheses.insert(hyp)
                        evolved.append(r)
                else:
                    self.logger.log(
                        "EvolutionFailed",
                        {"original": h[:100], "response_snippet": raw_output[:200]},
                    )

            except Exception as e:
                print(f"âŒ Exception: {type(e).__name__}: {e}")
                self.logger.log(
                    "EvolutionError", {"error": str(e), "hypotheses": h}
                )

        context["evolved"] = evolved
        self.logger.log(
            "EvolutionCompleted",
            {"evolved_count": len(evolved), "preferences": preferences},
        )

        return context

    async def graft_similar(self, context: dict, threshold: float = 0.90) -> list[str]:
        """
        Graft pairs of highly similar hypotheses into unified versions.
        """
        hypotheses = self.get_hypotheses(context)
        # TODO: use memory
        embeddings = [get_embedding(h, self.cfg) for h in hypotheses]
        used = set()
        grafted = []

        for (i, h1), (j, h2) in itertools.combinations(enumerate(hypotheses), 2):
            if i in used or j in used:
                continue
            sim = self.cosine_similarity(embeddings[i], embeddings[j])
            if sim >= threshold:
                self.logger.log(
                    "GraftingPair",
                    {"similarity": sim, "h1": h1[:60] + "...", "h2": h2[:60] + "..."},
                )
                prompt = (
                    f"Combine the following hypotheses into a clearer, unified statement:\n\n"
                    f"A: {h1}\nB: {h2}"
                )
                graft = self.call_llm(prompt, context)
                grafted.append(graft)
                used.update([i, j])

        # Add ungrafted hypotheses back
        hypotheses = context.get(HYPOTHESES, [])
        for i, h in enumerate(hypotheses):
            if i not in used:
                grafted.append(h)

        return grafted

    def cosine_similarity(self, vec1, vec2):
        """Compute cosine similarity between two vectors."""
        v1 = np.array(vec1)
        v2 = np.array(vec2)
        return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))

    def extract_hypothesis(self, text: str) -> list[str]:
        """
        Extracts hypothesis from markdown-style output with '**Hypothesis:**' section.
        Returns a list of simplified hypotheses.
        """
        pattern = re.compile(r"\*\*Hypothesis:\*\*\s*(.*?)\n", re.IGNORECASE)
        matches = pattern.findall(text)

        if len(matches) == 0:
            return [text]
        return [match.strip() for match in matches if match.strip()]
---END-OF-FILE---


"co_ai\agents\general_reasoner.py"
---START-OF-FILE---
from itertools import combinations
from typing import Optional

from co_ai.agents.base import BaseAgent
from co_ai.analysis.rubric_classifier import RubricClassifierMixin
from co_ai.constants import GOAL, GOAL_TEXT, PIPELINE, PIPELINE_RUN_ID
from co_ai.evaluator import LLMJudgeEvaluator, MRQSelfEvaluator
from co_ai.models import HypothesisORM, ScoreORM
from co_ai.prompts import PromptLoader
from co_ai.agents.mixins.scoring_mixin import ScoringMixin

class GeneralReasonerAgent(ScoringMixin, RubricClassifierMixin, BaseAgent):
    def __init__(self, cfg, memory, logger):
        super().__init__(cfg, memory, logger)

    async def run(self, context: dict) -> dict:
        goal = context.get(GOAL)

        self.logger.log("AgentRunStarted", {"goal": goal})

        # Generate hypotheses (if needed)
        if self.cfg.get("thinking_mode") == "generate_and_judge":
            hypotheses = self.generate_hypotheses(context)
        else:
            hypotheses = self.get_hypotheses(context)

        context["hypotheses"] = hypotheses
        context["scoring"] = []

        dimension_scores = []
        for hyp in hypotheses:
            scored = self.score_hypothesis(hyp, context, metrics="reasoning_cor")
            hyp["final_score"] = scored["score"]
            hyp["dimension_scores"] = scored["scores"]
            dimension_scores.append(scored)
        context["dimension_scores"] = dimension_scores


        best_hypothesis = max(hypotheses, key=lambda h: h["final_score"])
        
        # Classify with rubrics and store pattern stats
        pattern = self.classify_with_rubrics(
            hypothesis=best_hypothesis,
            context=context,
            prompt_loader=self.prompt_loader,
            cfg=self.cfg,
            logger=self.logger
        )


        summarized = self._summarize_pattern(pattern)
        context["pattern"] = summarized

        pattern_stats = self.generate_pattern_stats(
            goal=goal,
            hypothesis=best_hypothesis,
            pattern_dict=summarized,
            cfg=self.cfg,
            agent_name=self.name,
            confidence_score=best_hypothesis.get("confidence")
        )

        self.memory.pattern_stats.insert(pattern_stats)
        context["pattern_stats"] = summarized

        context[self.output_key] = best_hypothesis
        context["ranked_hypotheses"] = sorted(hypotheses, key=lambda h: h["final_score"], reverse=True)

        return context

    def generate_hypotheses(self, context: dict) -> list[dict]:
        """Generates multiple hypotheses using different strategies"""
        goal = context.get(GOAL)
        question = goal.get(GOAL_TEXT)

        strategies = self.cfg.get("generation_strategy_list", ["cot"])
        merged = {**context, "question": question}

        hypotheses = []
        goal_id = self.get_goal_id(goal)
        for strategy in strategies:
            prompt = self.prompt_loader.from_file(
                f"strategy_{strategy}.txt", self.cfg, merged
            )
            response = self.call_llm(prompt, merged)
            hypothesis = HypothesisORM(
                text=response,
                goal_id=goal_id,
                strategy=strategy,
                features={"strategy": strategy},
                source=self.name,
                pipeline_signature=context.get(PIPELINE),
                pipeline_run_id=context.get(PIPELINE_RUN_ID),
            )
            self.memory.hypotheses.insert(hypothesis)
            hypotheses.append(hypothesis.to_dict())

        return hypotheses

    def _summarize_pattern(self, pattern: dict):
        stats = {}
        for dimension, label in pattern.items():
            stats[label] = stats.get(label, 0) + 1
        return stats
---END-OF-FILE---


"co_ai\agents\generation.py"
---START-OF-FILE---
# co_ai/agents/generation.py

from co_ai.agents.base import BaseAgent
from co_ai.constants import (FEEDBACK, GOAL, GOAL_TEXT, HYPOTHESES, LITERATURE,
                             PIPELINE, PIPELINE_RUN_ID)
from co_ai.parsers import extract_hypotheses


class GenerationAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

    async def run(self, context: dict) -> dict:
        from co_ai.models import HypothesisORM

        goal = context.get(GOAL)

        self.logger.log("GenerationStart", {GOAL: goal})

        # Load literature if available
        literature = context.get(LITERATURE, {})

        # Build context for prompt
        render_context = {
            GOAL: goal.get(GOAL_TEXT),
            LITERATURE: literature,
            FEEDBACK: context.get(FEEDBACK, {}),
            HYPOTHESES: context.get(HYPOTHESES, []),
        }
        merged = {**context, **render_context}

        # Load prompt based on strategy
        prompt_text = self.prompt_loader.load_prompt(self.cfg, merged)
        response = self.call_llm(prompt_text, context)

        # Extract hypotheses
        hypotheses = extract_hypotheses(response)
        hypotheses_saved = []
        prompt = self.memory.prompt.get_from_text(prompt_text)
        for h in hypotheses:
            hyp = HypothesisORM(
                goal_id=goal.get("id"),
                text=h,
                prompt_id=prompt.id,
                pipeline_signature=context.get(PIPELINE),
                pipeline_run_id=context.get(PIPELINE_RUN_ID),
            )
            self.memory.hypotheses.insert(hyp)
            hypotheses_saved.append(hyp.to_dict())

        # Update context with new hypotheses
        context[self.output_key] = hypotheses_saved

        # Log event
        self.logger.log(
            "GeneratedHypotheses",
            {
                GOAL: goal,
                HYPOTHESES: hypotheses,
                "prompt_snippet": prompt_text[:100],
                "response_snippet": response[:200],
            },
        )

        return context
---END-OF-FILE---


"co_ai\agents\generic.py"
---START-OF-FILE---
# co_ai/agents/generic_agent.py
import re

from co_ai.agents.base import BaseAgent


class GenericAgent(BaseAgent):
    def __init__(self, cfg: dict, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.name = cfg.get("name")
        self.cfg = cfg
        self.memory = memory
        self.logger = logger

        # Input/output mapping
        self.strategy = cfg.get("strategy", "default")

        # Regex pattern to extract result
        self.extraction_regex = cfg.get("extraction_regex", r"response:(.*)")

        # Optional refinement
        self.refine_prompts = cfg.get("refine_prompts", False)

    async def run(self, context: dict) -> dict:
        """Run agent based on config-defined behavior"""
        try:
            # Build prompt from template and context
            prompt = self.prompt_loader.load_prompt(self.cfg, context)

            # Call LLM
            response = self.call_llm(prompt, context)

            # Extract result using regex
            match = re.search(self.extraction_regex, response, re.DOTALL)
            result = match.group(1).strip() if match else response

            # Store in context
            context[self.output_key] = {
                "title": self.name,
                "content": result,
                "prompt_used": prompt[:300],
                "strategy": self.strategy
            }

            self.logger.log("AgentRanSuccessfully", {
                "agent": self.name,
                "input_key": self.input_key,
                "output_key": self.output_key,
                "prompt_snippet": prompt[:200],
                "response_snippet": result[:300]
            })

            return context

        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            self.logger.log("AgentFailed", {
                "agent": self.name,
                "error": str(e),
                "context_snapshot": {k: len(str(v)) for k, v in context.items()}
            })
            return context
---END-OF-FILE---


"co_ai\agents\idea_evaluator.py"
---START-OF-FILE---
# co_ai/agents/idea_evaluator.py
from co_ai.agents.base import BaseAgent
from co_ai.constants import GOAL
from co_ai.evaluator.llm_judge_evaluator import LLMJudgeEvaluator
from co_ai.evaluator.mrq_self_evaluator import MRQSelfEvaluator


class IdeaEvaluatorAgent(BaseAgent):
    """
    Evaluates research ideas and hypotheses using multiple strategies:

    - LLM-based pairwise comparison (like DPO)
    - Preference learning via MR.Q Self Evaluator
    """

    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.strategy = cfg.get("strategy", "llm")  # llm | mrq
        self.evaluator = self._init_evaluator()
        self.top_k = cfg.get("top_k", 5)

    async def run(self, context: dict) -> dict:
        hypotheses = self.get_hypotheses(context)
        goal = context.get(GOAL)
        baseline = context.get("baseline_hypotheses", {}).get("text")

        if not hypotheses:
            self.logger.log("NoHypothesesToEvaluate", {})
            context["scored_hypotheses"] = []
            return context

        scored_results = []
        for hyp in hypotheses:
            hyp_text = hyp["text"]
            preferred, scores = self.evaluator.judge(
                prompt=hyp_text,
                output_a=baseline or hyp_text,
                output_b=hyp_text,
                context=context,
            )
            scored_results.append(
                {
                    "text": hyp_text,
                    "preferred": preferred,
                    "scores": scores,
                    "source": "llm-judge",
                    "score": scores.get("score_b", 0),
                    "reasoning": scores.get("reason", ""),
                }
            )

        scored_results.sort(key=lambda x: x["score"], reverse=True)
        context["scored_hypotheses"] = scored_results
        context["top_hypothesis"] = scored_results[0]
        return context

    def get_top_k(self, context: dict, k: int = 5):
        return sorted(
            context.get("scored_hypotheses", []), key=lambda x: x["score"], reverse=True
        )[:k]

    def _init_evaluator(self):
        if self.cfg.get("evaluator", "llm") == "llm":
            llm_model = self.cfg.get("evaluator_model", self.cfg.get("model"))
            prompt_file = self.cfg.get("evaluator_prompt_file", "evaluator.txt")
            return LLMJudgeEvaluator(
                self.cfg,
                llm_cfg=llm_model,
                prompt_file=prompt_file,
                llm=self.call_llm,
                logger=self.logger,
            )
        else:
            return MRQSelfEvaluator(
                memory=self.memory,
                logger=self.logger,
                device=self.cfg.get("device", "cpu"),
            )
---END-OF-FILE---


"co_ai\agents\idea_evolution.py"
---START-OF-FILE---
# co_ai/agents/evolution.py
import itertools

import numpy as np

from co_ai.agents.base import BaseAgent
from co_ai.constants import EVOLVED, GOAL, HYPOTHESES, PIPELINE, RANKING
from co_ai.models import HypothesisORM
from co_ai.parsers import extract_hypotheses


class IdeaEvolutionAgent(BaseAgent):
    """
    The Evolution Agent refines hypotheses iteratively using several strategies:

    - Grafting similar hypotheses into unified statements
    - Feasibility improvement through LLM reasoning
    - Out-of-the-box hypothesis generation
    - Inspiration from top-ranked ideas
    - Simplification and clarity enhancement

    These improvements are based on the paper:
    "NOVELSEEK: When Agent Becomes the Scientist â€“ Building Closed-Loop System from Hypothesis to Verification"
    """

    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.use_grafting = cfg.get("use_grafting", False)
        self.max_variants_per_idea = cfg.get("max_variants", 3)
        self.max_evolution_rounds = cfg.get("evolution_rounds", 4)
        self.selection_top_k = cfg.get("select_top_k", 5)
        self.preferences = cfg.get("preferences", ["novelty", "feasibility"])

    async def run(self, context: dict) -> dict:
        """
        Evolve top-ranked hypotheses across multiple rounds.
        """
        # Get input hypotheses
        ranked_hypotheses = context.get(RANKING, [])
        fallback_hypotheses = context.get(HYPOTHESES, [])
        preferences = context.get("preferences", self.preferences)
        current_round = context.get("evolution_round", 0)

        if not ranked_hypotheses and not fallback_hypotheses:
            self.logger.log("NoHypothesesToEvolve", {"reason": "no_ranked_or_unranked_input"})
            context[EVOLVED] = []
            return context

        # Decide which hypotheses to evolve
        top_texts = [h.get("text") for h, _ in ranked_hypotheses[:3]] if ranked_hypotheses else fallback_hypotheses

        # Run evolution strategies
        all_variants = await self._mutate_all(top_texts, context, preferences)

        # Optionally use grafting
        if self.use_grafting:
            all_variants += await self.graft_similar(context)

        # Score and select top K
        scored_variants = self._score_variants(all_variants, context)
        top_variants = scored_variants[:self.selection_top_k]

        # Save to DB
        self._save_evolved(top_variants, context)

        # Update context
        context["evolved"] = top_variants
        context["evolution_round"] = current_round + 1
        context["evolved_count"] = len(top_variants)

        self.logger.log(
            "EvolutionCompleted",
            {
                "evolved_count": len(top_variants),
                "preferences": preferences,
                "round": current_round + 1
            }
        )

        return context

    async def _mutate_all(self, hypotheses: list, context: dict, preferences: list) -> list:
        """Generate multiple variants for each hypothesis"""
        all_mutants = []

        for h in hypotheses:
            prompt_context = {
                "hypothesis": h,
                "literature_summary": context.get("knowledge_base_summaries", []),
                "critique": context.get("scores", {}),
                "focus_area": context.get(GOAL, {}).get("focus_area"),
                "preferences": ", ".join(preferences)
            }

            prompt = self.prompt_loader.load_prompt(self.cfg, prompt_context)
            raw_output = self.call_llm(prompt, context)

            mutants = extract_hypotheses(raw_output)
            self.logger.log("HypothesisMutated", {
                "original": h[:60],
                "mutations": mutants[:2]
            })

            all_mutants.extend(mutants)

        return all_mutants

    async def graft_similar(self, context: dict, threshold: float = 0.85) -> list:
        """
        Graft pairs of highly similar hypotheses into unified versions.
        """
        hypotheses = self.get_hypotheses(context)
        embeddings = [await self.memory.embedding.get_or_create(h.get("text")) for h in hypotheses]
        used = set()
        grafted = []

        for (i, h1), (j, h2) in itertools.combinations(enumerate(hypotheses), 2):
            if i in used or j in used:
                continue

            sim = self.cosine_similarity(embeddings[i], embeddings[j])
            if sim >= threshold:
                self.logger.log("GraftingPair", {
                    "similarity": sim,
                    "h1": h1[:60] + "...",
                    "h2": h2[:60] + "..."
                })
                prompt = (
                    f"Combine the following hypotheses into a clearer, more innovative statement:\n\n"
                    f'A: {h1.get("text")}\nB: {h2.get("text")}'
                )
                try:
                    response = self.call_llm(prompt, context)
                    combined = extract_hypotheses(response)
                    grafted.extend(combined)
                    used.update([i, j])
                except Exception as e:
                    self.logger.log("GraftingFailed", {"error": str(e)})
                    continue

        # Add ungrafted hypotheses back
        hypotheses = context.get(HYPOTHESES, [])
        for i, h in enumerate(hypotheses):
            if i not in used:
                grafted.append(h)

        return grafted

    def _score_variants(self, variants: list, context: dict) -> list:
        """
        Score variants using ScorerAgent logic and sort by total score
        """
        scorer = self.memory.scores
        scored = []

        for v in variants:
            score_data = scorer.score(v, context)
            score_data["text"] = v
            scored.append(score_data)

        # Sort by composite score
        scored.sort(key=lambda x: x["score"], reverse=True)
        return scored

    def _save_evolved(self, variants: list, context: dict):
        """
        Save evolved hypotheses to database with lineage info
        """
        goal_text = self.extract_goal_text(context.get(GOAL))
        pipeline_sig = context.get(PIPELINE)

        for v in variants:
            hyp = HypothesisORM(
                goal=goal_text,
                text=v["text"],
                pipeline_signature=pipeline_sig,
                parent=context.get("current_hypothesis", None),
                evolution_level=context.get("evolution_round", 0)
            )
            self.db.add(hyp)
        self.db.commit()

    def cosine_similarity(self, vec1, vec2):
        """Compute cosine similarity between two vectors."""
        v1 = np.array(vec1)
        v2 = np.array(vec2)
        return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))

---END-OF-FILE---


"co_ai\agents\idea_innovation.py"
---START-OF-FILE---
# co_ai/agents/idea_innovation.py
from co_ai.agents.base import BaseAgent
from co_ai.constants import GOAL
from co_ai.memory import IdeaStore
from co_ai.models.idea import IdeaORM


class IdeaInnovationAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

    async def run(self, context: dict) -> dict:
        goal = context.get(GOAL)
        survey_results = context.get("survey_results", [])
        search_results = context.get("search_results", [])

        # Build prompt context
        prompt_context = {
            "goal_text": goal.get("goal_text"),
            "focus_area": goal.get("focus_area"),
            "goal_type": goal.get("goal_type"),
            "strategy": goal.get("strategy"),
            "survey_summary": self._summarize_results(survey_results),
            "search_result_summaries": self._summarize_results(search_results),
            "preferences": self.cfg.get("preferences", []),
        }

        merged = {**context, **prompt_context}

        # Load and render prompt
        prompt = self.prompt_loader.load_prompt(self.cfg, merged)

        # Call LLM to generate ideas
        raw_ideas = self.call_llm(prompt, merged)

        # Parse and structure ideas
        ideas = self._parse_raw_ideas(raw_ideas, goal)

        # Store generated ideas
        stored_ideas = self.memory.ideas.bulk_add_ideas(ideas)

        # Update context with results
        context["ideas"] = [idea.to_dict() for idea in stored_ideas]
        context["idea_ids"] = [idea.id for idea in stored_ideas]

        return context

    def _summarize_results(self, results: list) -> str:
        """Converts list of result dicts into a summary string"""
        if not results:
            return "No prior research found."
        summaries = []
        for r in results[:5]:  # limit to top 5 for brevity
            title = r.get("title", "")
            summary = r.get("summary", "")[:200] + "..." if len(r.get("summary", "")) > 200 else ""
            url = r.get("url", "")
            summaries.append(f"- {title}: {summary} ({url})")
        return "\n".join(summaries)

    def _parse_raw_ideas(self, raw_text: str, goal: dict) -> list:
        """Parses raw LLM response into structured idea objects"""
        lines = [line.strip() for line in raw_text.splitlines() if line.strip()]
        ideas = []

        for line in lines:
            ideas.append({
                "idea_text": line,
                "parent_goal": goal.get("goal_text"),
                "focus_area": goal.get("focus_area"),
                "strategy": goal.get("strategy"),
                "source": "generated_by_IdeaInnovationAgent",
                "origin": "llm",
                "extra_data": {}
            })

        return ideas---END-OF-FILE---


"co_ai\agents\idea_sharpening.py"
---START-OF-FILE---
# co_ai/agents/idea_sharpening.py

from co_ai.agents.base import BaseAgent
from co_ai.constants import GOAL, HYPOTHESES, PIPELINE, PIPELINE_RUN_ID
from co_ai.evaluator import MRQSelfEvaluator
from co_ai.models import HypothesisORM


class IdeaSharpeningAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.target = cfg.get("target", "generation")
        self.device = cfg.get("device", "cpu")
        self.evaluator = MRQSelfEvaluator(memory, logger, device=self.device)
        self.templates = cfg.get("templates", ["critic"])
        self.save_count = cfg.get("save_count", 3)


    async def run(self, context: dict) -> dict:
        """
        Main execution loop for IdeaSharpeningAgent.

        Takes a list of ideas, sharpens them using templates,
        judges against baseline using evaluator, and logs results.
        """
        goal = context.get(GOAL, {})
        ideas = context.get("ideas", [])

        if not ideas:
            self.logger.log("NoIdeasToSharpen", {"reason": "empty_input"})
            return context

        sharpened_results = []
        for idea in ideas:
            idea_text = idea.get("idea_text")
            result = await self._sharpen_and_evaluate(idea_text, goal, context)
            sharpened_results.append(result)

        # Sort by score
        sharpened_results.sort(key=lambda x: x["score"], reverse=True)

        # Update context
        context["sharpened_ideas"] = [r["sharpened_hypothesis"] for r in sharpened_results]
        context["scored_ideas"] = sharpened_results
        best_idea = sharpened_results[0]["sharpened_hypothesis"]
        context["top_idea"] = best_idea

        hypotheses = context.get(HYPOTHESES, [])
        if hypotheses:
            # Find the hypothesis with the maximum confidence value
            sorted_hyps = sorted(
                hypotheses, key=lambda h: h.get("confidence", 0.0), reverse=True
            )

            # Keep only the top hypothesis
            context[HYPOTHESES] = sorted_hyps[:self.save_count]
            # For scoring later
            context["baseline_hypotheses"] = sorted_hyps[-1]

        return context

    async def _sharpen_and_evaluate(self, idea: str, goal: dict, context: dict) -> dict:
        # Build prompt for refinement
        focus_area = goal.get("focus_area", "")
        baselines = self.cfg.get("baselines")
        baseline = baselines.get(focus_area, baselines.get("default"))
        merged = {
            "goal": goal,
            "idea": idea,
            "baseline": baseline,
            "literature_summary": context.get("knowledge_base_summaries", []),
            "examples": self.memory.hypotheses.get_similar(idea, limit=3),
            "strategy": goal.get("strategy", "default"),
        }

        improved = None
        winner = "original"
        scores = {}

        for name in self.templates:
            prompt_template = self.prompt_loader.from_file(name, self.cfg, merged)
            sharpened = self.call_llm(prompt_template, merged)

            try:
                preferred_output, scores = self.evaluator.judge(
                    prompt=idea,
                    output_a=idea,
                    output_b=sharpened,
                    context=merged,
                )
                improved = preferred_output
                winner = "b" if improved == sharpened else "a"
            except Exception as e:
                self.logger.log("IdeaSharpeningFailed", {"error": str(e)})
                improved = idea
                winner = "a"
                scores = {"value_a": 5.0, "value_b": 5.0}

            result = {
                "template_used": name,
                "original_idea": idea,
                "sharpened_hypothesis": improved,
                "winner": winner,
                "improved": winner == "b",
                "scores": scores,
                "score": max(scores.values()),
                "pipeline_stage": context.get(PIPELINE),
                "prompt_template": prompt_template,
            }

            saved_hyp = self.save_improved(goal, idea, result, context)
            if saved_hyp:
                context.setdefault(HYPOTHESES, []).append(saved_hyp.to_dict())
            return result

    def save_improved(self, goal: dict, original_idea: str, result: dict, context: dict):
        if not result["improved"]:
            return None
        sharpened = result["sharpened_hypothesis"]
        prompt_id = self.memory.prompt.get_id_from_response(sharpened)

        # Save to HypothesisORM
        hyp = HypothesisORM(
            goal_id=goal.get("id"),
            text=sharpened,
            prompt_id=prompt_id,
            pipeline_signature=context.get(PIPELINE),
            pipeline_run_id=context.get(PIPELINE_RUN_ID),
            source="idea_sharpening_agent",
            confidence=result["score"]
        )
        self.memory.hypotheses.insert(hyp)

        self.logger.log(
            "IdeaSharpenedAndSaved",
            {
                "prompt_snippet": original_idea[:100],
                "response_snippet": sharpened[:100],
                "score": result["score"],
            },
        )

        return hyp
---END-OF-FILE---


"co_ai\agents\judge.py"
---START-OF-FILE---
import re

from co_ai.agents.base import BaseAgent
from co_ai.constants import GOAL, REFLECTION


class JudgeAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

    async def run(self, context: dict) -> dict:
        goal = self.extract_goal_text(context.get(GOAL))
        hypotheses = self.get_hypotheses(context)

        self.logger.log("JudgeRunStarted", {"goal": goal[:100], "hypothesis_count": len(hypotheses)})

        if len(hypotheses) < 2:
            self.logger.log("NotEnoughHypotheses", {"count": len(hypotheses)})
            return context

        reflections = context.get(REFLECTION, [])

        length = len(hypotheses)
        if length < 7:
            self.logger.log("JudgeStrategy", {"method": "all_pair"})
            rankings = await self._run_all_pair(context, hypotheses, reflections)
        elif length < 11:
            self.logger.log("JudgeStrategy", {"method": "tournament"})
            rankings = await self._run_tournament(context, hypotheses, reflections)
        elif length < 16:
            self.logger.log("JudgeStrategy", {"method": "top_k"})
            rankings = await self._run_top_k(context, hypotheses, reflections)
        else:
            self.logger.log("JudgeStrategy", {"method": "default"})
            rankings = await self._run_default(context, hypotheses, reflections)

        self._log_rankings(goal, rankings)

        best_idx = rankings.index(max(rankings, key=lambda x: x["score"]))
        context["best_hypothesis"] = hypotheses[best_idx]
        context[self.output_key] = rankings

        self.logger.log("JudgeRunCompleted", {"best_index": best_idx, "best_score": rankings[best_idx]["score"]})

        return context

    async def _run_all_pair(self, context, hypotheses, reflections):
        scores = [0] * len(hypotheses)

        for i in range(len(hypotheses)):
            for j in range(i + 1, len(hypotheses)):
                result = await self._compare_pair(
                    context,
                    hypotheses[i],
                    hypotheses[j],
                    reflections[i],
                    reflections[j],
                )
                self.logger.log("PairJudged", result)
                if result["winner"] == "A":
                    scores[i] += 1
                elif result["winner"] == "B":
                    scores[j] += 1

        return [
            {"index": idx, "score": score, "text": hypotheses[idx]}
            for idx, score in enumerate(scores)
        ]

    async def _run_tournament(self, context, hypotheses, reflections):
        current_round = list(range(len(hypotheses)))
        scores = [0] * len(hypotheses)

        while len(current_round) > 1:
            next_round = []

            for i in range(0, len(current_round) - 1, 2):
                a = current_round[i]
                b = current_round[i + 1]

                result = await self._compare_pair(
                    context,
                    hypotheses[a],
                    hypotheses[b],
                    reflections[a],
                    reflections[b],
                )
                self.logger.log("TournamentPairJudged", result)

                winner = a if result["winner"] == "A" else b
                loser = b if result["winner"] == "A" else a

                scores[winner] += 1
                next_round.append(winner)

            if len(current_round) % 2 == 1:
                next_round.append(current_round[-1])

            current_round = next_round

        return [
            {"index": idx, "score": scores[idx], "text": hypotheses[idx]}
            for idx in current_round
        ]

    async def _run_top_k(self, context, hypotheses, reflections, k=3):
        scored = []
        for i, h in enumerate(hypotheses):
            score = self._compute_composite_score(reflections[i])
            scored.append((i, score))
            self.logger.log("TopKScoreComputed", {"index": i, "score": score})

        scored.sort(key=lambda x: x[1], reverse=True)
        top_indices = [x[0] for x in scored[:k]]
        top_hypotheses = [hypotheses[i] for i in top_indices]
        top_reflections = [reflections[i] for i in top_indices]

        self.logger.log("TopKSelected", {"indices": top_indices})

        rankings = await self._run_all_pair(context, top_hypotheses, top_reflections)

        return rankings

    async def _run_default(self, context, hypotheses, reflections):
        scores = [0] * len(hypotheses)
        for i in range(0, len(hypotheses), 2):
            try:
                result = await self._compare_pair(
                    context,
                    hypotheses[i],
                    hypotheses[i + 1],
                    reflections[i],
                    reflections[i + 1],
                )
                self.logger.log("DefaultPairJudged", result)

                winner = i if result["winner"] == "A" else i + 1
                scores[winner] += 1
            except IndexError:
                scores[-1] += 1
                break

        return [
            {"index": idx, "score": scores[idx], "text": hypotheses[idx]}
            for idx in range(len(hypotheses))
        ]

    async def _compare_pair(
        self, context, hypothesis_a, hypothesis_b, reflection_a, reflection_b
    ):
        to_merge = {
            "hypothesis_a": hypothesis_a,
            "hypothesis_b": hypothesis_b,
            "reflection_a": reflection_a,
            "reflection_b": reflection_b,
            "notes": context.get("comparison_notes", ""),
        }
        merged = {**context, **to_merge}

        prompt = self.prompt_loader.load_prompt(
            self.cfg, merged
        )
        response = self.call_llm(prompt, context)

        winner_match = re.search(r"better hypothesis:<([AB])>", response, re.IGNORECASE)
        reason_match = re.search(r"reason:<(.+)>", response, re.DOTALL)

        winner = winner_match.group(1).upper() if winner_match else "A"
        reason = reason_match.group(1).strip() if reason_match else "No clear winner"

        return {
            "winner": winner,
            "reason": reason,
            "prompt_used": prompt[:500] + "...",
            "hypothesis_a_snippet": hypothesis_a[:200],
            "hypothesis_b_snippet": hypothesis_b[:200],
        }

    def _compute_composite_score(self, reflection):
        base_score = reflection.get("elo_rating", 1000) / 10

        correctness = reflection.get("correctness_score", 3) * 10
        novelty = reflection.get("novelty_score", 3) * 10
        feasibility = reflection.get("feasibility_score", 3) * 10

        total = base_score + correctness + novelty + feasibility

        return total

    def _log_rankings(self, goal: str, rankings: list):
        for item in rankings:
            self.logger.log(
                "HypothesisRanked",
                {
                    "goal_snippet": goal[:60],
                    "hypothesis_snippet": item["text"][:100],
                    "score": item["score"],
                },
            )
---END-OF-FILE---


"co_ai\agents\knowledge_loader.py"
---START-OF-FILE---
# co_ai/agents/knowledge_loader.py
from co_ai.agents.base import BaseAgent
from co_ai.models import SearchResultORM


class KnowledgeLoaderAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

    async def run(self, context: dict) -> dict:
        goal = context.get("goal")
        goal_id = goal.get("id")

        # Try to load from DB
        knowledge_base = self._load_from_db(goal_id)

        if knowledge_base:
            self.logger.log("LoadedFromDB", {"count": len(knowledge_base)})
            context[self.output_key] = knowledge_base
        else:
            self.logger.log("NoResultsFoundInDB", {})

        return context

    def _load_from_db(self, goal_id: int) -> list:
        """
        Load processed search results (with key concepts, insights, etc.)
        that are linked to this goal.
        """

        results = self.memory.search_results.get_by_goal_id(goal_id)
        return [
            {
                "title": r.title,
                "summary": r.summary,
                "refined_summary": r.refined_summary,
                "url": r.url,
                "source": r.source,
                "key_concepts": r.key_concepts,
                "technical_insights": r.technical_insights,
                "relevance_score": r.relevance_score,
                "related_ideas": r.related_ideas,
                "extracted_methods": r.extracted_methods,
                "domain_knowledge_tags": r.domain_knowledge_tags
            }
            for r in results
        ]---END-OF-FILE---


"co_ai\agents\lats.py"
---START-OF-FILE---
import re
import json
from dataclasses import asdict
from datetime import datetime

from co_ai.agents.base import BaseAgent
from co_ai.constants import PIPELINE, RUN_ID, PIPELINE_RUN_ID
from co_ai.models import RuleApplicationORM, ScoreORM


class PipelineJudgeAgent(BaseAgent):
    async def run(self, context: dict) -> dict:
        self.logger.log("PipelineJudgeAgentStart", {"run_id": context.get(RUN_ID)})

        goal = context["goal"]
        pipeline = context[PIPELINE]
        hypotheses = context.get("scored_hypotheses") or context.get("hypotheses") or []

        self.logger.log("HypothesesReceived", {
            "count": len(hypotheses),
            "source": "scored_hypotheses" if context.get("scored_hypotheses") else "hypotheses"
        })

        if not hypotheses:
            self.logger.log("JudgementSkipped", {
                "error": "No hypotheses found",
                "goal_id": goal.get("id"),
                "run_id": context.get(RUN_ID)
            })
            return context

        top_hypo = hypotheses[0]
        reflection = context.get("lookahead", {}).get("reflection", "")

        prompt_context = {
            "goal": goal,
            "pipeline": pipeline,
            "hypothesis": top_hypo,
            "lookahead": reflection,
        }

        prompt = self.prompt_loader.load_prompt(self.cfg, prompt_context)
        self.logger.log("PromptLoaded", {"prompt": prompt[:200]})

        judgement = self.call_llm(prompt, prompt_context).strip()
        self.logger.log("JudgementReceived", {"judgement": judgement[:300]})

        # Parse main score
        score_match = re.search(
            r"\*\*?score[:=]?\*\*?\s*([0-9]+(?:\.[0-9]+)?)", judgement, re.IGNORECASE
        )

        score = float(score_match.group(1)) if score_match else None
        rationale = judgement[score_match.end():].strip() if score_match else judgement

        if score is None:
            self.logger.log("ScoreParseFailed", {
                "agent": self.name,
                "judgement": judgement,
                "goal_id": goal.get("id"),
                "run_id": context.get(RUN_ID),
                "emoji": "ðŸš¨â“ðŸ§ "
            })
        else:
            self.logger.log("ScoreParsed", {"score": score, "rationale": rationale[:100]})

        # Parse extra dimensions (look for: relevance, clarity, originality, correctness, etc.)
        dimension_fields = ["relevance", "clarity", "originality", "correctness", "novelty", "feasibility"]
        dimensions = {}
        for field in dimension_fields:
            match = re.search(rf"{field}\s*[:=]?\s*([0-9]+(?:\.[0-9]+)?)", judgement, re.IGNORECASE)
            if match:
                dimensions[field.lower()] = float(match.group(1))

        self.logger.log("ScoreDimensionsParsed", {"dimensions": dimensions})

        # Link rule application if available
        rule_application_id = context.get("symbolic_rule_application_id")

        score_obj = ScoreORM(
            goal_id=self.get_goal_id(goal),
            hypothesis_id=self.get_hypothesis_id(top_hypo),
            agent_name=self.name,
            model_name=self.model_name,
            evaluator_name="PipelineJudgeAgent",
            score_type="pipeline_judgment",
            score=score,
            rationale=rationale,
            pipeline_run_id=context.get(RUN_ID),
            symbolic_rule_id=rule_application_id,
            extra_data={"raw_response": judgement},
            dimensions=dimensions  # new: parsed dimensions
        )

        self.memory.scores.insert(score_obj)
        self.logger.log("ScoreSaved", {
            "score_id": score_obj.id,
            "pipeline_run_id": context.get(PIPELINE_RUN_ID),
            "rule_application_id": rule_application_id,
        })

        context[self.output_key] = {
            "score": score_obj.to_dict(),
            "judgement": judgement
        }

        self.logger.log("PipelineJudgeAgentEnd", {"output_key": self.output_key})
        return context
---END-OF-FILE---


"co_ai\agents\literature.py"
---START-OF-FILE---
# co_ai/agents/literature.py

import re

from co_ai.agents.base import BaseAgent
from co_ai.constants import GOAL
from co_ai.tools import WebSearchTool
from co_ai.utils.file_utils import write_text_to_file


class LiteratureAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.strategy = cfg.get("strategy", "query_and_summarize")
        self.preferences = cfg.get("preferences", ["goal_consistency", "novelty"])
        self.max_results = cfg.get("max_results", 5)
        self.web_search_tool = WebSearchTool(cfg.get("web_search", {}), self.logger)

        self.logger.log("LiteratureAgentInit", {
            "strategy": self.strategy,
            "preferences": self.preferences,
            "max_results": self.max_results,
        })

    async def run(self, context: dict) -> dict:
        self.logger.log("LiteratureQuery", {"context": context})
        goal = self.extract_goal_text(context.get(GOAL))

        # Step 1: Generate search query using LLM
        search_query = self._generate_search_query(context)
        if not search_query:
            self.logger.log("LiteratureQueryFailed", {"goal": goal})
            return context

        self.logger.log("SearchingWeb", {"query": search_query, "goal": goal})

        # Step 2: Perform web search
        results = await self.web_search_tool.search(
            search_query, max_results=self.max_results
        )

        if not results:
            self.logger.log("NoResultsFromWebSearch", {
                "goal_snippet": goal[:60],
                "search_query": search_query,
            })
            return context

        self.logger.log("SearchResult", {"results": results})

        # Step 3: Parse each result with LLM
        parsed_results = []
        for result in results:
            summary_context = {
                **{
                    "title": result.get("title", "no Title"),
                    "link": result.get("url", ""),
                    "snippet": result.get("snippet", ""),
                    "page": result.get("page", ""),
                },
                **context,
            }

            summary = self._summarize_result(summary_context)

            if summary.strip():
                parsed_results.append(f"""
                    [Title: {result["title"]}]({result["url"]})\n
                    Summary: {summary}
                """)

        self.logger.log("LiteratureSearchCompleted", {
            "total_results": len(parsed_results),
            "goal": goal,
            "search_query": search_query,
        })

        context["literature"] = parsed_results

        return context

    def _generate_search_query(self, context: dict) -> str:
        try:
            prompt = self.prompt_loader.load_prompt(self.cfg, context)
            self.logger.log("LLMPromptGenerated_SearchQuery", {"prompt_snippet": prompt[:200]})

            response = self.call_llm(prompt, context)
            self.logger.log("LLMResponseReceived_SearchQuery", {"response_snippet": response[:200]})

            # Structured format
            match = re.search(r"search query:<([^>]+)>", response, re.IGNORECASE)
            if match:
                return match.group(1).strip()

            # Fallback format
            match = re.search(r"(?:query|search)[:\s]+\"([^\"]+)\"", response, re.IGNORECASE)
            if match:
                query = match.group(1).strip()
                self.logger.log("SearchQuery", {"Search Query": query})
                return query

            # Fallback to goal
            goal = self.extract_goal_text(context.get(GOAL))
            self.logger.log("FallingBackToGoalAsQuery", {"goal": goal})
            return f"{goal} productivity study"

        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            self.logger.log("LiteratureQueryGenerationFailed", {"error": str(e)})
            return f"{context.get('goal', '')} remote work meta-analysis"

    def _summarize_result(self, context: dict) -> str:
        try:
            prompt = self.prompt_loader.from_file(
                self.cfg.get("parse_prompt", "parse.txt"), self.cfg, context
            )
            self.logger.log("LLMPromptGenerated_Summarize", {
                "title": context.get("title", ""),
                "prompt_snippet": prompt[:200]
            })

            raw_summary = self.call_llm(prompt, context)
            self.logger.log("LLMResponseReceived_Summarize", {
                "title": context.get("title", ""),
                "response_snippet": raw_summary[:200]
            })

            # Try extracting "Summary" section
            summary_match = re.search(
                r"Summary\s*\n(?:.*\n)*?\s*(.+?)(?=\n#|\Z)",
                raw_summary,
                re.DOTALL | re.IGNORECASE,
            )
            if summary_match:
                return summary_match.group(1).strip()

            # Fallback: first paragraph of sufficient length
            lines = raw_summary.splitlines()
            for line in lines:
                if len(line.strip()) > 50:
                    return line.strip()

            return ""

        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            self.logger.log("FailedToParseLiterature", {"error": str(e)})
            return ""
---END-OF-FILE---


"co_ai\agents\lookahead.py"
---START-OF-FILE---
# co_ai/agents/lookahead.py
import re
from dataclasses import asdict

from co_ai.agents.base import BaseAgent
from co_ai.constants import GOAL, PIPELINE
from co_ai.models import LookaheadORM


class LookaheadAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

    async def run(self, context: dict):
        goal = self.memory.goals.get_or_create(context.get(GOAL))

        # Build context for prompt template

        pipeline = context.get(PIPELINE, [])

        agent_registry = context.get("agent_registry", {})
        # Current agents and all available agents from the registry
        pipeline_info = {
            step: agent_registry.get(step, {"description": "No description."})
            for step in pipeline
        }
        print(f"Pipeline info: {pipeline_info}")

        all_agents_info = {name: data for name, data in agent_registry.items()}
        print(f"All agents info: {all_agents_info}")

        prompt_context = {
            "goal": goal.goal_text,
            "goal_type": goal.goal_type,
            "focus_area": goal.focus_area,
            "strategy": goal.strategy,
            "llm_suggested_strategy": goal.llm_suggested_strategy,
            PIPELINE: pipeline,
            "pipeline_info": {
                step: agent_registry.get(step, {"description": "No description"})
                for step in pipeline
            },
            "all_agents": agent_registry, 
            **context
        }

        prompt_template = self.prompt_loader.load_prompt(self.cfg, prompt_context)

        # Call LLM to generate anticipated issues and fallbacks
        response = self.call_llm(prompt_template, prompt_context).strip()

        # Store the reflection for traceability
        model_name = self.cfg.get("model").get("name")
        extracted = self.parse_response(response)
        context.update(extracted)
        pipeline = context.get(PIPELINE, [])
        lookahead_data = LookaheadORM(
            goal=goal.id,
            agent_name=self.name,
            model_name=model_name,
            input_pipeline=context.get(PIPELINE),
            suggested_pipeline=["generation", "verifier", "judge"],
            rationale="Input pipeline lacks verification step.",
            reflection="# Predicted Risks\n- Hallucination risk\n- No validation",
            backup_plans=["Plan A: Add fact-checking", "Plan B: Use retrieval-augmented generation"],
            metadata={"domain": "AI Safety"},
            run_id=context.get("run_id")
        )
        self.memory.lookahead.insert(goal.id, lookahead_data)
        # Log the result
        self.logger.log(
            "LookaheadGenerated",
            {
                "goal": goal.goal_text,
                "lookahead": response[:250],  # short preview
            },
        )

        # Store in context
        context[self.output_key] = lookahead_data
        return context

    def parse_response(self, text: str) -> dict:
        import re

        suggested = re.search(r"# Suggested Pipeline\s*(.*?)\n#", text, re.DOTALL)
        rationale = re.search(r"# Rationale\s*(.*)", text, re.DOTALL)

        pipeline = suggested.group(1).strip().splitlines() if suggested else []
        pipeline = [line.strip("- ").strip() for line in pipeline if line.strip()]

        return {
            "suggested_pipeline": pipeline if pipeline else None,
            "rationale": rationale.group(1).strip() if rationale else None,
        }

    def extract_sections(self, text: str) -> dict:
        # Simple section splitting
        risks_match = re.search(r"# Predicted Risks\s*(.*?)(?:#|$)", text, re.DOTALL)
        backups_match = re.search(r"# Backup Plans\s*(.*)", text, re.DOTALL)

        return {
            "rationale": risks_match.group(1).strip() if risks_match else None,
            "backup_plans": [
                line.strip("- ").strip()
                for line in (
                    backups_match.group(1).strip().split("\n") if backups_match else []
                )
                if line.strip()
            ],
        }
---END-OF-FILE---


"co_ai\agents\meta_review.py"
---START-OF-FILE---
# co_ai/agents/meta_review.py

from co_ai.agents.base import BaseAgent
from co_ai.constants import (DATABASE_MATCHES, EVOLVED, HYPOTHESES, PROXIMITY,
                             RANKING, REFLECTION, REVIEW)


class MetaReviewAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        # Load preferences from config or default list
        self.preferences = cfg.get("preferences", ["goal_consistency", "plausibility"])

    async def run(self, context: dict) -> dict:
        """
        Synthesize insights from reviews and evolved hypotheses.

        Takes:Well I'd like to understand this what ok I've got to review the reflection ground
        - Evolved hypotheses
        - Reflections (from ReflectionAgent)
        - Reviews (from ReviewAgent)
        - Rankings (from RankingAgent)
        - Strategic directions (from ProximityAgent)

        Returns enriched context with:
        - meta_review summary
        - extracted feedback for future generations
        """

        # Get inputs from context
        evolved = context.get(EVOLVED, [])
        if len(evolved) == 0:
            evolved = context.get(HYPOTHESES, [])
        review = context.get(REVIEW, [])
        reflection = context.get(REFLECTION, [])
        ranking = context.get(RANKING, [])
        strategic_directions = context.get("strategic_directions", [])
        db_matches = context.get(PROXIMITY, {}).get(DATABASE_MATCHES, [])

        # Extract key themes from DB hypotheses
        db_themes = "\n".join(f"- {h[:100]}" for h in db_matches)

        # Extract text if needed
        hypothesis_texts = [h.text if hasattr(h, "text") else h for h in evolved]
        reflection_texts = [
            r.review if hasattr(r, "reflection") else r for r in reflection
        ]
        reviewed_texts = [r.review if hasattr(r, "text") else r for r in review]

        # Log inputs for traceability
        self.logger.log(
            "MetaReviewInput",
            {
                "hypothesis_count": len(hypothesis_texts),
                "evolved_count": len(evolved),
                "review_count": len(reviewed_texts),
                "ranked_count": len(ranking),
                "reflection_count": len(reflection_texts),
                "strategic_directions_count": len(strategic_directions),
                "strategic_directions": strategic_directions,
            },
        )

        merged = {
            **context,
            **{
                EVOLVED: evolved,
                REVIEW: review,
                RANKING: ranking,
                "db_themes": db_themes,
            },
        }
        prompt = self.prompt_loader.load_prompt(self.cfg, merged)

        raw_response = self.call_llm(prompt, context)

        # Store full response for debugging
        self.logger.log(
            "RawMetaReviewOutput", {"raw_output": raw_response[:500] + "..."}
        )

        # Add to context
        context[self.output_key] = raw_response

        # Extract structured feedback
        feedback = self._extract_feedback_from_meta_review(raw_response)
        context["feedback"] = feedback

        return context

    def _extract_feedback_from_meta_review(self, meta_review_text):
        try:
            sections = {}
            current_section = None

            for line in meta_review_text.split("\n"):
                line = line.strip()
                if line.startswith("# Meta-Analysis Summary"):
                    current_section = "summary"
                    sections[current_section] = []
                elif line.startswith("# Recurring Critique Points"):
                    current_section = "recurrent_critiques"
                    sections[current_section] = []
                elif line.startswith("# Strengths Observed"):
                    current_section = "strengths"
                    sections[current_section] = []
                elif line.startswith("# Recommended Improvements"):
                    current_section = "improvements"
                    sections[current_section] = []
                elif line.startswith("# Strategic Research Directions"):
                    current_section = "strategic_directions"
                    sections[current_section] = []
                elif line.startswith("- "):
                    if current_section not in sections:
                        sections[current_section] = []
                    sections[current_section].append(line[2:].strip())

            return {
                "summary": "\n".join(sections.get("summary", [])),
                "recurring_critiques": sections.get("recurrent_critiques", []),
                "strengths_observed": sections.get("strengths", []),
                "recommended_improvements": sections.get("improvements", []),
                "strategic_directions": sections.get("strategic_directions", []),
            }

        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            self.logger.log("FeedbackExtractionFailed", {"error": str(e)})
            return {}
---END-OF-FILE---


"co_ai\agents\method_planner.py"
---START-OF-FILE---
# co_ai/agents/method_planner.py
import re

from co_ai.agents.base import BaseAgent
from co_ai.constants import GOAL, HYPOTHESES, PIPELINE
from co_ai.models.method_plan import MethodPlanORM


class MethodPlannerAgent(BaseAgent):
    """
    The MethodPlannerAgent converts abstract research ideas into executable methodological frameworks.

    Based on NOVELSEEK's Method Development Agent:
    > _"The transformation function is represented as: T: I Ã— T Ã— B Ã— L â†’ M"_

    Where:
    - I = Research idea
    - T = Task description
    - B = Baseline implementation
    - L = Relevant literature or knowledge baseAll right Um OK so you're going to come here come here now what you can do is that
    - M = Resulting method plan

    This agent supports both initial planning and iterative refinement of methodologies.
    """

    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.max_refinements = cfg.get("max_refinements", 3)
        self.use_refinement = cfg.get("use_refinement", True)

    async def run(self, context: dict) -> dict:
        """
        Main execution loop for the MethodPlannerAgent.

        Args:
            context (dict): Contains goal, hypotheses, baseline code, and literature summary

        Returns:
            dict: Updated context with generated method plan
        """
        # Extract input from context
        goal = context.get(GOAL, {})
        hypothesis = context.get(HYPOTHESES, "")
        baseline_approach = self._get_baseline(goal.get("focus_area"))
        literature_summary = context.get("knowledge_base_summaries", [])
        pipeline_stage = context.get(PIPELINE, "initial_method_plan")

        # Build prompt context
        prompt_context = {
            "idea": hypothesis or goal.get("goal_text"),
            "task_description": self._extract_task_description(goal),
            "baseline_approach": baseline_approach,
            "literature_summary": self._summarize_literature(literature_summary),
            "preferences": self.cfg.get("preferences", ["novelty", "feasibility"]),
        }

        merged = {**context, **prompt_context}

        # Load and render prompt
        prompt = self.prompt_loader.load_prompt(self.cfg, merged)

        # Call LLM to generate method plan
        raw_plan = self.call_llm(prompt, merged)

        # Parse output into structured format
        try:
            plan_data = self.parse_method_plan_output(raw_plan)
        except Exception as e:
            self.logger.log("MethodPlanParseFailed", {"error": str(e), "raw": raw_plan})
            return context

        # Save to database
        method_plan = self._save_to_db(plan_data, context)

        # Update context with result
        context[self.output_key] = plan_data
        context["method_plan_id"] = method_plan.id
        context["code_plan"] = plan_data.get("code_plan", "")

        self.logger.log(
            "MethodPlanGenerated", {"plan": plan_data, "pipeline_stage": pipeline_stage}
        )

        return context

    def _extract_task_description(self, goal: dict) -> str:
        """
        Extract domain-specific constraints and goals
        Example: Reaction Yield Prediction on Suzuki-Miyaura dataset using SMILES input
        """
        if goal.get("focus_area") == "chemistry":
            return f"{goal.get('goal_text')} ({goal.get('focus_area')})"

        elif goal.get("focus_area") == "nlp":
            return f"{goal.get('goal_text')} ({goal.get('focus_area')})"

        else:
            return goal.get("goal_text", "")

    def _get_baseline(self, focus_area: str) -> str:
        """
        Retrieve baseline implementation from config or file system
        """
        if focus_area == "chemistry":
            return self.cfg.get("baselines").get("reaction_yield_model", "")
        elif focus_area == "nlp":
            return self.cfg.get("baselines").get("sentiment_transformer", "")
        elif focus_area == "cv":
            return self.cfg.get("baselines").get("pointnet_classifier", "")
        else:
            return ""

    def _summarize_literature(self, literature: list) -> str:
        """
        Format literature summaries for use in prompt
        """
        if not literature:
            return "No relevant prior work found."

        return "\n".join(
            [f"- {r['title']}: {r['refined_summary']}" for r in literature[:5]]
        )

    def parse_method_plan_output(self, output: str) -> dict:
        sections = {
            "research_objective": r"\*\*Research Objective:\*\*(.*?)\n\n",
            "key_components": r"\*\*Key Components:\*\*(.*?)\n\n",
            "experimental_plan": r"\*\*Experimental Plan:\*\*(.*?)\n\n",
            "hypothesis_mapping": r"\*\*Hypothesis Mapping:\*\*(.*?)\n\n",
            "search_strategy": r"\*\*Search Strategy:\*\*(.*?)\n\n",
            "knowledge_gaps": r"\*\*Knowledge Gaps:\*\*(.*?)\n\n",
            "next_steps": r"\*\*Next Steps:\*\*(.*?)$",
        }

        result = {}
        for key, pattern in sections.items():
            match = re.search(pattern, output, re.DOTALL)
            if match:
                content = match.group(1).strip()
                if key in ["key_components"]:
                    result[key] = [
                        line.strip() for line in content.splitlines() if line.strip()
                    ]
                else:
                    result[key] = content
            else:
                result[key] = ""

        return result

    def _save_to_db(self, plan_data: dict, goal_id: int) -> MethodPlanORM:
        """
        Store method plan in ORM with metadata
        """
        plan = MethodPlanORM(
            idea_text=plan_data.get("idea"),
            task_description=plan_data.get("task_description"),
            baseline_method=plan_data.get("baseline_used"),
            literature_summary=plan_data.get("relevant_papers"),
            code_plan=plan_data.get("code_plan"),
            score_novelty=plan_data.get("score_novelty"),
            score_feasibility=plan_data.get("score_feasibility"),
            score_impact=plan_data.get("score_impact"),
            score_alignment=plan_data.get("score_alignment"),
            goal_id=goal_id,
            focus_area=plan_data.get("focus_area"),
            strategy=plan_data.get("strategy"),
            evolution_level=0,  # Initial plan
        )

        self.memory.method_plans.add_method_plan(plan.to_dict())  # Or plan.to_dict() if needed
        return plan

    def _refine_plan(self, plan: dict, feedback: dict) -> dict:
        """
        Apply refinement logic based on critique or scoring data
        """
        refinement_prompt = self.prompt_loader.load_prompt(
            "prompts/method_refine.j2", {"current_plan": plan, "feedback": feedback}
        )

        raw_refined = self.call_llm(refinement_prompt)
        return self._parse_plan_output(raw_refined)

    def _score_plan(self, plan: dict, context: dict) -> dict:
        """
        Use ScorerAgent to evaluate methodology quality
        """
        scorer = self.memory.scorer
        scores = scorer.score(plan, context)
        return scores---END-OF-FILE---


"co_ai\agents\model_selector.py"
---START-OF-FILE---
from co_ai.agents.base import BaseAgent
from co_ai.constants import GOAL


class ModelSelectorAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.model_rankings = {}

    async def run(self, context: dict) -> dict:
        goal = self.memory.goals.get_or_create(context.get(GOAL))
        preferences = context.get("preferences", [])

        # Use metadata to select best model
        best_model = self._select_best_model(goal, preferences)

        context["model"] = best_model
        return context

    def _select_best_model(self, goal: str, preferences: list):
        """Select best model based on historical rankings"""
        if not preferences:
            return "qwen3"  # default

        if "novelty" in preferences:
            return "mistral"
        elif "biological_plausibility" in preferences:
            return "phi3"
        elif "simplicity" in preferences:
            return "llama3.2-3b"
        else:
            return "qwen3"  # fallback---END-OF-FILE---


"co_ai\agents\mrq_scoring.py"
---START-OF-FILE---
from co_ai.agents import BaseAgent
from co_ai.evaluator import MRQSelfEvaluator
from co_ai.models import ScoreORM


class MRQScoringAgent(BaseAgent):
    def __init__(self, cfg, memory, logger):
        super().__init__(cfg, memory, logger)
        self.evaluator = MRQSelfEvaluator(memory=memory, logger=logger)
        self.score_source = cfg.get("score_source", "mrq")

    async def run(self, context: dict) -> dict:
        goal = context.get("goal")
        goal_text = goal["goal_text"]
        hypotheses = self.memory.hypotheses.get_by_goal(goal_text)
        count_scored = 0

        for hypothesis in hypotheses:
            if not hypothesis.prompt or not hypothesis.text:
                continue

            existing_score = self.memory.scores.get_by_hypothesis_id(
                hypothesis.id, source=self.score_source
            )
            if existing_score:
                continue  # Skip if already scored by MR.Q

            # Run evaluator
            result = self.evaluator.score_single(
                prompt=hypothesis.prompt.prompt_text,
                output=hypothesis.text
            )

            # Handle result: could be float or dict of dimensions
            if isinstance(result, dict):
                score_value = result.get("overall", 0.0)
                dimensions = {k: v for k, v in result.items() if k != "overall"}
            else:
                score_value = result
                dimensions = {}

            rationale = (
                f"MRQSelfEvaluator assigned a score of {score_value:.4f} "
                f"based on hypothesis embedding alignment."
            )

            score_obj = ScoreORM(
                goal_id=hypothesis.goal_id,
                hypothesis_id=hypothesis.id,
                agent_name=self.name,
                model_name=self.model_name,
                evaluator_name="MRQScoringAgent",
                score_type=self.score_source,
                score=score_value,
                rationale=rationale,
                pipeline_run_id=context.get("pipeline_run_id"),
                extra_data=self.cfg,
                dimensions=dimensions  # ðŸ”¥ store rich sub-scores
            )

            self.memory.scores.insert(score_obj)
            count_scored += 1

        self.logger.log(
            "MRQScoringComplete",
            {
                "goal": goal,
                "scored_count": count_scored,
                "total_hypotheses": len(hypotheses),
            },
        )
        return context
---END-OF-FILE---


"co_ai\agents\mrq_strategy.py"
---START-OF-FILE---
from omegaconf import OmegaConf

from co_ai.agents import BaseAgent
from co_ai.constants import GOAL

DEFAULT_PIPELINES = [
    ["generation", "judge"],
    ["generation", "verifier", "judge"],
    ["generation", "reviewer", "judge"],
    ["cot_generator", "reviewer", "judge"],
    ["retriever", "generation", "judge"],
    ["retriever", "cot_generator", "judge"],
    ["retriever", "generation", "verifier", "judge"]
]

class MRQStrategyAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

        # Load candidate strategies
        file_path = cfg.get("strategy_file")
        if file_path:
            strategy_cfg = OmegaConf.load(file_path)
            self.candidate_strategies = strategy_cfg.get("candidate_strategies", [])
        else:
            self.candidate_strategies = cfg.get("candidate_strategies", [])

        # Initialize model
        self.trained_ranker = None
        self.training_data = []

        # Attempt training (will be partial if data is incomplete)
        self.train_from_reflection_deltas()


    async def run(self, context: dict) -> dict:
        goal = context.get("goal", {})
        goal_text = goal.get("goal_text", "")

        scored = []
        for pipeline in self.cfg.get("candidate_strategies", DEFAULT_PIPELINES):
            s = self.trained_ranker(pipeline)
            scored.append((pipeline, s))

        scored.sort(key=lambda x: x[1], reverse=True)
        best = scored[0][0]

        context["mrq_suggested_pipeline"] = best
        self.logger.log(
            "MRQPipelineSuggested",
            {"goal": goal_text, "suggested": best, "scored_candidates": scored},
        )

        return context

    def train_from_reflection_deltas(self):
        deltas = self.memory.reflection_deltas.get_all()
        examples = []

        for d in deltas:
            a = d.pipeline_a
            b = d.pipeline_b
            score_a = d.score_a
            score_b = d.score_b

            if not isinstance(a, list) or not isinstance(b, list):
                continue
            if score_a is None or score_b is None:
                continue
            if abs(score_a - score_b) < 0.05:
                continue

            label = "b" if score_b > score_a else "a"
            examples.append({
                "goal_text": d.get("goal_text"),
                "pipeline_a": a,
                "pipeline_b": b,
                "score_a": score_a,
                "score_b": score_b,
                "label": label
            })

        self.training_data = examples
        self.logger.log("MRQTrainingDataLoaded", {"count": len(examples)})

        # Train dummy ranker
        self.trained_ranker = self.symbolic_ranker()

    def symbolic_ranker(self):
        """
        Simple ranker that scores pipelines based on symbolic features.
        Prefers longer pipelines and known strong agents.
        """
        def score(pipeline):
            return (
                len(pipeline)
                + 1.5 * ("verifier" in pipeline)
                + 1.2 * ("reviewer" in pipeline)
                + 1.0 * ("retriever" in pipeline)
                + 0.8 * ("cot_generator" in pipeline)
            )
        return score---END-OF-FILE---


"co_ai\agents\pipeline_judge.py"
---START-OF-FILE---
import re

from co_ai.agents.base import BaseAgent
from co_ai.analysis.rule_analytics import RuleAnalytics
from co_ai.analysis.rule_effect_analyzer import RuleEffectAnalyzer
from co_ai.constants import GOAL, PIPELINE, PIPELINE_RUN_ID, RUN_ID
from co_ai.models import ScoreORM
from tabulate import tabulate

def extract_dimensions(text: str) -> dict:
    dimensions = {}
    for line in text.splitlines():
        match = re.match(r"(\w+)\s*[:=]\s*([0-9]+(?:\.[0-9]+)?)", line.strip(), re.IGNORECASE)
        if match:
            key = match.group(1).strip().lower()
            val = float(match.group(2))
            dimensions[key] = val
    return dimensions

class PipelineJudgeAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.print_results = cfg.get("print_results", True)

    async def run(self, context: dict) -> dict:
        self.logger.log("PipelineJudgeAgentStart", {PIPELINE_RUN_ID: context.get(PIPELINE_RUN_ID)})

        goal = context[GOAL]
        pipeline = context[PIPELINE]
        hypotheses = context.get("scored_hypotheses", []) or context.get("hypotheses", [])

        self.logger.log("HypothesesReceived", {
            "count": len(hypotheses),
            "source": "scored_hypotheses" if context.get("scored_hypotheses") else "hypotheses"
        })

        for hypo in hypotheses:
            reflection = context.get("lookahead", {}).get("reflection", "")
            prompt_context = {
                "pipeline": pipeline,
                "hypothesis": hypo.get("text"),
                "lookahead": reflection,
                **context
            }

            prompt = self.prompt_loader.load_prompt(self.cfg, prompt_context)
            self.logger.log("PromptLoaded", {"prompt": prompt[:200]})

            judgement = self.call_llm(prompt, prompt_context).strip()
            self.logger.log("JudgementReceived", {"judgement": judgement[:250]})

            # Score extraction
            # Extract main score and rationale
            score_match = re.search(r"(?:\*\*?score[:=]?\*\*?\s*)?([0-9]+(?:\.[0-9]+)?)", judgement, re.IGNORECASE)
            if score_match:
                score = float(score_match.group(1))
                rationale = judgement[score_match.end():].strip()
                dimensions = extract_dimensions(rationale)
                self.logger.log("ScoreParsed", {
                    "score": score,
                    "dimensions": dimensions,
                    "rationale": rationale[:100]
                })
            else:
                score = None
                rationale = judgement
                dimensions = {}
                self.logger.log("PipelineScoreParseFailed", {
                    "agent": self.name,
                    "judgement": judgement,
                    "goal_id": goal.get("id"),
                    RUN_ID: context.get(RUN_ID),
                })

            # Save score
            score_obj = ScoreORM(
                goal_id=self.get_goal_id(goal),
                hypothesis_id=self.get_hypothesis_id(hypo),
                agent_name=self.name,
                model_name=self.model_name,
                evaluator_name="PipelineJudgeAgent",
                score_type="pipeline_judgment",
                score=score,
                rationale=rationale,
                pipeline_run_id=context.get(PIPELINE_RUN_ID),
                extra_data={"raw_response": judgement},
                dimensions=dimensions,
            )
            self.memory.scores.insert(score_obj)
            self.logger.log("ScoreSaved", {"score_id": score_obj.id, "run_id": context.get(RUN_ID)})

            # Update rule applications (if matched)
            applications = self.memory.rule_effects.get_by_run_and_goal(
                context.get(PIPELINE_RUN_ID), goal.get("id")
            )

            for app in applications:
                if app.hypothesis_id == self.get_hypothesis_id(hypo):
                    app.result_score = score
                    app.result_label = "pipeline_judgment"
                    app.rule_application_id = app.id  # ensure linkage
                    self.memory.rule_effects.update(app)
                    self.logger.log("RuleApplicationUpdated", {
                        "rule_id": app.rule_id,
                        "score": score,
                        "hypothesis_id": app.hypothesis_id
                    })

        self.report_rule_analytics()
        self.run_rule_effects_evaluation(context)

        self.logger.log("PipelineJudgeAgentEnd", {"output_key": self.output_key})
        return context

    def report_rule_analytics(self):
        analytics = RuleAnalytics(db=self.memory, logger=self.logger)
        results = analytics.analyze_all_rules()

        if isinstance(results, list) and self.print_results:
            print("\n=== Rule Analytics Summary ===")
            print(f"{'Rule ID':<10}{'Applications':<15}{'Avg Score':<12}")
            print("-" * 40)
            for result in results:
                rule_id = result.get("rule_id")
                count = result.get("count", 0)
                avg_score = result.get("avg_score", 0.0)
                print(f"{rule_id:<10}{count:<15}{avg_score:<12.2f}")
            print("-" * 40)

    from tabulate import tabulate

    def run_rule_effects_evaluation(self, context: dict):
        analyzer = RuleEffectAnalyzer(session=self.memory.session, logger=self.logger)
        summary = analyzer.analyze(context.get(PIPELINE_RUN_ID))

        # Sort by average score descending
        top_rules = sorted(
            summary.items(), key=lambda x: x[1]["avg_score"], reverse=True
        )

        # Prepare table data
        table_data = []
        for rule_id, data in top_rules[:5]:
            table_data.append(
                [
                    rule_id,
                    f"{data['avg_score']:.2f}",
                    data["count"],
                    f"{data['min']} / {data['max']}",
                    f"{data['std']:.2f}",
                    f"{data['success_rate']:.2%}",
                ]
            )

        # Define headers
        headers = [
            "Rule ID",
            "Avg Score",
            "Count",
            "Min / Max",
            "Std Dev",
            "Success Rate â‰¥50",
        ]

        # Print table
        print("\nðŸ“ˆ Top Performing Rules:")
        print(tabulate(table_data, headers=headers, tablefmt="fancy_grid"))

        analyzer.pipeline_run_scores(context=context)
---END-OF-FILE---


"co_ai\agents\planner.py"
---START-OF-FILE---
from co_ai.agents import DOTSPlannerAgent, LookaheadAgent


class PipelinePlannerAgent:
    def __init__(self, cfg, memory=None, logger=None):
        self.cfg = cfg
        self.memory = memory
        self.logger = logger

        # Load sub-agents if enabled
        self.dots_enabled = cfg.get("dots_enabled", True)
        self.lookahead_enabled = cfg.get("lookahead_enabled", True)

        if self.dots_enabled:
            self.dots = DOTSPlannerAgent(cfg, memory, logger)
        if self.lookahead_enabled:
            self.lookahead = LookaheadAgent(cfg, memory, logger)


    async def run(self, context):
        if self.dots:
            context = await self.dots.run(context)
        if self.lookahead:
            context = await self.lookahead.run(context)
        return context
---END-OF-FILE---


"co_ai\agents\prompt_compiler.py"
---START-OF-FILE---
from co_ai.agents.base import BaseAgent
from co_ai.agents.mixins.prompt_evolver_mixin import PromptEvolverMixin
from co_ai.compiler.llm_compiler import LLMCompiler
from co_ai.compiler.passes.strategy_mutation_pass import StrategyMutationPass
from co_ai.constants import GOAL
from co_ai.models import HypothesisORM
import dspy
from co_ai.evaluator.evaluator_loader import get_evaluator

class PromptCompilerAgent(BaseAgent, PromptEvolverMixin):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.prompt_key = cfg.get("prompt_key", "default")
        self.sample_size = cfg.get("sample_size", 20)
        self.generate_count = cfg.get("generate_count", 10)
        self.version = cfg.get("version", 1)
        self.use_strategy_mutation = cfg.get("use_strategy_mutation", False)

        # Initialize the LLM compiler through DSPy
        llm = dspy.LM("ollama_chat/qwen3", api_base="http://localhost:11434")
        self.init_evolver(llm, logger=logger)
        self.compiler = LLMCompiler(llm=self.llm, logger=self.logger)
        self.evaluator = get_evaluator(cfg, memory, self.call_llm, logger)
        if self.use_strategy_mutation:
            self.strategy_pass = StrategyMutationPass(self.evaluator,
                compiler=self.compiler, logger=self.logger
            )


    async def run(self, context: dict) -> dict:
        goal = context.get(GOAL)
        goal_text = self.extract_goal_text(goal)
        total_count = self.sample_size + self.generate_count

        examples = self.memory.prompt.get_prompt_training_set(goal_text, total_count)
        if not examples:
            self.logger.log("PromptCompilerSkipped", {"reason": "no_examples", "goal": goal})
            return context

        refined_prompts = self.evolve_prompts(
            examples, context=context, sample_size=self.sample_size
        )

        scored = []
        hypotheses = []
        for prompt in refined_prompts:
            # Generate hypothesis from the compiled prompt
            hypothesis_text = self.call_llm(prompt, context).strip()
            prompt_id  = self.get_prompt_id(prompt)
            self.logger.log("CompiledPromptHypothesisGenerated", {
                "prompt": prompt[:100],
                "hypothesis": hypothesis_text[:100]
            })
            hyp = HypothesisORM(
                goal_id=goal.get("id"),
                prompt_id=prompt_id,
                source=self.name,
                text=hypothesis_text,
                strategy=self.strategy,
                features={"source": "compiled_prompt"},
                pipeline_run_id=context.get("pipeline_run_id"),
            )
            self.memory.hypotheses.insert(hyp)
            hypotheses.append(hyp.to_dict())
            score = self.score_prompt(
                prompt=prompt,
                reference_output=examples[0].get("hypothesis_text", ""),
                context=context
            )
            scored.append((prompt, score))
            self.add_to_prompt_history(context, prompt, {"source": "dspy_compiler"})

        self.logger.log(
            "PromptCompilerCompleted",
            {"goal": goal, "generated_count": len(refined_prompts)},
        )


        scored_sorted = sorted(scored, key=lambda x: get_winner_score(x[1]), reverse=True)

        context["refined_prompts"] = scored_sorted
        context["hypotheses"] = hypotheses

        # Log top results
        if self.logger:
            for i, (text, score) in enumerate(scored_sorted[:5]):
                self.logger.log(
                    "CompiledPromptScore",
                    {"rank": i + 1, "score": score, "prompt": text[:200]},
                )
        sorted_prompt = scored_sorted[0][0] if scored_sorted else None
        best_score_dict = scored_sorted[0][1] if scored_sorted else {}

        # Assume you already have: sorted_prompt, best_score_dict
        context["compiled_prompt"] = {
            "text": sorted_prompt,  # best-performing prompt
            "score_summary": best_score_dict,
            "compiler_agent": self.name,
        }
        self.logger.log("CompiledPromptSelected", context["compiled_prompt"])

        return context

    def score_prompt(self, prompt: str, reference_output, context:dict) -> float:
        if not self.evaluator:
            return 0.0
        try:
            score = self.evaluator.score_single(prompt, reference_output, context)
            if self.logger:
                self.logger.log("ScoringPrompt", {"score": score, "prompt": prompt[:100], "reference_output": reference_output[:100]})
            return score 
        except Exception as e:
            if self.logger:
                self.logger.log("PromptScoreError", {"prompt": prompt[:100], "error": str(e)})
            return 0.0

def get_winner_score(score_dict):
    if isinstance(score_dict, float):
        return score_dict
    if score_dict["winner"] == "A":
        return score_dict.get("score_a", 0)
    elif score_dict["winner"] == "B":
        return score_dict.get("score_b", 0)
    return 0  # fallback---END-OF-FILE---


"co_ai\agents\prompt_tuning.py"
---START-OF-FILE---
import re
from abc import ABC, abstractmethod

import dspy
from dspy import (BootstrapFewShot, Example, InputField, OutputField, Predict,
                  Signature)

from co_ai.agents.base import BaseAgent
from co_ai.constants import GOAL


# DSPy signature for prompt refinement: defines input/output fields for tuning
class PromptTuningSignature(Signature):
    goal = InputField(desc="Scientific research goal or question")
    input_prompt = InputField(desc="Original prompt used to generate hypotheses")
    hypotheses = InputField(desc="Best hypothesis generated")
    review = InputField(desc="Expert review of the hypothesis")
    score = InputField(desc="Numeric score evaluating the hypothesis quality")
    refined_prompt = OutputField(desc="Improved version of the original prompt")


# Simple evaluation result class to return from evaluator
class EvaluationResult:
    def __init__(self, score: float, reason: str):
        self.score = score
        self.reason = reason


# Base evaluator interface (not used directly, but useful for future extensions)
class BaseEvaluator(ABC):
    @abstractmethod
    def evaluate(
        self, original: str, proposal: str, metadata: dict = None
    ) -> EvaluationResult:
        pass


# DSPy-based evaluator that can run a Chain-of-Thought program
class DSPyEvaluator(BaseEvaluator):
    def __init__(self):
        self.program = dspy.ChainOfThought(PromptTuningSignature)

    def evaluate(
        self, original: str, proposal: str, metadata: dict = None
    ) -> EvaluationResult:
        result = self.program(
            goal=metadata["goal"],
            input_prompt=original,
            hypotheses=metadata["hypotheses"],
            review=metadata.get("review", ""),
            score=metadata.get("score", 750),
        )
        try:
            score = float(result.score)
        except (ValueError, TypeError):
            score = 0.0
        return EvaluationResult(score=score, reason=result.explanation)


# Main agent class responsible for training and tuning prompts using DSPy
class PromptTuningAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.agent_name = cfg.get("name", "prompt_tuning")
        self.prompt_key = cfg.get("prompt_key", "default")
        self.sample_size = cfg.get("sample_size", 20)
        self.generate_count = cfg.get("generate_count", 10)
        self.current_version = cfg.get("version", 1)

        # Configure DSPy with local LLM (Ollama)
        lm = dspy.LM(
            "ollama_chat/qwen3",
            api_base="http://localhost:11434",
            api_key="",
        )
        dspy.configure(lm=lm)

    async def run(self, context: dict) -> dict:
        goal = self.extract_goal_text(context.get(GOAL))
        generation_count = self.sample_size + self.generate_count
        self.logger.log(
            "PromptTuningExamples",
            {"samples size": self.sample_size, "generation count": generation_count},
        )

        # Get training + validation data
        examples = self.memory.prompt.get_prompt_training_set(goal, generation_count)
        train_data = examples[: self.sample_size]
        val_data = examples[self.sample_size :]

        if not examples:
            self.logger.log(
                "PromptTuningSkipped", {"reason": "no_training_data", "goal": goal}
            )
            return context

        # Build training set for DSPy
        training_set = [
            Example(
                goal=item["goal"],
                input_prompt=item["prompt_text"],
                hypotheses=item["hypothesis_text"],
                review=item.get("review", ""),
                score=item.get("elo_rating", 1000),
            ).with_inputs("goal", "input_prompt", "hypotheses", "review", "score")
            for item in train_data
        ]

        # Wrap our scoring metric so we can inject context during tuning
        def wrapped_metric(example, pred, trace=None):
            return self._prompt_quality_metric(example, pred, context=context)

        # Train prompt-tuning program
        tuner = BootstrapFewShot(metric=wrapped_metric)
        student = Predict(PromptTuningSignature)
        tuned_program = tuner.compile(student=student, trainset=training_set)

        # Use tuned program to generate and store new refined prompt
        await self.generate_and_store_refined_prompts(
            tuned_program, goal, context, val_data
        )
        self.logger.log(
            "PromptTuningCompleted",
            {
                "goal": goal,
                "example_count": len(training_set),
                "generated_count": len(val_data),
            },
        )

        return context

    async def generate_and_store_refined_prompts(
        self, tuned_program, goal: str, context: dict, val_set
    ):
        """
        Generate refined prompts using the tuned DSPy program and store them in the database.

        Args:
            tuned_program: A compiled DSPy program capable of generating refined prompts.
            goal: The scientific goal for this run.
            context: Shared pipeline state.
            val_set: Validation examples to run through the tuned program.
        """

        stored_count = 0
        for i, example in enumerate(val_set):
            try:
                # Run DSPy program on new example
                result = tuned_program(
                    goal=example["goal"],
                    input_prompt=example["prompt_text"],
                    hypotheses=example["hypothesis_text"],
                    review=example.get("review", ""),
                    score=example.get("elo_rating", 1000),
                )

                refined_prompt = result.refined_prompt.strip()

                # Store refined prompt to the DB
                self.memory.prompt.save(
                    goal={"goal_text": example["goal"]},
                    agent_name=self.name,
                    prompt_key=self.prompt_key,
                    prompt_text=refined_prompt,
                    response=None,
                    pipeline_run_id=context.get("pipeline_run_id"),
                    strategy="refined_via_dspy",
                    version=self.current_version + 1,
                )

                stored_count += 1

                # Update context with prompt history
                self.add_to_prompt_history(
                    context, refined_prompt, {"original": example["prompt_text"]}
                )

                self.logger.log(
                    "TunedPromptStored",
                    {"goal": goal, "refined_snippet": refined_prompt[:100]},
                )

            except Exception as e:
                print(f"âŒ Exception: {type(e).__name__}: {e}")
                self.logger.log(
                    "TunedPromptGenerationFailed",
                    {"error": str(e), "example_snippet": str(example)[:100]},
                )

        self.logger.log(
            "BatchTunedPromptsComplete", {"goal": goal, "count": stored_count}
        )

    def _prompt_quality_metric(self, example, pred, context: dict) -> float:
        """Run both prompts and compare results"""
        try:
            prompt_a = example.input_prompt
            prompt_b = pred.refined_prompt
            self.logger.log(
                "PromptQualityCompareStart",
                {
                    "prompt_a_snippet": prompt_a[:100],
                    "prompt_b_snippet": prompt_b[:100],
                },
            )

            hypotheses_a = self.call_llm(prompt_a, context)
            self.logger.log(
                "PromptAResponseGenerated", {"hypotheses_a_snippet": hypotheses_a[:200]}
            )

            hypotheses_b = self.call_llm(prompt_b, context)
            self.logger.log(
                "PromptBResponseGenerated", {"hypotheses_b_snippet": hypotheses_b[:200]}
            )

            # Run comparison
            merged = {
                **context,
                **{
                    "prompt_a": prompt_a,
                    "prompt_b": prompt_b,
                    "hypotheses_a": hypotheses_a,
                    "hypotheses_b": hypotheses_b,
                },
            }
            comparison_prompt = self.prompt_loader.load_prompt(self.cfg, merged)
            self.logger.log(
                "ComparisonPromptConstructed",
                {"comparison_prompt_snippet": comparison_prompt[:200]},
            )

            response = self.call_llm(comparison_prompt, context)
            self.logger.log(
                "ComparisonResponseReceived", {"response_snippet": response[:200]}
            )

            match = re.search(r"better prompt:<([AB])>", response, re.IGNORECASE)
            if match:
                choice = match.group(1).upper()
                score = 1.0 if choice == "B" else 0.5
                self.logger.log(
                    "PromptComparisonResult", {"winner": choice, "score": score}
                )
                return score
            else:
                self.logger.log("PromptComparisonNoMatch", {"response": response})
                return 0.0
        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            self.logger.log(
                "PromptQualityMetricError",
                {
                    "error": str(e),
                    "example_input_prompt_snippet": example.input_prompt[:100],
                    "refined_prompt_snippet": getattr(pred, "refined_prompt", "")[:100],
                },
            )
            return 0.0
---END-OF-FILE---


"co_ai\agents\prompt_validation.py"
---START-OF-FILE---
from collections import defaultdict
from co_ai.agents.base import BaseAgent
from co_ai.constants import PIPELINE_RUN_ID


class PromptValidationAgent(BaseAgent):
    async def run(self, context: dict) -> dict:
        self.logger.log("PromptValidationAgentStart", {PIPELINE_RUN_ID: context.get(PIPELINE_RUN_ID)})

        hypotheses = context.get("scored_hypotheses", [])
        compiled_prompt = context.get("compiled_prompt", {})
        compiled_prompt_hash = hash(compiled_prompt.get("text", ""))

        prompt_to_scores = defaultdict(list)

        for hypo in hypotheses:
            prompt_key = hypo.get("source_prompt")
            score = hypo.get("score")
            if prompt_key is not None and score is not None:
                prompt_to_scores[prompt_key].append(score)

        if not prompt_to_scores:
            self.logger.log("PromptValidationNoData", {"message": "No prompt-score mappings found."})
            return context

        prompt_avg_scores = {
            prompt: sum(scores) / len(scores)
            for prompt, scores in prompt_to_scores.items()
        }

        best_prompt, best_score = max(prompt_avg_scores.items(), key=lambda x: x[1])

        passed_validation = compiled_prompt_hash == best_prompt
        self.logger.log("PromptValidationResult", {
            "compiled_prompt_hash": compiled_prompt_hash,
            "best_prompt_hash": best_prompt,
            "best_score": best_score,
            "passed": passed_validation,
        })

        print("\n=== Prompt Validation ===")
        print(f"Selected prompt hash: {compiled_prompt_hash}")
        print(f"Best-scoring prompt hash: {best_prompt} (avg score: {best_score:.2f})")
        if passed_validation:
            print("\u2705 Prompt compiler selected the best-performing prompt!")
        else:
            print("\u26a0\ufe0f Mismatch: Consider revising compiler strategy or scoring alignment.")

        return context
---END-OF-FILE---


"co_ai\agents\proximity.py"
---START-OF-FILE---
# co_ai/agents/proximity.py
import itertools

import numpy as np

from co_ai.agents.base import BaseAgent
from co_ai.constants import (DATABASE_MATCHES, GOAL, GOAL_TEXT,
                             PIPELINE_RUN_ID, TEXT)
from co_ai.models import ScoreORM
from co_ai.scoring.proximity import ProximityScore


class ProximityAgent(BaseAgent):
    """
    The Proximity Agent calculates similarity between hypotheses and builds a proximity graph.
    """

    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.similarity_threshold = cfg.get("similarity_threshold", 0.75)
        self.max_graft_candidates = cfg.get("max_graft_candidates", 3)
        self.top_k_database_matches = cfg.get("top_k_database_matches", 5)

    async def run(self, context: dict) -> dict:
        goal = context.get(GOAL)
        goal_text = goal.get(GOAL_TEXT)
        current_hypotheses = self.get_hypotheses(context)

        db_texts = self.memory.hypotheses.get_similar(goal_text, limit=self.top_k_database_matches)
        self.logger.log("DatabaseHypothesesMatched", {
            GOAL: goal,
            "matches": [{"text": h[:100]} for h in db_texts],
        })

        hypotheses_texts = [h.get(TEXT) for h in current_hypotheses]
        all_hypotheses = list(set(hypotheses_texts + db_texts))

        if not all_hypotheses:
            self.logger.log("NoHypothesesForProximity", {"reason": "empty_input"})
            return context

        similarities = self._compute_similarity_matrix(all_hypotheses)
        self.logger.log("ProximityGraphComputed", {
            "total_pairs": len(similarities),
            "threshold": self.similarity_threshold,
            "top_matches": [
                {"pair": (h1[:60], h2[:60]), "score": sim}
                for h1, h2, sim in similarities[:3]
            ],
        })

        graft_candidates = [(h1, h2) for h1, h2, sim in similarities if sim >= self.similarity_threshold]
        clusters = self._cluster_hypotheses(graft_candidates)

        context[self.output_key] = {
            "clusters": clusters,
            "graft_candidates": graft_candidates,
            DATABASE_MATCHES: db_texts,
            "proximity_graph": similarities,
        }


        top_similar = similarities[: self.max_graft_candidates]
        to_merge = {
                GOAL: goal,
                "most_similar": "\n".join(
                    [f"{i + 1}. {h1} â†” {h2} (sim: {score:.2f})"
                     for i, (h1, h2, score) in enumerate(top_similar)]
                ),
            }

        merged = {**context, **to_merge}
        summary_prompt = self.prompt_loader.load_prompt(
            self.cfg, merged
        )

        summary_output = self.call_llm(summary_prompt, merged)
        context["proximity_summary"] = summary_output

        scorer = ProximityScore(self.cfg, memory=self.memory, logger=self.logger)
        score = scorer.compute({"proximity_analysis": summary_output}, context)

        self.logger.log(
            "ProximityAnalysisScored",
            {
                "score": score,
                "analysis": summary_output[:300],
            },
        )

        # Compute additional dimensions
        cluster_count = len(clusters)
        top_k_sims = [sim for _, _, sim in similarities[:self.max_graft_candidates]]
        avg_top_k_sim = sum(top_k_sims) / len(top_k_sims) if top_k_sims else 0.0
        graft_count = len(graft_candidates)

        dimensions = {
            "proximity_score": score,
            "cluster_count": cluster_count,
            "avg_similarity_top_k": round(avg_top_k_sim, 4),
            "graft_pair_count": graft_count,
        }

        # Save dimensional score for each hypothesis
        for hypothesis in current_hypotheses:
            score_obj = ScoreORM(
                agent_name=self.name,
                model_name=self.model_name,
                goal_id=goal.get("goal_id"),
                hypothesis_id=hypothesis.get("id"),
                score_type=self.name,
                evaluator_name=self.name,
                score=score,
                extra_data={"summary": summary_output},
                dimensions=dimensions,  # âœ… Added here
                pipeline_run_id=context.get(PIPELINE_RUN_ID),
            )
            self.memory.scores.insert(score_obj)

        return context

    def _compute_similarity_matrix(self, hypotheses: list[str]) -> list[tuple]:
        vectors = []
        valid_hypotheses = []

        for h in hypotheses:
            vec = self.memory.embedding.get_or_create(h)
            if vec is None:
                self.logger.log("MissingEmbedding", {"hypothesis_snippet": h[:60]})
                continue
            vectors.append(vec)
            valid_hypotheses.append(h)

        similarities = []
        for i, j in itertools.combinations(range(len(valid_hypotheses)), 2):
            h1 = valid_hypotheses[i]
            h2 = valid_hypotheses[j]
            sim = self._cosine(vectors[i], vectors[j])
            similarities.append((h1, h2, sim))

        similarities.sort(key=lambda x: x[2], reverse=True)
        return similarities

    def _cosine(self, a, b):
        a = np.array(list(a), dtype=float)
        b = np.array(list(b), dtype=float)
        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))

    def _cluster_hypotheses(self, graft_candidates: list[tuple]) -> list[list[str]]:
        clusters = []

        for h1, h2 in graft_candidates:
            found = False
            for cluster in clusters:
                if h1 in cluster or h2 in cluster:
                    if h1 not in cluster:
                        cluster.append(h1)
                    if h2 not in cluster:
                        cluster.append(h2)
                    found = True
                    break
            if not found:
                clusters.append([h1, h2])

        merged_clusters = []
        for cluster in clusters:
            merged = False
            for mc in merged_clusters:
                if set(cluster) & set(mc):
                    mc.extend(cluster)
                    merged = True
                    break
            if not merged:
                merged_clusters.append(list(set(cluster)))

        return merged_clusters
---END-OF-FILE---


"co_ai\agents\ranking.py"
---START-OF-FILE---
# co_ai/agents/ranking.py
import itertools
import random
import re
from typing import Optional

from co_ai.agents.base import BaseAgent


class RankingAgent(BaseAgent):
    """
    The Ranking agent simulates scientific debate between hypotheses using a tournament-style approach.

    From the paper:
    > 'The Ranking agent employs an Elo-based tournament to assess and prioritize generated hypotheses'
    """
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.elo_scores = {}
        self.strategy = cfg.get("strategy", "debate")
        self.max_comparisons = cfg.get("max_comparisons", 6)
        self.initial_elo_score = cfg.get("initial_elo_score", 750)
        self.win_history = []
        self.preferences = cfg.get("preferences", ["novelty", "feasibility"])
        self.elo_scores = {}         # map: hypothesis ID â†’ score
        self.hypothesis_lookup = {}  # map: hypothesis ID â†’ full dict

    async def run(self, context: dict) -> dict:
        """
        Rank hypotheses using pairwise comparisons and Elo updates.

        Args:
            context: Dictionary with keys:
                - hypotheses: list of hypothesis strings
                - goal: research objective
                - preferences: override criteria
        """
        hypotheses = self.get_hypotheses(context)

        if len(hypotheses) < 2:
            self.logger.log("NotEnoughHypothesesForRanking", {
                "count": len(hypotheses),
                "reason": "less than 2 hypotheses"
            })
            context[self.output_key] = [(h, 1000) for h in hypotheses]
            return context

        self._initialize_elo(hypotheses)

        pairs = list(itertools.combinations(hypotheses, 2))
        comparisons = random.sample(pairs, k=min(self.max_comparisons, len(pairs)))

        for hyp1, hyp2 in comparisons:
            prompt = self._build_ranking_prompt(hyp1, hyp2, context)
            response = self.call_llm(prompt, context)
            winner = self._parse_response(response)

            if winner:
                self._update_elo(hyp1, hyp2, winner)
            else:
                self.logger.log(
                    "ComparisonParseFailed",
                    {
                        "prompt_snippet": prompt[:200],
                        "response_snippet": response[:300],
                        "agent": self.__class__.__name__,
                    },
                )

        ranked_ids = sorted(self.elo_scores.items(), key=lambda x: x[1], reverse=True)
        context[self.output_key] = [
            (self.hypothesis_lookup[h_id], score) for h_id, score in ranked_ids
        ]

        self.logger.log(
            "TournamentCompleted",
            {
                "total_hypotheses": len(ranked_ids),
                "win_loss_patterns": self._extract_win_loss_feedback(),
                "preferences": self.preferences,
            },
        )

        return context

    def _initialize_elo(self, hypotheses):
        for h in hypotheses:
            hyp_id = h.get("id") or h.get("text")  # fallback to text if no id
            self.elo_scores[hyp_id] = self.initial_elo_score
            self.hypothesis_lookup[hyp_id] = h

    def _build_ranking_prompt(self, hyp1, hyp2, context):
        return self.prompt_loader.load_prompt(
            self.cfg,
            {
                **context,
                "hypothesis_a": hyp1.get("text"),
                "hypothesis_b": hyp2.get("text"),
            },
        )

    def _conduct_multi_turn_debate(self, context:dict, hyp1:str, hyp2:str, turns:int=3):
        """Simulate multi-turn scientific debate between hypotheses"""
        for i in range(turns):
            prompt = self._build_ranking_prompt(hyp1, hyp2, context=context)
            response = self.call_llm(prompt, context)
            winner = self._parse_response(response)
            if winner:
                self._update_elo(hyp1, hyp2, winner)
            else:
                break


    def _generate_pairwise_comparisons(self, hypotheses):
        """Generate combinations of hypothesis pairs for ranking"""
        return itertools.combinations(hypotheses, 2)

    def _generate_proximity_based_pairs(self, hypotheses):
        """Prioritize comparisons between similar hypotheses"""
        similarities = [
            (h1, h2, self._compute_similarity(h1, h2))
            for h1, h2 in itertools.combinations(hypotheses, 2)
        ]
        return sorted(similarities, key=lambda x: x[2], reverse=True)

    def _extract_win_loss_feedback(self):
        win_counts = {}
        for id1, id2, winner in self.win_history:
            winner_id = id1 if winner == "A" else id2
            win_counts[winner_id] = win_counts.get(winner_id, 0) + 1

        return {
            "top_performers": [
                {
                    "hypothesis": self.hypothesis_lookup[h],
                    "wins": w
                }
                for h, w in sorted(win_counts.items(), key=lambda x: x[1], reverse=True)
            ],
            "total_matches": len(self.win_history),
            "preferences_used": self.preferences
        }

    def _rank_pairwise(self, reviewed, context):
        pairs = list(itertools.combinations(reviewed, 2))
        if not pairs:
            return

        # Limit number of comparisons per round
        comparisons = random.sample(pairs, k=min(self.cfg.get("max_comparisons", 6), len(pairs)))

        for item1, item2 in comparisons:
            hyp1 = item1["hypotheses"]
            hyp2 = item2["hypotheses"]

            merged = {**self.cfg, **{"hypothesis_a": hyp1, "hypothesis_b": hyp2}}


            prompt = self.prompt_loader.load_prompt(merged, context=context)

            self.logger.log("RankingCompare", {"hyp1": hyp1[:60],  "hyp2":hyp2[:60]})

            try:
                response = self.call_llm(prompt, context)
                winner = self._parse_response(response)

                if winner:
                    self._update_elo(hyp1, hyp2, winner)
                else:
                    self.logger.log("ComparisonParseFailed", {
                        "prompt_snippet": prompt[:200],
                        "response_snippet": response[:300]
                    })
            except Exception as e:
                self.logger.log(
                    "ComparisonError",
                    {"error": str(e), "hypotheses": [hyp1[:100], hyp2[:100]]},
                )

    def _update_elo(self, hyp1, hyp2, winner):
        id1 = hyp1.get("id") or hyp1.get("text")
        id2 = hyp2.get("id") or hyp2.get("text")

        K = self.cfg.get("elo_k", 32)
        R1 = 10 ** (self.elo_scores[id1] / 400)
        R2 = 10 ** (self.elo_scores[id2] / 400)
        E1 = R1 / (R1 + R2)
        E2 = R2 / (R1 + R2)

        S1 = 1 if winner == "A" else 0
        S2 = 1 - S1

        self.elo_scores[id1] = max(100, min(2800, self.elo_scores[id1] + K * (S1 - E1)))
        self.elo_scores[id2] = max(100, min(2800, self.elo_scores[id2] + K * (S2 - E2)))

        self.memory.hypotheses.update_elo_rating(id1, self.elo_scores[id1])
        self.memory.hypotheses.update_elo_rating(id2, self.elo_scores[id2])

        self.win_history.append((id1, id2, winner))

        self.logger.log(
            "RankingUpdated",
            {
                "hypothesis_a": id1,
                "hypothesis_b": id2,
                "winner": winner,
                "elo_a": self.elo_scores[id1],
                "elo_b": self.elo_scores[id2],
            },
        )

    def _parse_response(self, response: str) -> Optional[str]:
        """
        Try multiple methods to extract winner from LLM output

        Returns:
            'A' or 'B' based on comparison
        """
        # Try matching structured formats first
        structured_match = re.search(r"better[\s_]?hypothesis[^\w]*([AB12])", response, re.IGNORECASE)
        if structured_match:
            winner_key = structured_match.group(1).upper()
            return "A" if winner_key in ("A", "1") else "B"

        # Try matching natural language statements
        lang_match = re.search(r"(?:prefer|choose|recommend|select)(\s+idea|\s+hypothesis)?[:\s]+([AB12])", response, re.IGNORECASE)
        if lang_match:
            winner_key = lang_match.group(2).upper()
            return "A" if winner_key in ("A", "1") else "B"

        # Try matching conclusion phrases
        conclusion_match = re.search(r"conclude[d]?\s+with\s+better[\s_]idea:\s*(\d)", response, re.IGNORECASE)
        if conclusion_match:
            winner_key = conclusion_match.group(1)
            return "A" if winner_key == "1" else "B"

        # Default fallback logic
        self.logger.log("ParseError", {
                    "error": "Could not extract winner from response",
                    "response": response
                })
        return response---END-OF-FILE---


"co_ai\agents\recovery_parser.py"
---START-OF-FILE---
import json

from co_ai.agents import BaseAgent


class RecoveryParserAgent(BaseAgent):
    def __init__(self, cfg=None, memory=None, logger=None):
        super().__init__(cfg or {}, memory, logger)

    def parse(self, raw_text: str, expected_fields: list[str], regex_hint: str = None) -> dict:
        prompt = self.prompt_loader.load_prompt(self.cfg, {
            "raw_text": raw_text,
            "expected_fields": expected_fields,
            "regex_hint": regex_hint or "None"
        })
        output = self.call_llm(prompt, {})
        try:
            return json.loads(output)
        except Exception as e:
            self.logger and self.logger.log("LLMParseFail", {
                "error": str(e),
                "raw_output": output,
            })
            return {}
---END-OF-FILE---


"co_ai\agents\refiner.py"
---START-OF-FILE---
from co_ai.agents.base import BaseAgent
from co_ai.constants import GOAL, HYPOTHESES, PIPELINE, PIPELINE_RUN_ID
from co_ai.models import HypothesisORM
from co_ai.parsers import extract_hypotheses


class RefinerAgent(BaseAgent):
    async def run(self, context: dict) -> dict:
        goal = self.extract_goal_text(context.get(GOAL))
        target_agent = self.cfg.get("target_agent", "generation")
        history = context.get("prompt_history", {}).get(target_agent, None)

        if not history:
            self.logger.log("RefinerNoHistoryFound", {
                "target_agent": target_agent,
                "context_keys": list(context.keys())
            })
            return context

        self.logger.log("RefinerStart", {
            "target_agent": target_agent,
            "goal": goal
        })

        original_prompt = history["prompt"]
        original_response = history["response"]
        preferences = history.get("preferences", [])
        original_hypotheses = context.get(HYPOTHESES, [])

        merged = {
            **context,
            "input_prompt": original_prompt,
            "example_output": original_response,
            "preferences": preferences,
        }

        try:
            prompt_improved_prompt = self.prompt_loader.load_prompt(
                self.cfg, context=merged
            )
            self.logger.log("RefinerImprovementPromptLoaded", {
                "snippet": prompt_improved_prompt[:200]
            })

            refined_prompt = self.call_llm(prompt_improved_prompt, context)
            self.logger.log("RefinerPromptGenerated", {
                "prompt_snippet": refined_prompt[:200]
            })

            refined_response = self.call_llm(refined_prompt, context)
            self.logger.log("RefinerResponseGenerated", {
                "response_snippet": refined_response[:200]
            })

            refined_hypotheses = extract_hypotheses(refined_response)
            self.logger.log(
                "RefinerHypothesesExtracted", {"count": len(refined_hypotheses)}
            )

            for h in refined_hypotheses:
                hyp = HypothesisORM(
                    goal=goal,
                    text=h,
                    prompt=refined_prompt,
                    pipeline_run_id=context.get(PIPELINE_RUN_ID),
                    pipeline_signature=context.get(PIPELINE)
                )
                self.memory.hypotheses.insert(hyp)

            info = {
                "original_response": original_response,
                "original_hypotheses": original_hypotheses,
                "refined_prompt": refined_prompt,
                "refined_hypotheses": refined_hypotheses
            }
            refined_merged = {**merged, **info}

            evaluation_template = self.cfg.get("evaluation_template", "evaluate.txt")
            evaluation_prompt = self.prompt_loader.from_file(
                evaluation_template, self.cfg, refined_merged
            )
            self.logger.log("RefinerEvaluationPromptGenerated", {
                "snippet": evaluation_prompt[:200]
            })

            evaluation_response = self.call_llm(evaluation_prompt, context)
            self.logger.log("RefinerEvaluationResponse", {
                "snippet": evaluation_response[:200]
            })

            if " 2" in evaluation_response:
                context[HYPOTHESES] = refined_hypotheses
                self.logger.log("RefinedUpdated", info)
            else:
                self.logger.log("RefinedSkipped", info)

        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            self.logger.log("RefinerError", {
                "error": str(e),
                "context_keys": list(context.keys())
            })

        return context
---END-OF-FILE---


"co_ai\agents\reflection_delta.py"
---START-OF-FILE---
from dataclasses import asdict
from datetime import datetime

from co_ai.agents.base import BaseAgent
from co_ai.analysis.reflection_delta import compute_pipeline_delta
from co_ai.constants import GOAL
from co_ai.models.reflection_delta import ReflectionDeltaORM


class ReflectionDeltaAgent(BaseAgent):
    async def run(self, context: dict) -> dict:
        goal = self.memory.goals.get_or_create(context.get(GOAL))
        if not goal:
            self.logger.log("ReflectionDeltaSkipped", {"reason": "no goal in context"})
            return context
        runs = self.memory.pipeline_runs.get_by_goal_id(goal.id)

        if len(runs) < 2:
            self.logger.log("ReflectionDeltaSkipped", {
                "goal": goal,
                "reason": "only one or zero runs"
            })
            return context

        logged_deltas = 0
        for i, run_a in enumerate(runs):
            for run_b in runs[i+1:]:
                scores_a = self.memory.scores.get_by_run_id(run_a.run_id)
                scores_b = self.memory.scores.get_by_run_id(run_b.run_id)

                if not scores_a or not scores_b:
                    continue  # skip unscored runs

                delta = compute_pipeline_delta(run_a, run_b, scores_a, scores_b)

                self.memory.reflection_deltas.insert(ReflectionDeltaORM(**delta))
                self.logger.log("ReflectionDeltaLogged", {
                    "goal_id": goal.id,
                    "run_id_a": run_a.run_id,
                    "run_id_b": run_b.run_id,
                    "score_delta": delta.get("score_delta"),
                    "causal": delta.get("causal_improvement")
                })
                logged_deltas += 1

        context["reflection_deltas_logged"] = logged_deltas
        return context
---END-OF-FILE---


"co_ai\agents\reflection.py"
---START-OF-FILE---
from co_ai.agents.base import BaseAgent
from co_ai.agents.mixins.scoring_mixin import ScoringMixin


class ReflectionAgent(ScoringMixin, BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

    async def run(self, context: dict) -> dict:
        hypotheses = self.get_hypotheses(context)

        reflections = []
        for hyp in hypotheses:
            score = self.score_hypothesis(hyp, context, metrics="reflection")
            self.logger.log(
                "ReflectionScoreComputed",
                score,
            )
            reflections.append(score)

        context[self.output_key] = reflections
        return context---END-OF-FILE---


"co_ai\agents\review.py"
---START-OF-FILE---
from co_ai.agents.base import BaseAgent
from co_ai.agents.mixins.scoring_mixin import ScoringMixin


class ReviewAgent(ScoringMixin, BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)

    async def run(self, context: dict) -> dict:
        hypotheses = self.get_hypotheses(context)
        reviews = []

        for hyp in hypotheses:
            # Score and update review
            score = self.score_hypothesis(hyp, context, metrics="review")
            self.logger.log(
                "ReviewScoreComputed",
                score,
            )
            reviews.append(score)

        context[self.output_key] = reviews
        return context---END-OF-FILE---


"co_ai\agents\rule_generator.py"
---START-OF-FILE---
from collections import defaultdict
import statistics

from co_ai.agents.base import BaseAgent
from co_ai.models import PipelineRunORM, ScoreORM, RuleApplicationORM, SymbolicRuleORM
from co_ai.constants import PIPELINE_RUN_ID

class RuleGeneratorAgent(BaseAgent):
    def __init__(self, *args, min_score_threshold=7.5, min_repeat_count=2, **kwargs):
        super().__init__(*args, **kwargs)
        self.min_score_threshold = min_score_threshold
        self.min_repeat_count = min_repeat_count

    async def run(self, context: dict) -> dict:
        self.logger.log("RuleGeneratorStart", {"run_id": context.get(PIPELINE_RUN_ID)})
        new_rules = []

        # Step 1: Get high-scoring runs without rule applications
        high_scores = self._get_high_performance_runs()
        grouped = self._group_by_context_signature(high_scores)

        for sig, entries in grouped.items():
            if len(entries) < self.min_repeat_count:
                continue

            # Check if a rule already exists for this context
            if self.memory.symbolic_rules.exists_by_signature(sig):
                continue

            # Step 2a: Heuristic-based rule suggestion
            rule = self._create_rule_from_signature(sig)
            if rule:
                self.memory.symbolic_rules.insert(rule)
                self.logger.log("HeuristicRuleGenerated", rule.to_dict())
                new_rules.append(rule.to_dict())
            else:
                # Step 2b: LLM fallback
                prompt = self._build_llm_prompt(entries)
                response = self.call_llm(prompt, context)
                self.logger.log("LLMGeneratedRule", {"response": response})
                # Optionally parse/validate this into a SymbolicRuleORM

        context["generated_rules"] = new_rules
        self.logger.log("RuleGeneratorEnd", {"generated_count": len(new_rules)})
        return context

    def _get_high_performance_runs(self):
        scores = self.memory.session.query(ScoreORM).filter(ScoreORM.score >= self.min_score_threshold).all()
        runs = []
        for score in scores:
            rule_app = (
                self.memory.session.query(RuleApplicationORM)
                .filter_by(hypothesis_id=score.hypothesis_id)
                .first()
            )
            if rule_app:
                continue  # Skip if rule already applied
            run = self.memory.session.get(PipelineRunORM, score.pipeline_run_id)
            if run:
                runs.append((score, run))
        return runs

    def _group_by_context_signature(self, scored_runs):
        grouped = defaultdict(list)
        for score, run in scored_runs:
            sig = self._make_signature(run.config)
            grouped[sig].append((score, run))
        return grouped

    def _make_signature(self, config: dict) -> str:
        # Could hash or stringify parts of the config, e.g. model + agent + goal
        model = config.get("model", {}).get("name")
        agent = config.get("agent")
        goal_type = config.get("goal", {}).get("goal_type")
        return f"{model}::{agent}::{goal_type}"

    def _create_rule_from_signature(self, sig: str) -> SymbolicRuleORM:
        try:
            model, agent, goal_type = sig.split("::")
            return SymbolicRuleORM(
                source="rule_generator",
                target="agent",
                filter={"goal_type": goal_type},
                attributes={"model.name": model},
                agent_name=agent,
                context_hash=SymbolicRuleORM.compute_context_hash(
                    {"goal_type": goal_type}, {"model.name": model}
                )
            )
        except Exception as e:
            self.logger.log("SignatureParseError", {"sig": sig, "error": str(e)})
            return None

    def _build_llm_prompt(self, entries: list) -> str:
        examples = "\n\n".join(
            f"Goal: {e[1].config.get('goal', {}).get('goal_text')}\n"
            f"Agent: {e[1].config.get('agent')}\n"
            f"Model: {e[1].config.get('model', {}).get('name')}\n"
            f"Score: {e[0].score}" for e in entries[:3]
        )
        return f"""You are a symbolic AI pipeline optimizer.
Given the following successful pipeline configurations with high scores, suggest a symbolic rule that could be applied to future similar tasks.

Examples:
{examples}

Return a YAML snippet that defines a rule with `target`, `filter`, and `attributes`.
"""
---END-OF-FILE---


"co_ai\agents\rule_refiner.py"
---START-OF-FILE---
import statistics

from co_ai.agents.base import BaseAgent
from co_ai.constants import PIPELINE_RUN_ID
from co_ai.models.rule_application import RuleApplicationORM
from co_ai.models.symbolic_rule import SymbolicRuleORM


class RuleRefinerAgent(BaseAgent):
    def __init__(self, *args, min_applications=3, min_score_threshold=6.5, **kwargs):
        super().__init__(*args, **kwargs)
        self.min_applications = min_applications
        self.min_score_threshold = min_score_threshold

    async def run(self, context: dict) -> dict:
        self.logger.log("RuleRefinerStart", {"run_id": context.get(PIPELINE_RUN_ID)})

        rule_apps = self.memory.session.query(RuleApplicationORM).all()
        grouped = self._group_by_rule(rule_apps)

        for rule_id, applications in grouped.items():
            if len(applications) < self.min_applications:
                continue

            scores = [app.result_score for app in applications if app.result_score is not None]
            if not scores:
                continue

            avg_score = statistics.mean(scores)
            if avg_score >= self.min_score_threshold:
                continue  # Only refine low-performing rules

            rule = self.memory.session.query(SymbolicRuleORM).get(rule_id)
            self.logger.log("LowPerformingRuleFound", {
                "rule_id": rule_id,
                "applications": len(scores),
                "avg_score": avg_score
            })

            refinement_prompt = self._build_prompt(rule, scores, applications)
            response = self.call_llm(refinement_prompt, context)
            self.logger.log("RefinementSuggestion", {
                "rule_id": rule_id,
                "suggestion": response.strip()
            })

        self.logger.log("RuleRefinerEnd", {"run_id": context.get(PIPELINE_RUN_ID)})
        return context

    def _group_by_rule(self, rule_apps):
        grouped = {}
        for app in rule_apps:
            grouped.setdefault(app.rule_id, []).append(app)
        return grouped

    def _build_prompt(self, rule: SymbolicRuleORM, scores: list, applications: list) -> str:
        attributes_str = str(rule.attributes) if rule.attributes else "{}"
        filter_str = str(rule.filter) if rule.filter else "{}"
        return f"""You are a symbolic rule optimizer.

The following rule has been applied {len(applications)} times with an average score of {statistics.mean(scores):.2f}.

Filter: {filter_str}
Attributes: {attributes_str}

Here are some example scores: {scores[:5]}

Please suggest improvements to this rule (e.g., modify attributes, adjust filter constraints, or recommend deprecation if not useful). Return only the proposed change."""
---END-OF-FILE---


"co_ai\agents\rule_tuner.py"
---START-OF-FILE---
from collections import defaultdict

from co_ai.agents.base import BaseAgent
from co_ai.analysis.rule_effect_analyzer import RuleEffectAnalyzer
from co_ai.memory.symbolic_rule_store import SymbolicRuleStore
from co_ai.rules import RuleTuner
from co_ai.constants import GOAL, PIPELINE_RUN_ID
from co_ai.models import PipelineRunORM, ScoreORM, RuleApplicationORM, SymbolicRuleORM

class RuleTunerAgent(BaseAgent):
    """
    Analyzes score dimensions from previous pipeline run and adjusts symbolic rule priorities or parameters.
    Also generates new symbolic rules for repeated high-performing configurations.
    """

    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.score_target = cfg.get("score_target", "correctness")  # could be 'overall', 'clarity', etc.
        self.rule_store = SymbolicRuleStore(memory=self.memory, logger=self.logger)
        self.rule_tuner = RuleTuner(memory=self.memory, logger=self.logger)
        self.min_score_threshold = cfg.get("min_score_threshold", 7.5)
        self.min_repeat_count = cfg.get("min_repeat_count", 2)

    async def run(self, context: dict) -> dict:
        run_id = context.get(PIPELINE_RUN_ID)
        goal = context.get(GOAL)

        self.logger.log("RuleTunerAgentStart", {"run_id": run_id, "goal_id": goal.get("id")})

        # Analyze which rules were effective
        analyzer = RuleEffectAnalyzer(session=self.memory.session, logger=self.logger)
        effects = analyzer.analyze(run_id)

        # Score target: e.g. maximize 'correctness' or 'reward'
        best_rules = [rid for rid, data in effects.items() if self.score_target in data.get("dimensions", {})]

        self.logger.log("BestRulesIdentified", {
            "target": self.score_target,
            "count": len(best_rules),
            "examples": best_rules[:5]
        })

        # Tune rule parameters or priorities based on dimension performance
        for rule_id in best_rules:
            result = self.rule_tuner.increase_priority(rule_id)
            self.logger.log("RulePriorityIncreased", {"rule_id": rule_id, "new_priority": result})

        context["rule_tuning"] = {
            "target": self.score_target,
            "top_rules": best_rules
        }

        # Auto-generate rules from high-performing runs without rules
        new_rules = self._generate_rules_from_high_scores()
        context["generated_rules"] = new_rules

        self.logger.log("RuleTunerAgentEnd", {"goal_id": goal.get("id"), "run_id": run_id})
        return context

    def _generate_rules_from_high_scores(self):
        scores = self.memory.session.query(ScoreORM).filter(ScoreORM.score >= self.min_score_threshold).all()
        runs = []
        for score in scores:
            rule_app = (
                self.memory.session.query(RuleApplicationORM)
                .filter_by(hypothesis_id=score.hypothesis_id)
                .first()
            )
            if rule_app:
                continue  # Skip if rule already applied
            run = self.memory.session.get(PipelineRunORM, score.pipeline_run_id)
            if run:
                runs.append((score, run))

        grouped = defaultdict(list)
        for score, run in runs:
            sig = self._make_signature(run.config)
            grouped[sig].append((score, run))

        new_rules = []
        for sig, entries in grouped.items():
            if len(entries) < self.min_repeat_count:
                continue

            if self.memory.symbolic_rules.exists_by_signature(sig):
                continue

            rule = self._create_rule_from_signature(sig)
            if rule:
                self.memory.symbolic_rules.insert(rule)
                self.logger.log("HeuristicRuleGenerated", rule.to_dict())
                new_rules.append(rule.to_dict())

        return new_rules

    def _make_signature(self, config: dict) -> str:
        model = config.get("model", {}).get("name")
        agent = config.get("agent")
        goal_type = config.get("goal", {}).get("goal_type")
        return f"{model}::{agent}::{goal_type}"

    def _create_rule_from_signature(self, sig: str) -> SymbolicRuleORM:
        try:
            model, agent, goal_type = sig.split("::")
            return SymbolicRuleORM(
                source="rule_generator",
                target="agent",
                filter={"goal_type": goal_type},
                attributes={"model.name": model},
                agent_name=agent,
                context_hash=SymbolicRuleORM.compute_context_hash(
                    {"goal_type": goal_type}, {"model.name": model}
                )
            )
        except Exception as e:
            self.logger.log("SignatureParseError", {"sig": sig, "error": str(e)})
            return None
---END-OF-FILE---


"co_ai\agents\score_dimension_analyzer.py"
---START-OF-FILE---
from co_ai.agents.base import BaseAgent
from collections import defaultdict
from co_ai.constants import PIPELINE_RUN_ID

class ScoreDimensionAnalyzerAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.min_count = cfg.get("min_count", 3)

    async def run(self, context: dict):
        pipeline_run_id = context.get(PIPELINE_RUN_ID)
        scores = self.memory.scores.get_by_pipeline_run(pipeline_run_id)

        # Collect dimension scores
        dimension_totals = defaultdict(float)
        dimension_counts = defaultdict(int)

        for score_obj in scores:
            extra_data = score_obj.extra_data or {}
            for k, v in extra_data.items():
                try:
                    v = float(v)
                    dimension_totals[k] += v
                    dimension_counts[k] += 1
                except (ValueError, TypeError):
                    continue

        # Compute averages
        dimension_averages = {
            k: dimension_totals[k] / dimension_counts[k]
            for k in dimension_totals
            if dimension_counts[k] >= self.min_count
        }

        # Log results
        self.logger.log("ScoreDimensionSummary", {
            "pipeline_run_id": pipeline_run_id,
            "dimension_averages": dimension_averages,
            "dimension_counts": dict(dimension_counts),
        })

        # Optionally attach to context for downstream agents
        context["dimension_scores"] = dimension_averages
        return context
---END-OF-FILE---


"co_ai\agents\scorer.py"
---START-OF-FILE---
from co_ai.agents import BaseAgent
from co_ai.models import HypothesisORM
from co_ai.scoring.proximity import ProximityScore


class ScorerAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.weight_proximity = cfg.get("weight_proximity", 0.4)
        self.weight_review = cfg.get("weight_review", 0.3)
        self.weight_llm_judge = cfg.get("weight_llm_judge", 0.2)
        self.weight_elo = cfg.get("weight_elo", 0.1)
        print(f"ScorerAgent initialized with weights: {self.weight_proximity}, {self.weight_review}, {self.weight_llm_judge}, {self.weight_elo}")

    def score(self, hypothesis: HypothesisORM, context: dict) -> float:
        # Extract features (assumes values already normalized 0â€“1)
        s = ProximityScore(self.cfg, self.memory, self.logger)
        proximity = s.get_score(hypothesis, context)
        review = hypothesis.review or 0.0
        llm_judge = hypothesis.reflection or 0.0
        elo = hypothesis.elo_rating or 0.0

        # Weighted score aggregation
        score = (
            self.weight_proximity * proximity +
            self.weight_review * review +
            self.weight_llm_judge * llm_judge +
            self.weight_elo * elo
        )

        return round(score, 4)

    async def run(self, context: dict):
        hypotheses = self.get_hypotheses(context)
        scored = []
        scores = []
        for hypo in hypotheses:
            h = self.memory.hypotheses.get_from_text(hypo)
            score = self.score(h, context)
            scores.append(score)

            self.logger.log("HypothesisScored", {
                "prompt": h.prompt_id,
                "hypothesis": h.text,
                "score": score
            })

            scored.append(hypo)

        context["scored_hypotheses"] = scored
        return context
---END-OF-FILE---


"co_ai\agents\search_orchestrator.py"
---START-OF-FILE---
from co_ai.agents.base import BaseAgent
from co_ai.constants import GOAL
from co_ai.tools import WebSearchTool
from co_ai.tools.arxiv_tool import search_arxiv
from co_ai.tools.cos_sim_tool import get_top_k_similar
from co_ai.tools.huggingface_tool import search_huggingface_datasets
from co_ai.tools.wikipedia_tool import WikipediaTool


class SearchOrchestratorAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.web_search_tool = WebSearchTool(cfg.get("web_search", {}), self.logger)
        self.wikipedia_tool = WikipediaTool(self.memory, self.logger)
        self.max_results = cfg.get("max_results", 5)

    async def run(self, context: dict) -> dict:
        goal = context.get(GOAL)
        queries = context.get("search_queries", [])
        goal_id = goal.get("id")
        results = []

        for query in queries:
            search_query = query.get("goal_text")
            source = self.route_query(goal, search_query)
            try:
                if source == "arxiv":
                    hits = await search_arxiv([search_query])
                elif source == "huggingface":
                    hits = await search_huggingface_datasets([search_query])
                elif source == "wikipedia":
                    hits = self.wikipedia_tool.find_similar(search_query)
                elif source == "web":
                    hits = await self.web_search_tool.search(
                        search_query, max_results=self.max_results
                    )
                else:
                    continue

                enriched_hits = [
                    {
                        "query": search_query,
                        "source": source,
                        "result_type": hit.get("type", "unknown"),
                        "title": hit.get("title", hit.get("name", "")),
                        "summary": hit.get("snippet", hit.get("description", "")),
                        "url": hit.get("url", ""),
                        "goal_id": goal_id,
                        "parent_goal": goal.get("goal_text"),
                        "strategy": goal.get("strategy"),
                        "focus_area": goal.get("focus_area"),
                        "extra_data": {
                            "source_specific": hit
                        }
                    }
                    for hit in hits
                ]

                # Store results in DB
                stored_results = self.memory.search_results.bulk_add_results(enriched_hits)
                results.extend(stored_results)

            except Exception as e:
                self.logger.log(
                    "SearchToolFailed",
                    {"query": search_query, "tool": source, "error": str(e)}
                )

        # Save result IDs or ORM objects back to context
        context["search_result_ids"] = [r.id for r in results]
        context["search_results"] = [r.to_dict() for r in results]
        return context

    def route_query(self, goal, query: str) -> str:
        """
        Decide which source to use based on query content.
        """
        query_lower = query.lower()

        # Try fast metadata path first
        source = self.fast_metadata_routing(goal, query_lower)
        if source:
            return source

        # Fallback to semantic similarity
        return self.semantic_fallback_routing(query)

    def fast_metadata_routing(self, goal, query_lower):
        focus_area = goal.get("focus_area", "").lower()
        goal_type = goal.get("goal_type", "").lower()

        if goal_type == "data_search" or "dataset" in query_lower:
            return "huggingface"
        if goal_type == "model_review" or "model" in query_lower:
            return "arxiv"
        if goal_type == "background" or any(k in query_lower for k in ["overview", "definition"]):
            return "wikipedia"
        if focus_area in ["nlp", "cv", "graph learning"] and "baseline" in query_lower:
            return "arxiv"

        return None

    def semantic_fallback_routing(self, query: str) -> str:
        intent_map = {
            "arxiv": ["find research paper", "latest ML study", "scientific method"],
            "huggingface": ["find dataset", "huggingface model", "nlp corpus"],
            "wikipedia": ["define concept", "what is", "overview of topic"],
            "web": ["general info", "random search", "link to resource"]
        }

        candidates = [(intent, phrase) for intent, phrases in intent_map.items() for phrase in phrases]
        phrases = [p for _, p in candidates]

        top = get_top_k_similar(query, phrases, self.memory, top_k=1)
        best_phrase = top[0][0]

        for intent, phrase in candidates:
            if phrase == best_phrase:
                return intent

        return "web"---END-OF-FILE---


"co_ai\agents\search_result_processing.py"
---START-OF-FILE---
# co_ai/agents/search_result_processing.py
from co_ai.agents.base import BaseAgent
from co_ai.utils.prompt_loader import PromptLoader


class SearchResultProcessingAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.strategy = cfg.get("strategy", "default")
        self.prompt_loader = PromptLoader()
        self.output_key = cfg.get("output_key", "knowledge_base")

    async def run(self, context: dict) -> dict:
        """
        Takes raw search results from SurveyAgent or SearchOrchestratorAgent,
        processes them into structured knowledge.
        """
        goal = context.get("goal")
        raw_results = context.get("search_results", [])

        if not raw_results:
            self.logger.log("NoResultsToProcess", {})
            return context

        processed_results = []

        for result in raw_results:
            # Build prompt context
            prompt_context = {
                "goal_text": goal.get("goal_text"),
                "focus_area": goal.get("focus_area"),
                "goal_type": goal.get("goal_type"),
                "strategy": goal.get("strategy"),
                "preferences": goal.get("preferences", []),
                "title": result.get("title", ""),
                "summary": result.get("summary", ""),
                "source": result.get("source", "")
            }

            # Load prompt template
            prompt = self.prompt_loader.load_prompt(
                self.cfg.get("prompt_file", "prompts/refine_result.j2"),
                prompt_context
            )

            # Call LLM to extract insights
            response = await self.call_llm(prompt)

            try:
                structured = self._parse_refined_result(response)
                processed_results.append(structured)
            except Exception as e:
                self.logger.log("RefinementFailed", {"error": str(e), "raw_response": response})

        # Update context with refined knowledge
        context[self.output_key] = processed_results
        return context

    def _parse_refined_result(self, raw_output: str) -> dict:
        """
        Parse LLM output into structured format (assumes JSON-like structure).
        """
        import json
        return json.loads(raw_output.strip())---END-OF-FILE---


"co_ai\agents\sharpening.py"
---START-OF-FILE---
# co_ai/agents/sharpening.py

from datetime import datetime

from co_ai.agents import BaseAgent
from co_ai.constants import GOAL, PIPELINE, PIPELINE_RUN_ID
from co_ai.evaluator import MRQSelfEvaluator
from co_ai.models import HypothesisORM
from co_ai.models.sharpening_result import SharpeningResultORM


class SharpeningAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.target = cfg.get("target", "generation")
        self.device = cfg.get("device", "cpu")
        self.evaluator = MRQSelfEvaluator(memory, logger, device=self.device)
        self.templates = cfg.get("templates", ["critic"])

    async def run(self, context: dict):
        goal = self.memory.goals.get_or_create(context.get(GOAL))

        self.evaluator.train_from_database(goal=goal, cfg=self.cfg)

        prompts = context.get("prompt_history", {}).get(self.target, [])
        results = []
        for data in prompts:
            if self.cfg.get("mode", "template") == "judge":
                result = self.run_judge_only(data, context)
            elif self.cfg.get("mode", "template") == "compare_mrq":
                result = self.compare_mrq(data, context)
            else:
                result = self.run_selected(data, context)
            results.append(result)
            if self.cfg.get("log_results", False):
                self.log_sharpening_results(
                    goal, data.get("prompt"), data.get("response"), result
                )
        context[self.output_key] = results
        return context

    def run_selected(self, data: dict, context: dict) -> list[dict]:
        goal = context.get(GOAL)
        results = []
        prompt = data.get("prompt")
        examples = self.memory.hypotheses.get_similar(prompt, 3)
        merged = {**context, **{"prompt": prompt, "examples": examples}}

        if prompt:
            for name in self.templates:
                prompt_template = self.prompt_loader.from_file(name, self.cfg, merged)
                sharpened_hypothesis = self.call_llm(prompt_template, merged)
                hypothesis = data.get("response")  # hypotheses result for prompt
                preferred_output, scores = self.evaluator.judge(
                    goal, prompt, hypothesis, sharpened_hypothesis
                )
                value_a = scores["value_a"]
                value_b = scores["value_b"]
                winner = "a" if value_a >= value_b else "b"
                score = max(value_a, value_b)
                score_diff = abs(value_a - value_b)
                improved = winner == "b"
                comparison = "sharpened_better" if improved else "original_better"
                result = {
                    "template": name,
                    "winner": winner,
                    "improved": improved,
                    "comparison": comparison,
                    "score": round(score, 4),
                    "score_diff": round(score_diff, 4),
                    "output": preferred_output,
                    "raw_scores": scores,
                    "sharpened_hypothesis": sharpened_hypothesis,
                    "prompt_template": prompt_template,
                    PIPELINE: context.get(PIPELINE),
                }
                self.save_improved(goal, prompt_template, result, context)
                results.append(result)
        return sorted(results, key=lambda x: x["score"], reverse=True)

    def compare_mrq(self, data: dict, context: dict) -> list[dict]:
        goal = self.extract_goal_text(context.get(GOAL))
        prompt = data.get("prompt")
        hypothesis = data.get("response")

        # For judge-only, use a simple reflection-based transformation (or leave unchanged)
        sharpened_hypothesis = hypothesis  # no change, just self-judging

        _, scores = self.evaluator.judge(prompt, hypothesis, sharpened_hypothesis, context)
        value_a = scores["value_a"]
        value_b = scores["value_b"]
        winner = "a" if value_a >= value_b else "b"
        score = max(value_a, value_b)
        score_diff = abs(value_a - value_b)
        improved = winner == "b"
        comparison = "sharpened_better" if improved else "original_better"

        result = {
            "template": "judge_only",
            "winner": winner,
            "improved": improved,
            "comparison": comparison,
            "score": round(score, 4),
            "score_diff": round(score_diff, 4),
            "output": hypothesis,
            "raw_scores": scores,
            "sharpened_hypothesis": sharpened_hypothesis,
            "prompt_template": None,
            PIPELINE: context.get(PIPELINE),
        }

        if improved:
            self.save_improved(goal, prompt, result, context)

        return [result]

    async def run_judge_only(self, data: dict, context: dict):
        prompt = data.get("prompt")
        examples = self.memory.hypotheses.get_similar(prompt, 3)
        merged = {**context, **{"prompt": prompt, "examples": examples}}
        prompt_template = self.prompt_loader.from_file(
            "self_reward.txt", self.cfg, merged
        )
        response = self.call_llm(prompt_template, context)
        return response

    def save_improved(self, goal, prompt: str, entry: dict, context: dict):
        if entry["improved"] and self.cfg.get("save_improved", True):
            # Save refined prompt (optional â€“ only if different enough)
            new_prompt_id = self.memory.prompt.save(
                goal=goal,
                agent_name=f"{self.name}_{entry['template']}",
                prompt_key="sharpening",
                prompt_text=prompt,
                response=entry["sharpened_hypothesis"],
                strategy=self.cfg.get("strategy", "default"),
                pipeline_run_id=context.get("pipeline_run_id"),
                meta_data={
                    "original_prompt": prompt,
                    "template": entry["template"],
                    "score_improvement": entry["score_diff"],
                },
            )

            self.logger.log(
                "SharpenedGoalSaved",
                {
                    "prompt_text": prompt[:100],
                },
            )
            hyp = HypothesisORM(
                goal=goal,
                text=entry["sharpened_hypothesis"],
                prompt=prompt,
                pipeline_run_id=context.get(PIPELINE_RUN_ID),
                pipeline_signature=entry.get(PIPELINE),
            )
            # Save new hypothesis for that prompt
            self.memory.hypotheses.insert(hyp)

            self.logger.log(
                "SharpenedHypothesisSaved",
                {
                    "prompt_id": new_prompt_id,
                    "text_snippet": entry["sharpened_hypothesis"][:100],
                    "score": entry["score"],
                },
            )

    from datetime import datetime

    def log_sharpening_results(
        self, goal: str, prompt: str, original_output: str, results: list[dict]
    ):
        for entry in results:
            # Create ORM object
            sharpening_result_orm = SharpeningResultORM(
                goal=goal,
                prompt=prompt,
                template=entry["template"],
                original_output=original_output,
                sharpened_output=entry["sharpened_hypothesis"],
                preferred_output=entry["output"],
                winner=entry["winner"],
                improved=entry["improved"],
                comparison=entry["comparison"],
                score_a=entry["raw_scores"]["value_a"],
                score_b=entry["raw_scores"]["value_b"],
                score_diff=entry["score_diff"],
                best_score=entry["score"],
                prompt_template=entry.get("prompt_template"),
                created_at=datetime.utcnow(),
            )

            # Save to DB via memory
            self.memory.mrq.insert_sharpening_result(sharpening_result_orm)

            # Log the event
            self.logger.log(
                "SharpeningResultSaved",
                sharpening_result_orm.to_dict()
            )---END-OF-FILE---


"co_ai\agents\survey.py"
---START-OF-FILE---
# co_ai/agents/survey.py
from co_ai.agents.base import BaseAgent
from co_ai.constants import GOAL


class SurveyAgent(BaseAgent):
    """
    The Survey Agent generates adaptive search queries for literature exploration.
    
    From the paper:
    > 'The Survey Agent deconstructs the research task into multiple keyword combinations'
    > 'It supports two distinct modes: literature review mode and deep research mode'
    > 'Each idea is mapped to testable components before being executed'
    """

    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.max_queries = cfg.get("max_queries", 5)

    async def run(self, context: dict) -> dict:
        goal = context.get(GOAL, {})
        if not goal:
            self.logger.log("NoGoalProvided", {"reason": "survey_agent_skipped"})
            return context

        # Generate new queries based on goal + baseline + preferences
        prompt_context = {
            "goal_text": goal.get("goal_text"),
            "focus_area": goal.get("focus_area"),
            "baseline_method": context.get("baseline_method", ""),
            "preferences": context.get("preferences", ["novelty", "feasibility"]),
            "previous_ideas": context.get("ideas", [])
        }
        merged = {**self.cfg, **prompt_context}

        prompt = self.prompt_loader.load_prompt(self.cfg, merged)


        raw_output = self.call_llm(prompt, context)
        queries = self._parse_query_response(goal, raw_output)

        # Store in context for SearchOrchestratorAgent
        context["search_queries"] = queries
        context["search_strategy"] = self.strategy

        self.logger.log("SurveyQueriesGenerated", {
            "queries": queries,
            "strategy_used": self.strategy,
            "pipeline_stage": context.get("pipeline_stage")
        })

        return context

    def _parse_query_response(self, goal, response: str) -> list:
        """Parse LLM output into clean list of search queries"""
        lines = [line.strip() for line in response.splitlines() if line.strip()]
        if not lines:
            # Fallback strategy
            return [
                f"{goal.get('focus_area')} machine learning",
                f"{goal.get('goal_text')}"
            ]
        return lines[:self.max_queries]

    def expand_queries_to_goals(self, queries: list, base_goal: dict) -> list:
        """
        Convert queries into sub-goals for future pipeline stages
        
        Args:
            queries (list): Generated search strings
            base_goal (dict): Original goal
            
        Returns:
            list: List of structured sub-goals
        """
        return [
            {
                "goal_text": q,
                "parent_goal": base_goal.get("goal_text"),
                "focus_area": base_goal.get("focus_area"),
                "strategy": base_goal.get("strategy"),
                "source": "survey_agent"
            }
            for q in queries
        ]---END-OF-FILE---


"co_ai\agents\symbolic_optimizer.py"
---START-OF-FILE---
from collections import defaultdict
from co_ai.agents import BaseAgent
from co_ai.constants import GOAL, PIPELINE
from co_ai.memory.symbolic_rule_store import SymbolicRuleORM


class SymbolicOptimizerAgent(BaseAgent):
    def __init__(self, cfg, memory=None, logger=None):
        super().__init__(cfg, memory, logger)
        self.score_target = cfg.get("score_target", "correctness")
        self.min_scores = cfg.get("min_scores", 2)

    async def run(self, context: dict) -> dict:
        goal = context.get(GOAL, {})
        goal_type = goal.get("goal_type", "unknown")

        # Step 1: Retrieve score history for this goal type
        score_history = self.memory.scores.get_by_goal_type(goal_type, score_type=self.score_target)

        # Step 2: Analyze pipelines
        best_pipeline = self.find_best_pipeline(score_history)

        if best_pipeline:
            rule_dict = {
                "target": "pipeline",
                "filter": {"goal_type": goal_type},
                "attributes": {"pipeline": best_pipeline},
                "source": "symbolic_optimizer",
            }

            context["symbolic_suggestion"] = rule_dict

            if self.cfg.get("auto_write_rules", False):
                existing = self.memory.symbolic_rules.find_matching_rule(
                    target="pipeline",
                    filter={"goal_type": goal_type},
                    attributes={"pipeline": best_pipeline},
                )
                if not existing:
                    new_rule = SymbolicRuleORM.from_dict(rule_dict)
                    self.memory.symbolic_rules.insert(new_rule)
                    self.logger.log("SymbolicRuleAutoCreated", rule_dict)

            self.logger.log("SymbolicPipelineSuggestion", {
                "goal_type": goal_type,
                "suggested_pipeline": best_pipeline,
                "score_type": self.score_target
            })

        return context

    def find_best_pipeline(self, score_history):
        scores_by_pipeline = defaultdict(list)

        for score in score_history:
            run_id = score.get("run_id")
            if not run_id:
                continue
            pipeline_run = self.memory.pipeline_runs.get_by_run_id(run_id)
            if not pipeline_run or not pipeline_run.pipeline:
                continue

            str_pipeline = str(pipeline_run.pipeline)
            score_val = score.get("score")
            if score_val is not None:
                scores_by_pipeline[str_pipeline].append(score_val)

        # Only keep pipelines with enough data
        pipeline_scores = {
            pipe: sum(vals) / len(vals)
            for pipe, vals in scores_by_pipeline.items()
            if len(vals) >= self.min_scores
        }

        self.logger.log(
            "PipelineScoreSummary",
            {
                "score_type": self.score_target,
                "pipeline_scores": {
                    pipe: round(avg, 4) for pipe, avg in pipeline_scores.items()
                }
            },
        )

        if not pipeline_scores:
            return None

        best = max(pipeline_scores.items(), key=lambda x: x[1])
        return list(best[0])  # convert stringified list back to list
---END-OF-FILE---


"co_ai\analysis\__init__.py"
---START-OF-FILE---
from .rubric_classifier import RubricClassifierMixin
from .rubric_clusterer import RubricClusterer
from .score_analyzer import ScoreAnalyzer

---END-OF-FILE---


"co_ai\analysis\reflection_delta.py"
---START-OF-FILE---
# co_ai/analyzer/reflection_delta.py
from datetime import datetime, timezone
from statistics import mean


def compare_pipeline_runs(memory, goal_id):
    runs = memory.pipeline_runs.get_by_goal_id(goal_id)
    if len(runs) < 2:
        return []

    deltas = []
    for i, run_a in enumerate(runs):
        for run_b in runs[i+1:]:
            scores_a = memory.scores.get_by_run_id(run_a.run_id)
            scores_b = memory.scores.get_by_run_id(run_b.run_id)

            if not scores_a or not scores_b:
                continue  # skip if unscored

            delta = compute_pipeline_delta(run_a, run_b, scores_a, scores_b)
            deltas.append(delta)

    return deltas


def average_score(scores):
    numeric_scores = [s["score"] for s in scores if s.get("score") is not None]
    return round(mean(numeric_scores), 4) if numeric_scores else None

def list_diff(list1, list2):
    return {
        "only_in_a": [x for x in list1 if x not in list2],
        "only_in_b": [x for x in list2 if x not in list1]
    }

def compute_pipeline_delta(run_a, run_b, scores_a, scores_b):
    score_a = average_score(scores_a)
    score_b = average_score(scores_b)

    return {
        "goal_id": run_a.goal_id,
        "run_id_a": run_a.run_id,
        "run_id_b": run_b.run_id,
        "score_a": score_a,
        "score_b": score_b,
        "score_delta": round(score_b - score_a, 4) if score_a is not None and score_b is not None else None,
        "pipeline_a": run_a.pipeline,
        "pipeline_b": run_b.pipeline,
        "pipeline_diff": list_diff(run_a.pipeline, run_b.pipeline),
        "strategy_diff": run_b.strategy != run_a.strategy,
        "model_diff": run_b.model_name != run_a.model_name,
        "rationale_diff": (
            run_a.lookahead_context.get("rationale") if run_a.lookahead_context else None,
            run_b.lookahead_context.get("rationale") if run_b.lookahead_context else None,
        ),
        "created_at": datetime.now(timezone.utc).isoformat(),
    }
---END-OF-FILE---


"co_ai\analysis\rubric_classifier.py"
---START-OF-FILE---
from datetime import datetime, timezone

from co_ai.constants import GOAL
from co_ai.models import PatternStatORM


class RubricClassifierMixin:
    def _load_enabled_rubrics(self, cfg):
        enabled_rubrics = []
        rubrics_cfg = cfg.get("rubrics", [])
        for entry in rubrics_cfg:
            if entry.get("enabled", False):
                enabled_rubrics.append(
                    {
                        "dimension": entry["dimension"],
                        "rubric": entry["rubric"],
                        "options": entry["options"],
                    }
                )
        return enabled_rubrics

    def classify_with_rubrics(self, hypothesis, context, prompt_loader, cfg, logger):
        results = {}
        pattern_file = cfg.get("pattern_prompt_file", "cot_pattern.txt")
        rubrics = self._load_enabled_rubrics(cfg)

        for rubric in rubrics:
            rubric["hypotheses"] = hypothesis.get("text")
            merged = {**context, **rubric}
            prompt_text = prompt_loader.from_file(pattern_file, cfg, merged)
            custom_llm = cfg.get("analysis_model", None)
            result = self.call_llm(prompt_text, merged, custom_llm)
            results[rubric["dimension"]] = result
            logger.log(
                "RubricClassified",
                {
                    "dimension": rubric["dimension"],
                    "rubric": rubric["rubric"],
                    "classification": result,
                },
            )

        return results

    def classify_and_store_patterns(
        self,
        hypothesis,
        context,
        prompt_loader,
        cfg,
        memory,
        logger,
        agent_name,
        score=None,  # Optional numeric score or win count
    ):
        """Classifies rubrics and stores pattern stats for the given hypothesis."""
        pattern = self.classify_with_rubrics(
            hypothesis=hypothesis,
            context=context,
            prompt_loader=prompt_loader,
            cfg=cfg,
            logger=logger,
        )

        goal = context.get(GOAL)
        summarized = self._summarize_pattern(pattern)

        pattern_stats = self.generate_pattern_stats(
            goal, hypothesis, summarized, cfg, agent_name, score
        )
        memory.pattern_stats.insert(pattern_stats)
        logger.log(
            "RubricPatternsStored",
            {"summary": summarized, "goal": goal, "hypothesis": hypothesis},
        )

        context["pattern_stats"] = summarized
        return summarized

    def generate_pattern_stats(
        self,
        goal,
        hypothesis,
        pattern_dict,
        cfg,
        agent_name,
        confidence_score=None,
    ):
        """
        Create PatternStatORM entries for a classified CoT using DB lookup for IDs.
        """
        try:
            # Get or create goal
            goal_id = self.get_goal_id(goal)

            # Get hypothesis by text
            hypothesis_id = self.get_hypothesis_id(hypothesis)
            model_name = cfg.get("model", {}).get("name", "unknown")

            stats = []
            for dimension, label in pattern_dict.items():
                stat = PatternStatORM(
                    goal_id=goal_id,
                    hypothesis_id=hypothesis_id,
                    model_name=model_name,
                    agent_name=agent_name,
                    dimension=dimension,
                    label=label,
                    confidence_score=confidence_score,
                    created_at=datetime.now(timezone.utc).isoformat(),
                )
                stats.append(stat)

            return stats
        except Exception as e:
            print(f"âŒ Failed to generate pattern stats: {e}")
            raise
---END-OF-FILE---


"co_ai\analysis\rubric_clusterer.py"
---START-OF-FILE---
import numpy as np
import pandas as pd
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_similarity


class RubricClusterer:
    def __init__(self, memory):
        """
        Parameters:
        - memory: allos us to call the embedding object.
        """
        self.memory = memory

    def embed_rubrics(self, rubrics):
        """Embed each rubric using the provided embedding function."""
        embedded = []
        for r in rubrics:
            text = r["rubric"]
            vec = self.memory.embedding.get_or_create(text)
            embedded.append({
                "text": text,
                "dimension": r.get("dimension", "Unknown"),
                "vector": vec
            })
        return embedded

    def cluster_rubrics(self, embedded_rubrics, num_clusters=6):
        vectors = np.array([r["vector"] for r in embedded_rubrics])
        clustering = AgglomerativeClustering(n_clusters=num_clusters)
        labels = clustering.fit_predict(vectors)
        for i, label in enumerate(labels):
            embedded_rubrics[i]["cluster"] = int(label)
        return embedded_rubrics

    def summarize_clusters(self, clustered_rubrics):
        """Pick the most central rubric in each cluster as representative."""
        df = pd.DataFrame(clustered_rubrics)
        summaries = []

        for cluster_id in sorted(df["cluster"].unique()):
            items = df[df["cluster"] == cluster_id]
            vectors = np.stack(items["vector"])
            centroid = np.mean(vectors, axis=0)
            sims = cosine_similarity([centroid], vectors)[0]
            best_idx = np.argmax(sims)
            rep = items.iloc[best_idx]

            summaries.append({
                "cluster": int(cluster_id),
                "representative_rubric": rep["text"],
                "dimension": rep["dimension"],
                "count": len(items)
            })

        return summaries
---END-OF-FILE---


"co_ai\analysis\rule_analytics.py"
---START-OF-FILE---
from collections import defaultdict
from typing import Dict, List, Optional

from co_ai.models.rule_application import RuleApplicationORM
from tabulate import tabulate

class RuleAnalytics:
    def __init__(self, db, logger=None):
        self.db = db
        self.logger = logger

    def get_score_summary(self, rule_id: int) -> dict:
        scores = (
            self.db.session.query(RuleApplicationORM.post_score)
            .filter(RuleApplicationORM.rule_id == rule_id)
            .filter(RuleApplicationORM.post_score != None)
            .all()
        )
        values = [s[0] for s in scores]
        if not values:
            return {"average": None, "count": 0}
        return {
            "average": sum(values) / len(values),
            "count": len(values),
            "min": min(values),
            "max": max(values),
        }

    def get_feedback_summary(self, rule_id: int) -> Dict[str, int]:
        results = (
            self.db.session.query(RuleApplicationORM.change_type)
            .filter(RuleApplicationORM.rule_id == rule_id)
            .all()
        )
        summary = defaultdict(int)
        for (label,) in results:
            if label:
                summary[label] += 1
        return dict(summary)

    def compute_rule_rank(
        self,
        score_avg: Optional[float],
        usage_count: int,
        feedback: Dict[str, int]
    ) -> float:
        """Compute a basic rule quality score. Can be replaced with DPO/MRQ later."""
        if score_avg is None:
            return -float("inf")
        bonus = feedback.get("good", 0)
        penalty = feedback.get("bad", 0) * 0.5
        return score_avg + bonus - penalty

    def analyze_all_rules(self) -> List[dict]:
        rules = self.db.symbolic_rules.get_all_rules()
        output = []
        table_rows = []
        for rule in rules:
            score_summary = self.get_score_summary(rule.id)
            feedback_summary = self.get_feedback_summary(rule.id)
            rank = self.compute_rule_rank(
                score_summary.get("average"), score_summary.get("count"), feedback_summary
            )
            result = {
                "rule_id": rule.id,
                "rule_text": rule.rule_text,
                "target": rule.target,
                "attributes": rule.attributes,
                "score_summary": score_summary,
                "feedback_summary": feedback_summary,
                "rank_score": rank,
            }
            output.append(result)

            avg_score = score_summary.get("average") or 0.0
            score_count = score_summary.get("count") or 0
            pos_feedback = feedback_summary.get("positive", 0)
            neg_feedback = feedback_summary.get("negative", 0)

            # Prepare a table row summary for printout
            table_rows.append([
                rule.id,
                rule.target or "â€”",
                rule.rule_text[:30] + "â€¦" if rule.rule_text and len(rule.rule_text) > 30 else rule.rule_text,
                f"{avg_score:.2f}",
                score_count,
                pos_feedback,
                neg_feedback,
                f"{rank:.2f}",
                ])
            output.append(result)

        # Print final table
        print("\nðŸ“‹ Rule Analysis Summary:")
        headers = [
            "Rule ID",
            "Target",
            "Rule Text",
            "Avg Score",
            "Score Count",
            "ðŸ‘ Feedback",
            "ðŸ‘Ž Feedback",
            "Rank Score",
        ]
        print(tabulate(table_rows, headers=headers, tablefmt="fancy_grid"))
        return output
---END-OF-FILE---


"co_ai\analysis\rule_effect_analyzer.py"
---START-OF-FILE---
import json
import math
from collections import defaultdict
from typing import Optional

from sqlalchemy.orm import Session
from tabulate import tabulate

from co_ai.models import (PipelineRunORM, RuleApplicationORM, ScoreORM,
                          ScoreRuleLinkORM)
from tabulate import tabulate

class RuleEffectAnalyzer:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger

    def _compute_stats(self, scores: list[float]) -> dict:
        if not scores:
            return {}

        avg = sum(scores) / len(scores)
        min_score = min(scores)
        max_score = max(scores)
        std = math.sqrt(sum((x - avg) ** 2 for x in scores) / len(scores))
        success_rate = len([s for s in scores if s >= 50]) / len(scores)

        return {
            "avg_score": avg,
            "count": len(scores),
            "min": min_score,
            "max": max_score,
            "std": std,
            "success_rate": success_rate,
        }

    def analyze(self, pipeline_run_id: int) -> dict:
        """
        Analyze rule effectiveness by collecting all scores linked to rule applications.

        Returns:
            dict: rule_id â†’ summary of performance metrics, broken down by param config.
        """
        rule_scores = defaultdict(list)
        param_scores = defaultdict(lambda: defaultdict(list))  # rule_id â†’ param_json â†’ scores

        # Join ScoreRuleLinkORM with RuleApplicationORM to filter on pipeline_run_id
        links = (
            self.session.query(ScoreRuleLinkORM)
            .join(RuleApplicationORM, RuleApplicationORM.id == ScoreRuleLinkORM.rule_application_id)
            .filter(RuleApplicationORM.pipeline_run_id == pipeline_run_id)
            .all()
        )

        for link in links:
            score = self.session.get(ScoreORM, link.score_id)
            rule_app = self.session.get(RuleApplicationORM, link.rule_application_id)

            if not score or not rule_app or score.score is None:
                if self.logger:
                    self.logger.log("SkipScoreLink", {
                        "reason": "missing score or rule_app or score is None",
                        "score_id": getattr(link, "score_id", None),
                        "rule_application_id": getattr(link, "rule_application_id", None),
                    })
                continue

            rule_id = rule_app.rule_id
            rule_scores[rule_id].append(score.score)

            # Normalize stage_details as sorted JSON
            try:
                param_key = json.dumps(rule_app.stage_details or {}, sort_keys=True)
            except Exception as e:
                param_key = "{}"
                if self.logger:
                    self.logger.log("StageDetailsParseError", {
                        "error": str(e),
                        "raw_value": str(rule_app.stage_details),
                    })


            param_scores[rule_id][param_key].append(score.score)

        # Build summary output
        results = {}
        for rule_id, scores in rule_scores.items():
            rule_summary = self._compute_stats(scores)
            results[rule_id] = {
                **rule_summary,
                "by_params": {},
            }

            print(f"\nðŸ“˜ Rule {rule_id} Summary:")
            print(tabulate([
                ["Average Score", f"{rule_summary['avg_score']:.2f}"],
                ["Count", rule_summary["count"]],
                ["Min / Max", f"{rule_summary['min']} / {rule_summary['max']}"],
                ["Std Dev", f"{rule_summary['std']:.2f}"],
                ["Success Rate â‰¥50", f"{rule_summary['success_rate']:.2%}"],
            ], tablefmt="fancy_grid"))

            for param_key, score_list in param_scores[rule_id].items():
                param_summary = self._compute_stats(score_list)
                results[rule_id]["by_params"][param_key] = param_summary

                print(f"\n    ðŸ”§ Param Config: {param_key}")
                print(tabulate([
                    ["Average Score", f"{param_summary['avg_score']:.2f}"],
                    ["Count", param_summary["count"]],
                    ["Min / Max", f"{param_summary['min']} / {param_summary['max']}"],
                    ["Std Dev", f"{param_summary['std']:.2f}"],
                    ["Success Rate â‰¥50", f"{param_summary['success_rate']:.2%}"],
                ], tablefmt="rounded_outline"))
        return results

    def pipeline_run_scores(self, pipeline_run_id: Optional[int] = None, context: dict = None) -> None:
        """
        Generate a summary log showing all scores for a specific pipeline run.

        Args:
            pipeline_run_id (Optional[int]): ID of the pipeline run to inspect.
            context (dict): Optional context containing 'pipeline_run_id' as fallback.
        """
        if pipeline_run_id is None:
            if context and "pipeline_run_id" in context:
                pipeline_run_id = context["pipeline_run_id"]
            else:
                raise ValueError("No pipeline_run_id provided or found in context.")

        pipeline_run = self.session.get(PipelineRunORM, pipeline_run_id)
        if not pipeline_run:
            raise ValueError(f"No pipeline run found with ID {pipeline_run_id}")

        scores = (
            self.session.query(ScoreORM)
            .filter(ScoreORM.pipeline_run_id == pipeline_run_id)
            .all()
        )

        if not scores:
            if self.logger:
                self.logger.log(
                    "PipelineRunScoreSummary",
                    {
                        "pipeline_run_id": pipeline_run_id,
                        "total_scores": 0,
                        "message": "No scores found",
                    },
                )
            return

        table_rows = []
        for score in scores:
            rule_app_link = (
                self.session.query(ScoreRuleLinkORM)
                .filter(ScoreRuleLinkORM.score_id == score.id)
                .first()
            )
            rule_app = (
                self.session.get(RuleApplicationORM, rule_app_link.rule_application_id)
                if rule_app_link
                else None
            )

            row = [
                score.id,
                score.agent_name or "N/A",
                score.model_name or "N/A",
                score.evaluator_name or "N/A",
                score.score_type or "N/A",
                score.score,
                rule_app.rule_id if rule_app else "â€”",
                score.hypothesis_id or "â€”",
            ]
            table_rows.append(row)

        headers = [
            "Score ID",
            "Agent",
            "Model",
            "Evaluator",
            "Type",
            "Value",
            "Rule ID",
            "Hypothesis ID",
        ]

        # Print the table
        print(f"\nðŸ“Š Scores for Pipeline Run {pipeline_run_id}:")
        print(tabulate(table_rows, headers=headers, tablefmt="fancy_grid"))

        if self.logger:
            self.logger.log("PipelineRunScoreSummary", {
                "pipeline_run_id": pipeline_run_id,
                "total_scores": len(scores)
            })
---END-OF-FILE---


"co_ai\analysis\score_analyzer.py"
---START-OF-FILE---
# analysis/score_analyzer.py
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

class ScoreAnalyzer:
    def __init__(self, score_data: pd.DataFrame):
        """
        Expected format:All right
        - 'hypothesis_id': str
        - 'dimension': str
        - 'score': float
        - Optional: 'outcome' (e.g., final ranking, human eval)
        """
        self.df = score_data
        self.pivot = self.df.pivot(index='hypothesis_id', columns='dimension', values='score')

    def describe_scores(self):
        return self.pivot.describe()

    def fit_linear_regression(self, outcome_col: str):
        merged = self.pivot.copy()
        merged[outcome_col] = self.df.drop_duplicates(subset='hypothesis_id').set_index('hypothesis_id')[outcome_col]
        merged = merged.dropna()
        X = merged.drop(columns=[outcome_col])
        y = merged[outcome_col]
        model = LinearRegression().fit(X, y)
        return model, dict(zip(X.columns, model.coef_))

    def perform_pca(self, n_components=2):
        pca = PCA(n_components=n_components)
        components = pca.fit_transform(self.pivot.fillna(0))
        return components, pca.explained_variance_ratio_

    def cluster_outputs(self, n_clusters=3):
        km = KMeans(n_clusters=n_clusters, n_init=10)
        labels = km.fit_predict(self.pivot.fillna(0))
        return labels

    def plot_pca_clusters(self, n_clusters=3):
        components, _ = self.perform_pca()
        labels = self.cluster_outputs(n_clusters=n_clusters)
        plt.scatter(components[:, 0], components[:, 1], c=labels, cmap='tab10')
        plt.xlabel('PC1')
        plt.ylabel('PC2')
        plt.title('PCA of Score Vectors (Colored by Cluster)')
        plt.show()---END-OF-FILE---


"co_ai\analysis\score_evaluator.py"
---START-OF-FILE---
from co_ai.models.score_dimension import ScoreDimensionORM
from co_ai.models.score import ScoreORM
from sqlalchemy.orm import Session
import yaml
from pathlib import Path
from jinja2 import Template
import re
from tabulate import tabulate


class ScoreEvaluator:
    def __init__(self, dimensions, prompt_loader, cfg, logger, memory):
        self.dimensions = dimensions
        self.prompt_loader = prompt_loader
        self.cfg = cfg
        self.logger = logger
        self.memory = memory
        self.output_format = cfg.get("output_format", "simple")  # default fallback

    @classmethod
    def from_db(
        cls, session: Session, stage: str, prompt_loader=None, agent_config=None
    ):
        rows = session.query(ScoreDimensionORM).filter_by(stage=stage).all()
        dimensions = [
            {
                "name": row.name,
                "prompt_template": row.prompt_template,
                "weight": row.weight,
                "parser": cls.get_parser(row.extra_data or {}),
                "file": row.extra_data.get("file") if row.extra_data else None,
            }
            for row in rows
        ]
        return cls(dimensions, prompt_loader=prompt_loader, agent_config=agent_config)

    @classmethod
    def from_file(cls, filepath: str, prompt_loader, cfg, logger, memory):
        with open(Path(filepath), "r") as f:
            data = yaml.safe_load(f)

        # Default to 'simple' if not provided
        output_format = data.get("output_format", "simple")

        dimensions = [
            {
                "name": d["name"],
                "file": d.get("file"),
                "prompt_template": d.get("prompt_template", d.get("file")),  # fallback to file
                "weight": d.get("weight", 1.0),
                "parser": cls.get_parser(d.get("extra_data", {})),
            }
            for d in data["dimensions"]
        ]

        # Ensure the output_format is accessible in instance
        cfg = cfg.copy()
        cfg["output_format"] = output_format

        return cls(
            dimensions=dimensions,
            prompt_loader=prompt_loader,
            cfg=cfg,
            logger=logger,
            memory=memory,
        )

    @staticmethod
    def get_parser(extra_data):
        parser_type = extra_data.get("parser", "numeric")
        if parser_type == "numeric":
            return lambda r: ScoreEvaluator.extract_score_from_last_line(r)
        if parser_type == "numeric_cor":
            return lambda r: ScoreEvaluator.parse_numeric_cor(r)
        
        return lambda r: 0.0

    @staticmethod
    def extract_score_from_last_line(response: str) -> float:
        """
        Looks for a line ending with 'score: <number>' (case-insensitive).
        """
        lines = response.strip().splitlines()
        for line in reversed(lines):
            match = re.search(r"score:\s*(\d+(\.\d+)?)", line.strip(), re.IGNORECASE)
            if match:
                return float(match.group(1))
        return 0.0

    @staticmethod
    def parse_numeric_cor(response: str) -> float:
        """
        Extracts the numeric score from a <answer>All right [[X]]</answer> block.
        Example: <answer>[[3]]</answer> â†’ 3.0
        """
        match = re.search(r"(?:<answer>\s*)?\[\[(\d+(?:\.\d+)?)\]\](?:\s*</answer>)?", response, re.IGNORECASE)
        if not match:
            raise ValueError(f"Could not extract numeric score from CoR-style answer: {response}")
        return float(match.group(1))

    def evaluate(self, hypothesis: dict, context: dict = {}, llm_fn=None):
        if self.output_format == "cor":
            return self._evaluate_cor(hypothesis=hypothesis, context=context, llm_fn=llm_fn)
        else:
            return self._evaluate_simple(hypothesis=hypothesis, context=context, llm_fn=llm_fn)

    def _evaluate_cor(self, hypothesis: dict, context: dict = {}, llm_fn=None):
        """
        Evaluate using Chain-of-Rubrics (CoR) format with rubric, eval, and <answer>[[score]]</answer>.
        """
        if llm_fn is None:
            raise ValueError(
                "You must pass a call_llm function (e.g., agent.call_llm) to ScoreEvaluator.evaluate"
            )

        results = {}
        for dim in self.dimensions:
            # Load prompt using prompt_loader and dimension-specific CoR template
            if self.prompt_loader and dim.get("file"):
                prompt = self.prompt_loader.from_file(
                    file_name=dim["file"],
                    config=self.cfg,
                    context={"hypothesis": hypothesis, **context},
                )
            elif dim.get("prompt_template"):
                prompt = Template(dim["prompt_template"]).render(
                    hypothesis=hypothesis, **context
                )
            else:
                raise ValueError(f"No prompt found for dimension {dim['name']}")

            response = llm_fn(prompt, context=context)
            try:
                score = dim["parser"](response)
            except Exception as e:
                self.logger.log("ScoreParseError", {
                    "dimension": dim["name"],
                    "response": response,
                    "error": str(e)
                })
                score = 0.0

            self.logger.log("DimensionEvaluated", {
                "dimension": dim["name"],
                "score": score,
                "response": response
            })

            results[dim["name"]] = {
                "score": score,
                "rationale": response,
                "weight": dim["weight"],
            }

        self.save_score_to_memory(results, hypothesis, context)
        return results


    def _evaluate_simple(self, hypothesis: dict, context: dict = {}, llm_fn=None):
        if llm_fn is None:
            raise ValueError(
                "You must pass a call_llm function (e.g., agent.call_llm) to ScoreEvaluator.evaluate"
            )

        results = {}
        for dim in self.dimensions:
            if self.prompt_loader and dim.get("file"):
                prompt = self.prompt_loader.from_file(
                    file_name=dim["file"],
                    config=self.cfg,
                    context={"hypothesis": hypothesis, **context},
                )
            else:
                prompt = Template(dim["prompt_template"]).render(
                    hypothesis=hypothesis, **context
                )

            response = llm_fn(prompt, context=context)
            score = dim["parser"](response)
            self.logger.log(
                "DimensionEvaluated",
                {"dimension": dim["name"], "score": score, "response": response},
            )
            results[dim["name"]] = {
                "score": score,
                "rationale": response,
                "weight": dim["weight"],
            }
        self.save_score_to_memory(results, hypothesis, context)
        return results

    def save_score_to_memory(self, results, hypothesis, context):
        """Save all dimension scores in one record using unified 'scores' JSON field."""
        goal = context.get("goal")
        pipeline_run_id = context.get("pipeline_run_id")
        hypothesis_id = hypothesis.get("id")

        weighted_score = sum(
            s["score"] * s.get("weight", 1.0) for s in results.values()
        ) / max(sum(s.get("weight", 1.0) for s in results.values()), 1.0)

        scores_json = {
            "stage": self.cfg.get("stage", "review"),
            "dimensions": results,
            "final_score": round(weighted_score, 2),
        }

        self.memory.scores.insert(
            ScoreORM(
                goal_id=goal.get("id"), 
                pipeline_run_id=pipeline_run_id,
                hypothesis_id=hypothesis_id,
                agent_name=self.cfg.get("name"),
                model_name=self.cfg.get("model", {}).get("name"),
                evaluator_name=self.cfg.get("evaluator", "ScoreEvaluator"),
                strategy=self.cfg.get("strategy"),
                reasoning_strategy=self.cfg.get("reasoning_strategy"),
                scores=scores_json,
                extra_data={"source": "ScoreEvaluator"},
            )
        )

        self.logger.log("ScoreSavedToMemory", {
            "goal_id": goal.get("id"),
            "hypothesis_id": hypothesis_id,
            "scores": scores_json,
        })

        self.display_results(results, weighted_score)


    def display_results(self, results, weighted_score):
        table_data = [
            [dim_name, f"{dim_data['score']:.2f}", dim_data['weight'], dim_data['rationale'][:60]]
            for dim_name, dim_data in results.items()
        ]
        table_data.append(["FINAL", f"{weighted_score:.2f}", "-", "Weighted average"])

        print("\nðŸ“Š Dimension Scores Summary")
        print(tabulate(
            table_data,
            headers=["Dimension", "Score", "Weight", "Rationale (preview)"],
            tablefmt="fancy_grid"
        ))
---END-OF-FILE---


"co_ai\compiler\passes\strategy_mutation_pass.py"
---START-OF-FILE---
# co_ai/compiler/passes/strategy_mutation_pass.py

class StrategyMutationPass:
    def __init__(self, cfg:dict, compiler=None, evaluator=None, logger=None):
        self.cfg = cfg    
        self.compiler = compiler
        self.evaluator = evaluator
        self.logger = logger

    def apply(self, base_prompt: str, metadata: dict) -> list[dict]:
        """Generate and score prompt mutations using the evaluator."""
        mutations = []

        # Example symbolic mutations (could be expanded)
        candidates = [
            base_prompt.replace("Let's think step by step.", "Let's work through this carefully."),
            base_prompt + "\nProvide a rationale before giving your final answer.",
            base_prompt.replace("explain", "analyze"),
        ]

        for variant in candidates:
            try:
                score = self.evaluator.evaluate(variant, metadata=metadata) if self.evaluator else 0
                mutations.append({"prompt": variant, "score": score})
            except Exception as e:
                if self.logger:
                    self.logger.log("StrategyMutationEvalError", {
                        "error": str(e),
                        "prompt_snippet": variant[:100],
                    })

        # Optionally sort by score descending
        return sorted(mutations, key=lambda x: x["score"], reverse=True)
---END-OF-FILE---


"co_ai\compiler\llm_compiler.py"
---START-OF-FILE---
from jinja2 import Environment, BaseLoader
from co_ai.models.prompt_program import PromptProgramORM

class LLMCompiler:
    def __init__(self, llm, evaluator=None, logger=None):
        """
        llm: callable that takes prompt_text and returns a response
        evaluator: optional scoring function (e.g., MR.Q or LLM judge)
        logger: optional logging tool
        """
        self.llm = llm
        self.evaluator = evaluator
        self.logger = logger
        self.jinja_env = Environment(loader=BaseLoader())

    def render_prompt(self, program: PromptProgramORM) -> str:
        try:
            template = self.jinja_env.from_string(program.template)
            rendered = template.render(**program.inputs)
            program.prompt_text = rendered
            return rendered
        except Exception as e:
            if self.logger:
                self.logger.log("PromptRenderError", {"error": str(e), "template": program.template})
            raise

    def execute(self, program: PromptProgramORM, context: dict = {}) -> PromptProgramORM:
        try:
            # Step 1: Render prompt
            prompt = self.render_prompt(program)

            # Step 2: Call LLM
            response = self.llm(prompt)
            program.hypothesis = response
            program.execution_trace = response  # raw output; extend if needed

            # Step 3: Score hypothesis (optional)
            if self.evaluator:
                score_result = self.evaluator.evaluate(program, context=context)
                program.score = score_result.score
                program.rationale = score_result.reason

            if self.logger:
                self.logger.log("PromptProgramExecuted", {
                    "program_id": program.id,
                    "score": program.score,
                    "rationale_snippet": program.rationale[:100] if program.rationale else None,
                })

            return program

        except Exception as e:
            if self.logger:
                self.logger.log("PromptExecutionError", {"error": str(e)})
            raise
---END-OF-FILE---


"co_ai\compiler\prompt_evaluator.py"
---START-OF-FILE---
from abc import ABC, abstractmethod

class EvaluationResult:
    def __init__(self, score: float, reason: str):
        self.score = score
        self.reason = reason

class BasePromptEvaluator(ABC):
    @abstractmethod
    def evaluate(self, program, context: dict = None) -> EvaluationResult:
        pass


class MRQPromptEvaluator(BasePromptEvaluator):
    def __init__(self, llm, prompt_loader, logger=None):
        self.llm = llm
        self.prompt_loader = prompt_loader
        self.logger = logger

    def evaluate(self, program, context: dict = None) -> EvaluationResult:
        context = context or {}
        try:
            evaluation_context = {
                **context,
                "goal": program.goal,
                "prompt": program.prompt_text,
                "hypothesis": program.hypothesis,
            }
            prompt = self.prompt_loader.load_prompt("prompt_evaluation", evaluation_context)
            response = self.llm(prompt)

            # Very basic scoring extraction
            import re
            match = re.search(r"score:(\d+(\.\d+)?)", response)
            score = float(match.group(1)) if match else 0.0

            return EvaluationResult(score=score, reason=response)

        except Exception as e:
            if self.logger:
                self.logger.log("PromptEvaluationFailed", {"error": str(e)})
            return EvaluationResult(score=0.0, reason=str(e))
---END-OF-FILE---


"co_ai\compiler\prompt_evolver.py"
---START-OF-FILE---
from co_ai.compiler.prompt_mutator import PromptMutator
from co_ai.compiler.llm_compiler import LLMCompiler
from co_ai.compiler.prompt_evaluator import EvaluationResult
from co_ai.compiler.passes.strategy_mutation_pass import StrategyMutationPass
from co_ai.models.prompt_program import PromptProgramORM
from co_ai.evaluator.evaluator_loader import get_evaluator
import dspy
from dspy import BootstrapFewShot, Predict, Example
from co_ai.compiler.prompt_tuning_signature import PromptTuningSignature


class PromptEvolver:
    def __init__(self, llm, logger=None, use_strategy_mutation=False, evaluator_cfg=None, memory=None):
        self.llm = llm
        self.logger = logger
        self.use_strategy_mutation = use_strategy_mutation
        dspy.configure(lm=self.llm)

        self.compiler = LLMCompiler(llm=self.llm, logger=self.logger)
        if self.use_strategy_mutation:
            self.strategy_pass = StrategyMutationPass(compiler=self.compiler, logger=self.logger)

        self.evaluator = None
        if evaluator_cfg:
            self.evaluator = get_evaluator(evaluator_cfg, memory=memory, llm=llm, logger=logger)

    def evolve(self, examples: list[dict], context: dict = {}, sample_size: int = 10) -> list[str]:
        """
        Use DSPy to tune prompts based on performance signals.
        Optionally use symbolic strategy mutation.
        Returns a list of refined prompt strings.
        """
        if not examples:
            return []

        training_set = [
            Example(
                goal=ex["goal"],
                input_prompt=ex["prompt_text"],
                hypotheses=ex["hypothesis_text"],
                review=ex.get("review", ""),
                score=ex.get("elo_rating", 1000),
            ).with_inputs("goal", "input_prompt", "hypotheses", "review", "score")
            for ex in examples[:sample_size]
        ]

        def fallback_metric(example, pred, trace=None):
            return 1.0  # fallback metric for training

        tuner = BootstrapFewShot(metric=fallback_metric)
        student = Predict(PromptTuningSignature)
        tuned_program = tuner.compile(student=student, trainset=training_set)

        refined_prompts = []

        # Use DSPy tuned program
        for ex in examples[sample_size:]:
            try:
                result = tuned_program(
                    goal=ex["goal"],
                    input_prompt=ex["prompt_text"],
                    hypotheses=ex["hypothesis_text"],
                    review=ex.get("review", ""),
                    score=ex.get("elo_rating", 1000),
                )
                refined = result.refined_prompt.strip()
                refined_prompts.append(refined)
            except Exception as e:
                if self.logger:
                    self.logger.log("DSPyPromptEvolutionFailed", {"error": str(e)})

        # Optionally add symbolic strategy mutations
        if self.use_strategy_mutation:
            for ex in examples:
                base_prompt = ex["prompt_text"]
                metadata = {
                    "goal": ex["goal"],
                    "hypotheses": ex.get("hypothesis_text", ""),
                    "review": ex.get("review", ""),
                    "score": ex.get("elo_rating", 1000),
                }
                try:
                    mutations = self.strategy_pass.apply(base_prompt, metadata)
                    for mut in mutations:
                        prompt_text = mut["prompt"]
                        score = self.score_prompt(prompt_text, reference_output=metadata["hypotheses"], context=context)
                        if score >= 0:  # optionally apply a score threshold
                            refined_prompts.append(prompt_text)
                except Exception as e:
                    if self.logger:
                        self.logger.log("StrategyMutationFailed", {"error": str(e)})

        return refined_prompts

    def score_prompt(self, prompt: str, reference_output: str = "", context:dict={}) -> float:
        return self.evaluator.score_single(prompt, reference_output, context)
---END-OF-FILE---


"co_ai\compiler\prompt_mutator.py"
---START-OF-FILE---
# co_ai/compiler/prompt_mutator.py

class PromptMutator:
    """
    A utility class to generate mutated versions of a prompt using symbolic or structural transformations.
    """

    def __init__(self, strategies: list[str] = None):
        self.strategies = strategies or [
            "Think step by step.",
            "Take a skeptical perspective.",
            "Consider multiple points of view.",
            "Use a detailed explanation.",
        ]

    def mutate_with_strategies(self, base_prompt: str) -> list[str]:
        """
        Prepend various reasoning strategies to the base prompt.
        """
        return [f"{strategy} {base_prompt}" for strategy in self.strategies]

    def mutate(self, base_prompt: str, metadata: dict = None) -> list[str]:
        """
        Apply one or more symbolic mutations to a prompt.
        """
        return self.mutate_with_strategies(base_prompt)

    def mutate_with_templates(self, base_prompt: str, template_list: list[str]) -> list[str]:
        """
        Apply a custom list of templates where `{prompt}` is replaced with the base prompt.
        """
        return [template.format(prompt=base_prompt) for template in template_list]
---END-OF-FILE---


"co_ai\compiler\prompt_tuning_signature.py"
---START-OF-FILE---
# co_ai/compiler/prompt_tuning_signature.py

import dspy
from dspy import Signature, InputField, OutputField

class PromptTuningSignature(Signature):
    goal = InputField(desc="Scientific research goal or question")
    input_prompt = InputField(desc="Original prompt used to generate hypotheses")
    hypotheses = InputField(desc="Best hypothesis generated")
    review = InputField(desc="Expert review of the hypothesis")
    score = InputField(desc="Numeric score evaluating the hypothesis quality")
    refined_prompt = OutputField(desc="Improved version of the original prompt")
---END-OF-FILE---


"co_ai\data\interfaces.py"
---START-OF-FILE---
from abc import ABC, abstractmethod


class DataSource(ABC):
    @abstractmethod
    def get_training_pairs(self, goal: str, limit: int) -> list[dict]:
        pass

    @abstractmethod
    def get_prompt_examples(self, goal: str, limit: int) -> list[dict]:
        pass
---END-OF-FILE---


"co_ai\dataloaders\__init__.py"
---START-OF-FILE---
from .arm_to_mrq_dpo import ARMDataLoader---END-OF-FILE---


"co_ai\dataloaders\arm_to_mrq_dpo.py"
---START-OF-FILE---
import json
import random
from collections import Counter
from typing import Dict, List, Optional

from datasets import load_dataset

REASONING_FORMATS = {
    "direct": "<Direct>",
    "short_cot": "<Short_CoT>",
    "code": "<Code>",
    "long_cot": "<Long_CoT>"
}

FORMAT_END_TAGS = {
    "direct": "</Direct>",
    "short_cot": "</Short_CoT>",
    "code": "</Code>",
    "long_cot": "</Long_CoT>"
}


class ARMDataLoader:
    def __init__(
        self,
        dataset_name: str = "aqua_rat",
        subset: Optional[str] = None,
        split: str = "train",
        max_samples: int = 500,
        memory=None,
        logger=None,
    ):
        self.dataset_name = dataset_name
        self.subset = subset
        self.split = split
        self.max_samples = max_samples
        self.memory = memory
        self.logger = logger

        # Format tokens
        self.format_tokens = {
            "direct": "<Direct>",
            "short_cot": "<Short_CoT>",
            "code": "<Code>",
            "long_cot": "<Long_CoT>",
        }
        self.format_end_tokens = {
            "direct": "</Direct>",
            "short_cot": "</Short_CoT>",
            "code": "</Code>",
            "long_cot": "</Long_CoT>",
        }

        self._debug_count = 0
        self.dataset = None

    def log(self, event_name: str, payload: dict):
        if self.logger:
            self.logger.log(event_name, payload)
        else:
            print(f"[{event_name}] {json.dumps(payload)}")

    def adapt(self, context: dict):
        """Main method: Load â†’ Convert â†’ Save to Memory"""
        self.log("DatasetLoading", {"name": self.dataset_name, "split": self.split})
        self.load_dataset()
        self.summarize_difficulties()
        self.print_samples_by_difficulty()

        total_samples = len(self.dataset)
        indices = random.sample(
            range(total_samples), min(self.max_samples, total_samples)
        )

        count = 0
        goal_text = context.get("goal").get("goal_text")
        run_id = context.get("run_id")
        for idx in indices:
            sample = self.dataset[idx]
            pairs = self.build_preference_pairs(sample)
            for pair in pairs:
                prompt = pair["prompt"]
                chosen = pair["chosen"]
                rejected = pair["rejected"]
                preferred = pair["preferred_format"]
                fmt_a = self.detect_format(chosen)
                fmt_b = self.detect_format(rejected)
                difficulty = self.detect_difficulty(prompt)
                # Embed everything once
                self._get_or_cache_embedding(prompt)
                self._get_or_cache_embedding(chosen)
                self._get_or_cache_embedding(rejected)

                # Save to database
                try:
                    self.memory.mrq.add_preference_pair(
                        goal=goal_text,
                        prompt=prompt,
                        output_a=chosen,
                        output_b=rejected,
                        preferred=preferred,
                        fmt_a=fmt_a,
                        fmt_b=fmt_b,
                        difficulty=difficulty,
                        run_id=run_id,
                    )
                    count += 1
                except Exception as e:
                    self.log(
                        "PreferencePairSaveError",
                        {
                            "error": str(e),
                            "prompt": prompt[:80],
                            "chosen": chosen[:80],
                            "rejected": rejected[:80],
                        },
                    )

        self.log("PreferencePairsSaved", {"count": count, "goal": "arm_dpo"})
        context["dpo_samples"] = count
        return context

    def _get_or_cache_embedding(self, text: str) -> List[float]:
        """
        Get embedding from cache or compute and store.
        Uses your existing memory.embedding.get_or_create() method.
        """
        emb = self.memory.embedding.get_or_create(text)
        return emb

    def load_dataset(self):
        """Load dataset from Hugging Face."""
        try:
            self.dataset = load_dataset(
                self.dataset_name, self.subset, split=self.split
            )
            self.log("DatasetLoaded", {"count": len(self.dataset)})
        except Exception as e:
            raise RuntimeError(
                f"Failed to load dataset '{self.dataset_name}': {str(e)}"
            )

    def _detect_difficulty(self, question: str) -> str:
        words = question.split()
        if len(words) < 20:
            return "easy"
        elif len(words) < 50:
            return "medium"
        else:
            return "hard"

    def build_preference_pairs(self, sample: Dict) -> List[Dict]:
        """
        Build DPO-style preference pairs by comparing formats.
        Returns list of dicts like:
        {
          'prompt': ...,
          'chosen': ...,
          'rejected': ...,
          'preferred_format': ...,
          'difficulty': ...
        }
        """
        question = sample.get("question", "").strip()
        ground_truth = sample.get("correct", "").strip()
        difficulty = self._detect_difficulty(question)

        # Generate all four reasoning formats
        direct = self.generate_direct(ground_truth)
        short_cot = self.generate_short_cot(question, ground_truth)
        code = self.generate_code(question, ground_truth)
        long_cot = self.generate_long_cot(question, ground_truth)

        format_to_response = {
            "direct": direct,
            "short_cot": short_cot,
            "code": code,
            "long_cot": long_cot,
        }

        # Filter out empty responses
        valid_formats = [
            fmt for fmt, resp in format_to_response.items() if resp.strip()
        ]
        format_to_response = {
            k: v for k, v in format_to_response.items() if k in valid_formats
        }

        # Define which formats are preferred based on difficulty
        if difficulty == "easy":
            preferred_formats = ["direct", "short_cot", "code"]
            non_preferred_formats = ["long_cot"]
        elif difficulty == "hard":
            preferred_formats = ["long_cot", "code"]
            non_preferred_formats = ["direct", "short_cot"]
        else:  # medium or default case
            preferred_formats = ["short_cot", "code"]
            non_preferred_formats = ["direct", "long_cot"]

        # Build all possible pairs
        pairs = []
        for pref in preferred_formats:
            p_resp = format_to_response.get(pref)
            if not p_resp:
                continue
            for non_pref in non_preferred_formats:
                np_resp = format_to_response.get(non_pref)
                if not np_resp:
                    continue
                pairs.append(
                    {
                        "prompt": question,
                        "chosen": p_resp,
                        "rejected": np_resp,
                        "preferred_format": pref,
                        "rejected_format": non_pref,
                        "difficulty": difficulty,
                    }
                )

        return pairs

    def summarize_difficulties(self):
        counts = Counter()
        for sample in self.dataset:
            question = sample.get("question", "")
            detected = self._detect_difficulty(question)
            counts[detected] += 1
        self.log("DifficultySummary", dict(counts))
        return counts

    def print_samples_by_difficulty(self, count_per_level=3):
        buckets = {"easy": [], "medium": [], "hard": []}
        for sample in self.dataset:
            question = sample.get("question", "")
            difficulty = self._detect_difficulty(question)
            if len(buckets[difficulty]) < count_per_level:
                buckets[difficulty].append(question)

        for diff, questions in buckets.items():
            self.log("SampleByDifficulty", {"difficulty": diff, "examples": questions})

    def _detect_difficulty(self, question: str) -> str:
        """Basic heuristic to infer difficulty based on question length."""
        words = question.split()
        if len(words) < 20:
            return "easy"
        elif len(words) < 50:
            return "medium"
        else:
            return "hard"

    def generate_direct(self, answer: str) -> str:
        return f"{self.format_tokens['direct']}The answer is {answer}.{self.format_end_tokens['direct']}"

    def generate_short_cot(self, question: str, answer: str) -> str:
        return (
            f"{self.format_tokens['short_cot']}"
            "Let me think briefly:\n"
            "Step 1: Understand the question.\n"
            "Step 2: Apply basic logic.\n"
            f"Final Answer: {answer}"
            f"{self.format_end_tokens['short_cot']}"
        )

    def generate_code(self, question: str, answer: str) -> str:
        return (
            f"{self.format_tokens['code']}"
            "def solve():\n"
            "    # Placeholder code generated by GPT-4o\n"
            f"    return '{answer}'\n"
            "solve()\n"
            f"# Output: {answer}"
            f"{self.format_end_tokens['code']}"
        )

    def generate_long_cot(self, question: str, answer: str) -> str:
        return (
            f"{self.format_tokens['long_cot']}"
            "Let's analyze this step-by-step:\n\n"
            "1. Read the question carefully.\n"
            "2. Identify key information.\n"
            "3. Consider multiple approaches.\n"
            "4. Evaluate thoroughly.\n"
            "...\n"
            "Reflection: This approach ensures correctness by exploring multiple paths.\n"
            f"Final Answer: {answer}"
            f"{self.format_end_tokens['long_cot']}"
        )

    def detect_difficulty(self, text: str) -> str:
        words = text.split()
        if len(words) < 20:
            return "easy"
        elif len(words) < 50:
            return "medium"
        else:
            return "hard"

    @staticmethod
    def detect_format(text: str) -> str:
        text = text.strip().lower()
        if not text:
            return "unknown"
        if "<direct>" in text:
            return "direct"
        elif "<short_cot>" in text:
            return "short_cot"
        elif "<code>" in text:
            return "code"
        elif "<long_cot>" in text:
            return "long_cot"
        
        # Direct Answer
        if text.startswith("the answer is") or text.startswith("answer:"):
            return "direct"

        # Short CoT
        elif text.startswith("let me think briefly"):
            return "short_cot"

        # Long CoT
        elif text.startswith("let's analyze this step-by-step"):
            return "long_cot"

        # Code
        elif any(kw in text for kw in ["def", "return", "solve()", "print(", "for ", "if "]):
            return "code"

        else:
            print(f"[WARNING] Unknown format:\n{text[:100]}...")
            return "unknown"---END-OF-FILE---


"co_ai\evaluator\__init__.py"
---START-OF-FILE---
from .arm_reassoning_self_evaluator import ARMReasoningSelfEvaluator
from .hypothesis_value_predictor import HypothesisValuePredictor
from .llm_judge_evaluator import LLMJudgeEvaluator
from .mrq_self_evaluator import MRQSelfEvaluator
from .text_encoder import TextEncoder
from .evaluator_loader import get_evaluator
---END-OF-FILE---


"co_ai\evaluator\agreement_checker.py"
---START-OF-FILE---
# evaluators/agreement_checker.py

class EvaluatorAgreementChecker:
    def __init__(self, logger):
        self.logger = logger

    def check(self, mrq_scores: dict, llm_judgement: str, context: dict = None):
        """
        Compares MR.Q and LLM preferred outputs and logs agreement.
        `llm_judgement` should be 'a' or 'b' (or full judgement string if parsed).
        """
        mrq_preference = "a" if mrq_scores["value_a"] >= mrq_scores["value_b"] else "b"

        agreement = mrq_preference == llm_judgement

        self.logger.log("JudgementAgreement", {
            "mrq_preference": mrq_preference,
            "llm_preference": llm_judgement,
            "agreement": agreement,
            "value_a": round(mrq_scores["value_a"], 4),
            "value_b": round(mrq_scores["value_b"], 4),
            "context": context or {}
        })

        return agreement
---END-OF-FILE---


"co_ai\evaluator\arm_reassoning_self_evaluator.py"
---START-OF-FILE---
import json

import torch
import torch.nn.functional as F

from co_ai.dataloaders import ARMDataLoader
from co_ai.evaluator.base import BaseEvaluator
from co_ai.evaluator.hypothesis_value_predictor import HypothesisValuePredictor
from co_ai.evaluator.text_encoder import TextEncoder
from copy import deepcopy


class ARMReasoningSelfEvaluator(BaseEvaluator):
    def __init__(self, cfg, memory, logger):
        self.cfg = cfg
        self.memory = memory
        self.logger = logger
        self.device = cfg.get("device", "cpu")

        self.format_freq = cfg.get(
            "format_freq", {"direct": 1, "short_cot": 1, "code": 1, "long_cot": 1}
        )
        self.format_rewards = cfg.get(
            "format_rewards", {k: [0.5] for k in self.format_freq}
        )

        self.apply_penalty_bonus = cfg.get("apply_penalty_bonus", True)
        self.epsilon = cfg.get("epsilon", 0.1)
        self.kl_penalty_coeff = cfg.get("kl_penalty_coeff", 0.1)

        self.encoder = TextEncoder().to(self.device)
        self.value_predictor = HypothesisValuePredictor(512, 1024).to(self.device)
        self.ref_value_predictor = deepcopy(self.value_predictor)
        self.ref_value_predictor.eval()

    def judge(self, prompt, output_a, output_b, context: dict):
        prompt_emb = torch.tensor(
            self.memory.embedding.get_or_create(prompt), device=self.device
        ).unsqueeze(0)
        output_a_emb = torch.tensor(
            self.memory.embedding.get_or_create(output_a), device=self.device
        ).unsqueeze(0)
        output_b_emb = torch.tensor(
            self.memory.embedding.get_or_create(output_b), device=self.device
        ).unsqueeze(0)

        zsa_a = self.encoder(prompt_emb, output_a_emb)
        zsa_b = self.encoder(prompt_emb, output_b_emb)

        value_a = self.value_predictor(zsa_a).item()
        value_b = self.value_predictor(zsa_b).item()

        preferred_output = output_a if value_a >= value_b else output_b
        scores = {
            "value_a": value_a,
            "value_b": value_b,
            "fmt_a": ARMDataLoader.detect_format(output_a),
            "fmt_b": ARMDataLoader.detect_format(output_b),
        }

        return preferred_output, scores

    def score_single(self, prompt: str, output: str, context) -> float:
        """Minimal ABC-compliant scoring method."""
        prompt_emb = torch.tensor(
            self.memory.embedding.get_or_create(prompt), device=self.device
        ).unsqueeze(0)
        output_emb = torch.tensor(
            self.memory.embedding.get_or_create(output), device=self.device
        ).unsqueeze(0)
        zsa = self.encoder(prompt_emb, output_emb)
        return self.value_predictor(zsa).item()

    def _update_format_stats(self, fmt: str, reward: float):
        """
        Track format usage and average reward per format.

        This enables format-aware reward shaping and prevents format collapse.
        """
        if fmt not in self.format_freq:
            self.format_freq[fmt] = 0
            self.format_rewards[fmt] = []

        self.format_freq[fmt] += 1
        self.format_rewards[fmt].append(reward)

    def train_from_database(self, goal_text: str, cfg: dict):
        limit = cfg.get("limit", 1000)
        epochs = cfg.get("epochs", 20)
        lr = cfg.get("lr", 1e-4)
        batch_size = cfg.get("batch_size", 16)

        samples = self.memory.mrq.get_training_pairs(goal=goal_text, limit=limit)
        if not samples:
            self.logger.log(
                "TrainingError", {"message": "No samples found", "goal": goal_text}
            )
            return

        inputs, labels = [], []
        for item in samples:
            prompt_emb = self.memory.embedding.get_or_create(item["prompt"])
            output_a_emb = self.memory.embedding.get_or_create(item["output_a"])
            output_b_emb = self.memory.embedding.get_or_create(item["output_b"])
            preferred = item["preferred"]

            zsa_a = self.encoder(
                torch.tensor(prompt_emb).unsqueeze(0).to(self.device),
                torch.tensor(output_a_emb).unsqueeze(0).to(self.device),
            )
            zsa_b = self.encoder(
                torch.tensor(prompt_emb).unsqueeze(0).to(self.device),
                torch.tensor(output_b_emb).unsqueeze(0).to(self.device),
            )

            diff = zsa_a - zsa_b if preferred == "a" else zsa_b - zsa_a
            inputs.append(diff.squeeze(0).detach())
            labels.append(torch.tensor([1.0], device=self.device))

        dataset = torch.utils.data.TensorDataset(
            torch.stack(inputs), torch.stack(labels)
        )
        dataloader = torch.utils.data.DataLoader(
            dataset, batch_size=batch_size, shuffle=True
        )

        opt = torch.optim.Adam(self.value_predictor.parameters(), lr=lr)
        self.value_predictor.train()

        for epoch in range(epochs):
            total_loss = 0.0
            for x_batch, y_batch in dataloader:
                preds = self.value_predictor(x_batch)
                loss = -torch.log(torch.sigmoid(preds)).mean()
                opt.zero_grad()
                loss.backward()
                opt.step()
                total_loss += loss.item()

            avg_loss = total_loss / len(dataloader)
            self.logger.log(
                "TrainingEpoch",
                {"epoch": epoch + 1, "avg_loss": avg_loss, "goal": goal_text},
            )

        self.logger.log("TrainingComplete", {"goal": goal_text})

    def score(self, prompt: str, response: str) -> float:
        """Framework-level scoring method with reward shaping."""
        base_score = self.score_single(prompt, response, context={})
        if not self.apply_penalty_bonus:
            return base_score

        token_len = len(response.split())
        fmt = ARMDataLoader.detect_format(response)
        rarity_bonus = 1.0 / (1 + self.format_freq.get(fmt, 1))
        shaped_score = base_score - 0.01 * token_len + rarity_bonus
        self._update_format_stats(fmt, shaped_score)
        return shaped_score

    def _score_response(self, prompt_emb, response_emb):
        """Score a single response using prompt-response encoder + value predictor"""
        zsa = self.encoder(prompt_emb, response_emb)
        return self.value_predictor(zsa), zsa

    def train_from_context(self, context: dict, cfg: dict):
        """
        Trains the value predictor using DPO samples stored in the context.
        Applies format-aware reward shaping and KL penalty.
        """
        dpo_samples = context.get("dpo_samples", [])
        if not dpo_samples:
            self.logger.log(
                "TrainingError", {"message": "No DPO samples found in context."}
            )
            return

        self.logger.log(
            "TrainingStarted", {"sample_count": len(dpo_samples), "config": cfg}
        )

        inputs, labels = [], []

        # Extract preference data
        for item in dpo_samples:
            prompt_emb = self.memory.embedding.get_or_create(item["prompt"])
            output_a_emb = self.memory.embedding.get_or_create(item["chosen"])
            output_b_emb = self.memory.embedding.get_or_create(item["rejected"])

            zsa_a = self.encoder(
                torch.tensor(prompt_emb).unsqueeze(0).to(self.device),
                torch.tensor(output_a_emb).unsqueeze(0).to(self.device),
            )
            zsa_b = self.encoder(
                torch.tensor(prompt_emb).unsqueeze(0).to(self.device),
                torch.tensor(output_b_emb).unsqueeze(0).to(self.device),
            )

            diff = zsa_a - zsa_b if item["preferred_format"] == "a" else zsa_b - zsa_a
            inputs.append(diff.squeeze(0).detach())
            labels.append(torch.tensor([1.0]))

        dataset = torch.utils.data.TensorDataset(
            torch.stack(inputs), torch.stack(labels)
        )
        dataloader = torch.utils.data.DataLoader(
            dataset, batch_size=cfg.get("batch_size", 16), shuffle=True
        )

        opt = optim.Adam(self.value_predictor.parameters(), lr=cfg.get("lr", 1e-4))
        self.value_predictor.train()

        epochs = cfg.get("epochs", 20)
        best_loss = float("inf")
        patience_counter = 0
        patience = cfg.get("patience", 3)

        for epoch in range(epochs):
            total_loss = 0.0
            for x_batch, y_batch in dataloader:
                preds = self.value_predictor(x_batch)
                policy_log_probs = torch.log_softmax(preds, dim=-1)

                with torch.no_grad():
                    ref_preds = self.ref_value_predictor(x_batch)
                    ref_log_probs = torch.log_softmax(ref_preds, dim=-1)

                advantages = policy_log_probs - ref_log_probs
                advantages = (advantages - advantages.mean()) / (
                    advantages.std() + 1e-6
                )

                ratios = torch.exp(policy_log_probs - ref_log_probs)
                clipped_ratios = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon)
                unclipped_loss = ratios * advantages
                clipped_loss = clipped_ratios * advantages

                policy_loss = -torch.min(unclipped_loss, clipped_loss).mean()
                kl = F.kl_div(ref_log_probs, policy_log_probs, reduction="batchmean")
                loss = policy_loss + self.kl_penalty_coeff * kl

                loss.backward()
                opt.step()
                opt.zero_grad()

                total_loss += loss.item()

            avg_loss = total_loss / len(dataloader)
            self.logger.log(
                "TrainingEpoch",
                {
                    "epoch": epoch + 1,
                    "avg_loss": round(avg_loss, 5),
                    "goal": "arm_dpo",
                    "format_usage": self.format_freq.copy(),
                    "format_rewards": {
                        k: round(sum(v) / len(v), 5) if v else 0
                        for k, v in self.format_rewards.items()
                    },
                },
            )

            if avg_loss < best_loss - 0.0001:
                best_loss = avg_loss
                patience_counter = 0
            else:
                patience_counter += 1

            if patience_counter >= patience:
                self.logger.log(
                    "EarlyStopping",
                    {"stopped_epoch": epoch + 1, "best_loss": round(best_loss, 5)},
                )
                break

        self.logger.log(
            "TrainingComplete",
            {"total_epochs": epoch + 1, "final_loss": round(avg_loss, 5)},
        )

    def export_samples_to_json(self, samples: list, output_path: str):
        """
        Exports raw preference pairs to a structured JSON file.

        Each entry includes:
            - Prompt
            - Output A / B
            - Format A / B
            - Preferred side
            - Token lengths
            - Rarity bonuses
            - Difficulty level
        """
        processed = []

        for item in samples:
            prompt = item.get("prompt", "")
            output_a = item.get("output_a", "")
            output_b = item.get("output_b", "")
            preferred = item.get("preferred", "a")

            # Detect format types
            fmt_a = ARMDataLoader.detect_format(output_a)
            fmt_b = ARMDataLoader.detect_format(output_b)

            # Count tokens
            token_len_a = len(output_a.split())
            token_len_b = len(output_b.split())

            # Add rarity bonus
            G = len(samples)
            F_a = (
                sum(
                    1
                    for s in samples
                    if ARMDataLoader.detect_format(s.get("output_a", "")) == fmt_a
                )
                + 1
            )
            F_b = (
                sum(
                    1
                    for s in samples
                    if ARMDataLoader.detect_format(s.get("output_b", "")) == fmt_b
                )
                + 1
            )

            rarity_bonus_a = G / F_a
            rarity_bonus_b = G / F_b

            # Infer difficulty from question length
            words = prompt.split()
            if len(words) < 20:
                difficulty = "easy"
            elif len(words) < 50:
                difficulty = "medium"
            else:
                difficulty = "hard"

            processed.append(
                {
                    "prompt": prompt,
                    "output_a": output_a,
                    "output_b": output_b,
                    "preferred": preferred,
                    "fmt_a": fmt_a,
                    "fmt_b": fmt_b,
                    "token_len_a": token_len_a,
                    "token_len_b": token_len_b,
                    "rarity_bonus_a": round(rarity_bonus_a, 3),
                    "rarity_bonus_b": round(rarity_bonus_b, 3),
                    "difficulty": difficulty,
                }
            )

        with open(output_path, "w") as fp:
            json.dump(processed, fp, indent=2)

        print(f"[INFO] Exported {len(processed)} samples to {output_path}")
---END-OF-FILE---


"co_ai\evaluator\base.py"
---START-OF-FILE---
# co_ai/evaluator/base.py
from abc import ABC, abstractmethod


class BaseEvaluator(ABC):
    @abstractmethod
    def judge(self, prompt, output_a, output_b, context:dict) -> dict:
        pass

    @abstractmethod
    def score_single(self, prompt, output, context:dict) -> float:
        pass

---END-OF-FILE---


"co_ai\evaluator\callibration.py"
---START-OF-FILE---
# evaluators/calibration.py

import torch


class MRQCalibrator:
    @staticmethod
    def calibrated_preference(value_a: float, value_b: float) -> float:
        """
        Returns a probability (0 to 1) that A is preferred over B.
        """
        return torch.sigmoid(torch.tensor(value_a - value_b)).item()

    @staticmethod
    def predicted_preference(value_a: float, value_b: float) -> str:
        """
        Returns 'a' or 'b' based on which value is higher.
        """
        return "a" if value_a >= value_b else "b"
---END-OF-FILE---


"co_ai\evaluator\evaluator_loader.py"
---START-OF-FILE---
# co_ai/evaluator/evaluator_loader.py

def get_evaluator(cfg, memory=None, call_llm=None, logger=None):
    if cfg["judge"] == "llm":
        from co_ai.evaluator.llm_judge_evaluator import LLMJudgeEvaluator

        llm = cfg.get("judge_model", cfg.get("model"))
        prompt_file = cfg.get("judge_prompt_file", "evaluator.txt")
        logger.log(
            "EvaluatorInit", {"strategy": "LLM", "prompt_file": prompt_file}
        )
        return LLMJudgeEvaluator(cfg, llm, prompt_file, call_llm, logger)
    elif cfg["judge"] == "mrq":
        from co_ai.evaluator.mrq_self_evaluator import MRQSelfEvaluator
        return MRQSelfEvaluator(memory=memory, logger=logger)
    else:
        raise ValueError(f"Unknown evaluator type: {cfg['type']}")
---END-OF-FILE---


"co_ai\evaluator\hypothesis_evaluator.py"
---START-OF-FILE---
from co_ai.models import ScoreORM


class HypothesisEvaluator:
    def __init__(self, session):
        self.session = session

    def get_hypothesis_scores(self, hypothesis_id):
        scores = (
            self.session.query(ScoreORM)
            .filter(ScoreORM.hypothesis_id == hypothesis_id)
            .all()
        )
        return {
            score.score_type: score.score
            for score in scores
        }

    def get_composite_score(self, hypothesis_id):
        scores = self.get_hypothesis_scores(hypothesis_id)
        # Example: weighted average
        weights = {
            "review": 0.3,
            "reflection": 0.2,
            "llm_judge": 0.4,
            "elo": 0.1
        }
        total = sum(scores.get(k, 0) * v for k, v in weights.items())
        return round(total, 2)---END-OF-FILE---


"co_ai\evaluator\hypothesis_value_predictor.py"
---START-OF-FILE---
from torch import nn


class HypothesisValuePredictor(nn.Module):
    """Predicts a quality score for a hypothesis given its embedding."""
    def __init__(self, zsa_dim=512, hdim=1024):
        super().__init__()
        self.value_net = nn.Sequential(
            nn.Linear(zsa_dim, hdim),
            nn.ReLU(),
            nn.Linear(hdim, 1)
        )

    def forward(self, zsa_embedding):
        return self.value_net(zsa_embedding)
---END-OF-FILE---


"co_ai\evaluator\llm_judge_evaluator.py"
---START-OF-FILE---
# co_ai/evaluator/llm_judge_evaluator.py

import re

from co_ai.evaluator.base import BaseEvaluator
from co_ai.prompts import PromptLoader


class LLMJudgeEvaluator(BaseEvaluator):
    def __init__(self, cfg, llm_cfg, prompt_file, llm, logger):
        self.cfg = cfg
        self.llm_cfg = llm_cfg
        self.prompt_file = prompt_file
        self.llm = llm  # callable: prompt, context, llm_cfg -> response
        self.logger = logger

    def judge(self, prompt, output_a, output_b, context: dict):
        """
        Compare two outputs using an LLM-based judge.

        Args:
            prompt (str): The name or identifier for the evaluation prompt.
            output_a (str): First hypothesis/output to compare.
            output_b (str): Second hypothesis/output to compare.
            context (dict): Execution context with additional variables.

        Returns:
            tuple: (preferred_output, score_details)
        """

        # Step 1: Merge context with hypotheses and optional notes
        eval_context = {
            **context,
            "hypothesis_a": output_a,
            "hypothesis_b": output_b,
            "comparison_notes": self.cfg.get("comparison_notes", ""),
        }

        # Step 2: Load the evaluation prompt
        prompt_loader = PromptLoader(None, self.logger)
        prompt_text = prompt_loader.from_file(self.prompt_file, self.cfg, eval_context)

        # Step 3: Run the LLM to get a judgement
        raw_response = self.llm(prompt_text, eval_context, llm_cfg=self.llm_cfg)
        cleaned_response = remove_think_blocks(raw_response)
        parsed = parse_response(cleaned_response)

        # Step 4: Determine preferred output and package scores
        preferred_output = output_a if parsed["winner"] == "A" else output_b
        scores = {
            "winner": parsed["winner"],
            "reason": parsed["reason"],
            "score_a": parsed["score_a"],
            "score_b": parsed["score_b"],
        }

        # Step 5: Logging
        self.logger.log(
            "LLMJudgeResult",
            {
                "prompt": prompt,
                "output_a": output_a[:100],
                "output_b": output_b[:100],
                "winner": parsed["winner"],
                "score_a": parsed["score_a"],
                "score_b": parsed["score_b"],
                "reason": parsed["reason"],
                "raw_response": cleaned_response[:300],
            },
        )

        return preferred_output, scores

    def score_single(self, prompt, output, context: dict):
        """
        Compare two outputs using an LLM-based judge.

        Args:
            prompt (str): The name or identifier for the evaluation prompt.
            output_a (str): First hypothesis/output to compare.
            output_b (str): Second hypothesis/output to compare.
            context (dict): Execution context with additional variables.

        Returns:
            tuple: (preferred_output, score_details)
        """

        # Step 1: Merge context with hypotheses and optional notes
        eval_context = {
            **context,
            "hypothesis": output,
            "comparison_notes": self.cfg.get("comparison_notes", ""),
        }

        # Step 2: Load the evaluation prompt
        prompt_loader = PromptLoader(None, self.logger)
        prompt_text = prompt_loader.from_file(self.prompt_file, self.cfg, eval_context)

        # Step 3: Run the LLM to get a judgement
        raw_response = self.llm(prompt_text, eval_context, llm_cfg=self.llm_cfg)
        cleaned_response = remove_think_blocks(raw_response)
        parsed = parse_response(cleaned_response)

        # Step 4: Determine preferred output and package scores
        # Step 5: Logging
        self.logger.log(
            "LLMJudgeSinglwResult",
            {
                "prompt": prompt,
                "output": output[:100],
                "score_a": parsed["score_a"],
                "score_b": parsed["score_b"],
                "reason": parsed["reason"],
                "raw_response": cleaned_response[:300],
            },
        )

        return parsed


def parse_response(response: str):
    # Normalize spacing
    lines = response.strip().splitlines()
    text = "\n".join(
        [line.strip() for line in lines if line.strip()]
    )  # remove extra spaces

    # Flexible matchers
    winner_match = re.search(
        r"better hypothesis[:ï¼š]\s*<?([AB])>?", text, re.IGNORECASE
    )
    reason_match = re.search(
        r"reason[:ï¼š]\s*(.+?)(?=\n(?:score_a|score_b)[:ï¼š])",
        text,
        re.IGNORECASE | re.DOTALL,
    )
    score_a_match = re.search(r"score_a[:ï¼š]\s*<?(\d{1,3})>?", text, re.IGNORECASE)
    score_b_match = re.search(r"score_b[:ï¼š]\s*<?(\d{1,3})>?", text, re.IGNORECASE)

    return {
        "winner": (winner_match.group(1).upper() if winner_match else "A"),
        "reason": (
            reason_match.group(1).strip() if reason_match else "No reason provided."
        ),
        "score_a": int(score_a_match.group(1)) if score_a_match else 0,
        "score_b": int(score_b_match.group(1)) if score_b_match else 0,
    }


def remove_think_blocks(text: str) -> str:
    return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()
---END-OF-FILE---


"co_ai\evaluator\mrq_self_evaluator.py"
---START-OF-FILE---
from dataclasses import asdict

import torch

from co_ai.evaluator.base import BaseEvaluator
from co_ai.evaluator.hypothesis_value_predictor import HypothesisValuePredictor
from co_ai.evaluator.text_encoder import TextEncoder
from co_ai.models.sharpening_prediction import SharpeningPredictionORM


class MRQSelfEvaluator(BaseEvaluator):
    def __init__(self, memory, logger, device="cpu"):
        self.device = device
        self.memory = memory  # memory provides get_embedding
        self.logger = logger
        self.encoder = TextEncoder().to(self.device)
        self.value_predictor = HypothesisValuePredictor(512, 1024).to(self.device)

    def judge(self, goal, prompt, output_a, output_b):
        prompt_emb = torch.tensor(
            self.memory.embedding.get_or_create(prompt), device=self.device
        ).unsqueeze(0)
        output_a_emb = torch.tensor(
            self.memory.embedding.get_or_create(output_a), device=self.device
        ).unsqueeze(0)
        output_b_emb = torch.tensor(
            self.memory.embedding.get_or_create(output_b), device=self.device
        ).unsqueeze(0)

        zsa_a = self.encoder(prompt_emb, output_a_emb)
        zsa_b = self.encoder(prompt_emb, output_b_emb)

        value_a = self.value_predictor(zsa_a).item()
        value_b = self.value_predictor(zsa_b).item()

        preferred_output = output_a if value_a >= value_b else output_b
        scores = {"value_a": value_a, "value_b": value_b}

        if self.memory.mrq.log_evaluations():
            prediction = SharpeningPredictionORM(
                id=None,
                goal_id=-1,
                prompt_text=prompt,
                output_a=output_a,
                output_b=output_b,
                preferred="a" if value_a >= value_b else "b",
                predicted="a" if value_a >= value_b else "b",
                value_a=value_a,
                value_b=value_b,
            )

            self.memory.sharpening.insert_sharpening_prediction(prediction.to_dict(), goal)

        return preferred_output, scores

    def score_single(self, prompt: str, output: str, context:dict) -> float:
        prompt_emb = torch.tensor(self.memory.embedding.get_or_create(prompt), device=self.device).unsqueeze(0)
        output_emb = torch.tensor(self.memory.embedding.get_or_create(output), device=self.device).unsqueeze(0)
        zsa = self.encoder(prompt_emb, output_emb)
        value = self.value_predictor(zsa).item()
        return value

    def train_from_database(self, goal:str, cfg:dict):
        if self.memory is None:
            raise ValueError("Database connectHelloion not provided.")

        limit = cfg.get("limit", 1000)
        epochs = cfg.get("epochs", 20)
        lr = cfg.get("lr", 1e-4)
        patience = cfg.get("patience", 3)
        min_delta = cfg.get("min_delta", 0.0001)

        self.logger.log("MRQTrainingStart", {
            "goal": goal,
            "limit": limit,
            "epochs": epochs,
            "learning_rate": lr,
            "patience": patience,
            "min_delta": min_delta
        })

        samples = self.memory.mrq.get_training_pairs(goal=goal, limit=limit)
        if not samples or len(samples) == 0:
            self.logger.log("MRQTrainingError", {
                "error": "No training samples found for the given goal.",
                "goal": goal
            })
            print("[ERROR] No training samples found. Cannot train MR.Q evaluator.")
            return  # Exit gracefully
        else:
            self.logger.log("MRQTraining", {
                "Sample count": len(samples),
                "goal": goal
            })


        inputs, labels = [], []
        for item in samples:
            prompt_emb = self.memory.embedding.get_or_create(item["prompt"])
            output_a_emb = self.memory.embedding.get_or_create(item["output_a"])
            output_b_emb = self.memory.embedding.get_or_create(item["output_b"])
            preferred = item["preferred"]

            zsa_a = self.encoder(
                torch.tensor(prompt_emb).unsqueeze(0).to(self.device),
                torch.tensor(output_a_emb).unsqueeze(0).to(self.device),
            )
            zsa_b = self.encoder(
                torch.tensor(prompt_emb).unsqueeze(0).to(self.device),
                torch.tensor(output_b_emb).unsqueeze(0).to(self.device),
            )

            diff = zsa_a - zsa_b if preferred == "a" else zsa_b - zsa_a
            inputs.append(diff.squeeze(0).detach())
            labels.append(torch.tensor([1.0], device=self.device))

        dataset = torch.utils.data.TensorDataset(
            torch.stack(inputs), torch.stack(labels)
        )
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)

        opt = torch.optim.Adam(self.value_predictor.parameters(), lr=lr)

        self.value_predictor.train()

        best_loss = float("inf") # early stopping
        epochs_no_improve = 0
        for epoch in range(epochs):
            total_loss = 0.0
            for x_batch, y_batch in dataloader:
                preds = self.value_predictor(x_batch)
                loss = -torch.log(torch.sigmoid(preds)).mean()
                opt.zero_grad()
                loss.backward()
                opt.step()
                total_loss += loss.item()

            avg_loss = total_loss / len(dataloader)
            self.logger.log(
                "MRQTrainingEpoch",
                {"epoch": epoch + 1, "avg_loss": round(avg_loss, 5), "goal": goal},
            )
            # ðŸ›‘ Early stopping logic
            if best_loss - avg_loss > min_delta:
                best_loss = avg_loss
                epochs_no_improve = 0
            else:
                epochs_no_improve += 1
                if epochs_no_improve >= patience:
                    self.logger.log(
                        "MRQEarlyStopping",
                        {
                            "stopped_epoch": epoch + 1,
                            "best_loss": round(best_loss, 5),
                            "goal": goal,
                        },
                    )
                    break

        self.logger.log("MRQTrainingComplete", {
            "goal": goal,
            "epochs": epochs
        })


    def train_from_database(self, goal:str, cfg:dict):
        if self.memory is None:
            raise ValueError("Database connection not provided.")

        limit = cfg.get("limit", 1000)
        epochs = cfg.get("epochs", 20)
        lr = cfg.get("lr", 1e-4)
        patience = cfg.get("patience", 3)
        min_delta = cfg.get("min_delta", 0.0001)

        self.logger.log("MRQTrainingStart", {
            "goal": goal,
            "limit": limit,
            "epochs": epochs,
            "learning_rate": lr,
            "patience": patience,
            "min_delta": min_delta
        })

        samples = self.memory.mrq.get_training_pairs(goal=goal, limit=limit)
        if not samples or len(samples) == 0:
            self.logger.log("MRQTrainingError", {
                "error": "No training samples found for the given goal.",
                "goal": goal
            })
            print("[ERROR] No training samples found. Cannot train MR.Q evaluator.")
            return  # Exit gracefully
        else:
            self.logger.log("MRQTraining", {
                "Sample count": len(samples),
                "goal": goal
            })


        inputs, labels = [], []
        for item in samples:
            prompt_emb = self.memory.embedding.get_or_create(item["prompt"])
            output_a_emb = self.memory.embedding.get_or_create(item["output_a"])
            output_b_emb = self.memory.embedding.get_or_create(item["output_b"])
            preferred = item["preferred"]

            zsa_a = self.encoder(
                torch.tensor(prompt_emb).unsqueeze(0).to(self.device),
                torch.tensor(output_a_emb).unsqueeze(0).to(self.device),
            )
            zsa_b = self.encoder(
                torch.tensor(prompt_emb).unsqueeze(0).to(self.device),
                torch.tensor(output_b_emb).unsqueeze(0).to(self.device),
            )

            diff = zsa_a - zsa_b if preferred == "a" else zsa_b - zsa_a
            inputs.append(diff.squeeze(0).detach())
            labels.append(torch.tensor([1.0], device=self.device))

        dataset = torch.utils.data.TensorDataset(
            torch.stack(inputs), torch.stack(labels)
        )
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)

        opt = torch.optim.Adam(self.value_predictor.parameters(), lr=lr)

        self.value_predictor.train()

        best_loss = float("inf") # early stopping
        epochs_no_improve = 0
        for epoch in range(epochs):
            total_loss = 0.0
            for x_batch, y_batch in dataloader:
                preds = self.value_predictor(x_batch)
                loss = -torch.log(torch.sigmoid(preds)).mean()
                opt.zero_grad()
                loss.backward()
                opt.step()
                total_loss += loss.item()

            avg_loss = total_loss / len(dataloader)
            self.logger.log(
                "MRQTrainingEpoch",
                {"epoch": epoch + 1, "avg_loss": round(avg_loss, 5), "goal": goal},
            )
            # ðŸ›‘ Early stopping logic
            if best_loss - avg_loss > min_delta:
                best_loss = avg_loss
                epochs_no_improve = 0
            else:
                epochs_no_improve += 1
                if epochs_no_improve >= patience:
                    self.logger.log(
                        "MRQEarlyStopping",
                        {
                            "stopped_epoch": epoch + 1,
                            "best_loss": round(best_loss, 5),
                            "goal": goal,
                        },
                    )
                    break

        self.logger.log("MRQTrainingComplete", {
            "goal": goal,
            "epochs": epochs
        })

    def retrieve_similar(self, prompt, k=3):
        """Fetch top-k similar (prompt, output, reward) tuples from memory."""
        return self.memory.prompt.get_similar(prompt, k)

    def train_from_context(self, context: dict, cfg: dict):
        """
        Trains the value predictor using training samples stored in the context.
        Each sample should include: prompt, output_a, output_b, preferred ('a' or 'b').
        """
        samples = context.get("mrq_training_pairs", [])
        if not samples:
            self.logger.log("MRQContextTrainingError", {
                "error": "No training samples found in context."
            })
            return

        limit = cfg.get("limit", 1000)
        epochs = cfg.get("epochs", 20)
        lr = cfg.get("lr", 1e-4)
        patience = cfg.get("patience", 3)
        min_delta = cfg.get("min_delta", 0.0001)

        self.logger.log("MRQContextTrainingStart", {
            "sample_count": len(samples),
            "epochs": epochs,
            "learning_rate": lr,
            "patience": patience,
            "min_delta": min_delta
        })

        inputs, labels = [], []

        for item in samples[:limit]:
            prompt_emb = self.memory.embedding.get_or_create(item["prompt"])
            output_a_emb = self.memory.embedding.get_or_create(item["output_a"])
            output_b_emb = self.memory.embedding.get_or_create(item["output_b"])
            preferred = item["preferred"]

            zsa_a = self.encoder(
                torch.tensor(prompt_emb).unsqueeze(0).to(self.device),
                torch.tensor(output_a_emb).unsqueeze(0).to(self.device),
            )
            zsa_b = self.encoder(
                torch.tensor(prompt_emb).unsqueeze(0).to(self.device),
                torch.tensor(output_b_emb).unsqueeze(0).to(self.device),
            )

            diff = zsa_a - zsa_b if preferred == "a" else zsa_b - zsa_a
            inputs.append(diff.squeeze(0).detach())
            labels.append(torch.tensor([1.0], device=self.device))

        dataset = torch.utils.data.TensorDataset(torch.stack(inputs), torch.stack(labels))
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)

        opt = torch.optim.Adam(self.value_predictor.parameters(), lr=lr)

        self.value_predictor.train()

        best_loss = float("inf")
        epochs_no_improve = 0

        for epoch in range(epochs):
            total_loss = 0.0
            for x_batch, y_batch in dataloader:
                preds = self.value_predictor(x_batch)
                loss = -torch.log(torch.sigmoid(preds)).mean()
                opt.zero_grad()
                loss.backward()
                opt.step()
                total_loss += loss.item()

            avg_loss = total_loss / len(dataloader)
            self.logger.log("MRQContextTrainingEpoch", {
                "epoch": epoch + 1,
                "avg_loss": round(avg_loss, 5)
            })

            if best_loss - avg_loss > min_delta:
                best_loss = avg_loss
                epochs_no_improve = 0
            else:
                epochs_no_improve += 1
                if epochs_no_improve >= patience:
                    self.logger.log("MRQContextEarlyStopping", {
                        "stopped_epoch": epoch + 1,
                        "best_loss": round(best_loss, 5)
                    })
                    break

        self.logger.log("MRQContextTrainingComplete", {
            "epochs_trained": epoch + 1,
            "final_loss": round(avg_loss, 5)
        })
---END-OF-FILE---


"co_ai\evaluator\sae.py"
---START-OF-FILE---
import torch
from torch import nn


# Sparse Autoencoder for embedding analysis and dimensionality reduction
class SparseAutoencoder(nn.Module):
    def __init__(self, input_dim=1024, hidden_dim=64):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim, input_dim),
            nn.Sigmoid()
        )

    def forward(self, x):
        latent = self.encoder(x)
        reconstructed = self.decoder(latent)
        return reconstructed, latent

    def encode(self, x):
        return self.encoder(x)---END-OF-FILE---


"co_ai\evaluator\scores_to_training.py"
---START-OF-FILE---
class MRQScoresToTraining:
    def __init__(self, memory, logger, min_score_diff=0.1):
        self.memory = memory
        self.logger = logger
        self.min_score_diff = min_score_diff

    def get_training_pairs(self, context: dict = None) -> list:
        goal_groups = {}
        scores = self.memory.scores.get_all()

        for score in scores:
            hypothesis = self.memory.hypotheses.get_by_id(score.get("hypothesis_id"))
            if not hypothesis or not hypothesis.prompt or not hypothesis.text:
                continue

            goal_id = hypothesis.goal_id
            goal_groups.setdefault(goal_id, []).append({
                "prompt": hypothesis.prompt.prompt_text,
                "response": hypothesis.text,
                "score": score.get("score"),
                "hypothesis_id": hypothesis.id,
            })

        training_pairs = []
        for goal_id, items in goal_groups.items():
            sorted_items = sorted(items, key=lambda x: x["score"], reverse=True)
            for i in range(len(sorted_items)):
                for j in range(i + 1, len(sorted_items)):
                    hi = sorted_items[i]
                    hj = sorted_items[j]
                    if abs(hi["score"] - hj["score"]) >= self.min_score_diff:
                        training_pairs.append({
                            "prompt": hi["prompt"],
                            "output_a": hi["response"],
                            "output_b": hj["response"],
                            "preferred": "a",
                            "goal_id": goal_id,
                            "hypothesis_a_id": hi["hypothesis_id"],
                            "hypothesis_b_id": hj["hypothesis_id"],
                        })

        if self.logger:
            self.logger.log("MRQPairsGenerated", {"count": len(training_pairs)})

        if context is not None:
            context["mrq_training_pairs"] = training_pairs

        return training_pairs
---END-OF-FILE---


"co_ai\evaluator\scores_training.py"
---START-OF-FILE---
---END-OF-FILE---


"co_ai\evaluator\text_encoder.py"
---START-OF-FILE---
import torch
import torch.nn as nn
import torch.nn.functional as F


# TextEncoder for embedding prompts and hypotheses
class TextEncoder(nn.Module):
    def __init__(self, embedding_dim=1024, zs_dim=512, za_dim=256, zsa_dim=512, hdim=1024):
        super().__init__()
        self.zs_mlp = nn.Sequential(
            nn.Linear(embedding_dim, hdim),
            nn.ReLU(),
            nn.Linear(hdim, zs_dim)
        )
        self.za_mlp = nn.Sequential(
            nn.Linear(embedding_dim, hdim),
            nn.ReLU(),
            nn.Linear(hdim, za_dim)
        )
        self.zsa_mlp = nn.Sequential(
            nn.Linear(zs_dim + za_dim, zsa_dim),
            nn.ReLU(),
            nn.Linear(zsa_dim, zsa_dim)
        )

    def forward(self, prompt_emb, response_emb):
        zs = F.relu(self.zs_mlp(prompt_emb))
        za = F.relu(self.za_mlp(response_emb))
        zsa = self.zsa_mlp(torch.cat([zs, za], dim=1))
        return zsa
---END-OF-FILE---


"co_ai\interface\__init__.py"
---START-OF-FILE---
"""User interaction modules (e.g., CLI input)"""
from .cli import get_user_goal
---END-OF-FILE---


"co_ai\interface\cli.py"
---START-OF-FILE---
# co_ai/interface/cli.py

def get_user_goal() -> str:
    return input("Enter your research goal: ").strip()
---END-OF-FILE---


"co_ai\logs\__init__.py"
---START-OF-FILE---
"""Main logger for app"""
from .icons_enum import get_event_icon
from .json_logger import JSONLogger
---END-OF-FILE---


"co_ai\logs\console_logger.py"
---START-OF-FILE---
import sys
from datetime import datetime, timezone

from co_ai.logs.icons_enum import get_event_icon


class ConsoleLogger:
    def __init__(self, stream=None):
        self.stream = stream or sys.stdout  # default to stdout

    def log(self, event_type: str, data: dict):
        icon = get_event_icon(event_type)
        truncated = str(data)[:100]

        log_entry = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "event_type": event_type,
            "data": data,
        }

        try:
            print(f"{icon} [{event_type}] {truncated}", file=self.stream)
            print(f"ðŸ•’  {log_entry['timestamp']}", file=self.stream)
        except Exception as e:
            print("âŒ [ConsoleLogger] Failed to print log entry.", file=sys.stderr)
            print(f"ðŸ› ï¸  Event Type: {event_type}", file=sys.stderr)
            print(f"ðŸªµ  Error: {e}", file=sys.stderr)
            print(f"ðŸ§±  Data: {repr(data)[:200]}", file=sys.stderr)
---END-OF-FILE---


"co_ai\logs\icons_enum.py"
---START-OF-FILE---
# co_ai/logs/icons.py


def get_event_icon(event_type: str) -> str:
    """
    Get the icon associated with a specific event type.
    """
    return EVENT_ICONS.get(event_type, "â“")


EVENT_ICONS = {
    # General System & Initialization
    "AgentInitialized": "ðŸ› ï¸",
    "ContextLoaded": "ðŸ“‚",
    "ContextSaved": "ðŸ’¾",
    "ContextAfterStage": "ðŸ—ƒï¸",
    "ContextYAMLDumpSaved": "ðŸ“„",
    "debug": "ðŸž",
    "StageContext": "ðŸŽ¯",
    "AgentInit": "ðŸ¤–",
    "EvaluatorInit": "ðŸ§ª",
    "RuleApplicationUpdated": "ðŸ§©",      # Suggests a symbolic piece being modified
    "MRQScoringComplete": "ðŸ“ˆ",          # Indicates successful scoring/completion
    "NoSymbolicAgentRulesApplied": "ðŸš«", # Signifies nothing matched/applied
    "RuleApplicationsScored": "ðŸŽ¯",      # Represents target scoring of rule usage
    "RuleApplicationCount": "ðŸ”¢",        # Suggests counting or tracking quantity
    # Pipeline Execution
    "StoreRegistered": "ðŸ›’",
    "SupervisorInit": "ðŸ§‘â€ðŸ«",
    "PipelineStart": "ðŸ”¬",
    "PipelineStageStart": "ðŸš€",
    "PipelineStageEnd": "ðŸ",
    "PipelineStageSkipped": "â­ï¸",
    "PipelineIterationStart": "ðŸ”„",
    "PipelineIterationEnd": "ðŸ”âœ…ðŸ”š",
    "PipelineRunInserted": "ðŸ”ðŸ—ƒï¸",
    "PipelineSuccess": "âœ…",
    "PipelineError": "âŒ",
    "IterationStart": "ðŸ”„",
    "IterationEnd": "ðŸ”š",
    "AgentRunStarted": "ðŸš€",
    "AgentRunCompleted": "ðŸ",
    "AgentRanSuccessfully": "âœ…",
    "TrainingEpoch": "ðŸ‹ï¸â€â™‚ï¸",
    "EarlyStopping": "â¹ï¸â³",
    "TrainingComplete": "ðŸŽ‰âœ…",
    "SymbolicAgentOverride": "ðŸ› ï¸",
    "RuleApplicationLogged": "ðŸ“œ",
    "ScoreParsed": "ðŸ“Š",
    "SymbolicRulesFound": "ðŸ§©",
    "DuplicateSymbolicRuleSkipped": "â™»ï¸",
    "SymbolicAgentRulesFound": "ðŸ”Ž",
    "PromptLookup": "ðŸ“š",
    "PipelineJudgeAgentStart": "âš–ï¸ðŸš¦",
    "HypothesesReceived": "ðŸ§ ðŸ“¥",
    "PromptLoaded": "ðŸ“",
    "JudgementReceived": "ðŸ—£ï¸",
    "ScoreSaved": "ðŸ’¾",
    "PipelineJudgeAgentEnd": "ðŸ›‘âš–ï¸",
    "PipelineScoreSummary": "ðŸ“ˆðŸ§®",
    "SymbolicPipelineSuggestion": "ðŸ§ ðŸ’¡",

    # Prompt Processing & Tuning
    "Prompt": "ðŸ“œ",
    "PromptGenerated": "ðŸ“",
    "PromptStored": "ðŸ—ƒðŸ—ƒï¸",
    "PromptLogged": "ðŸ§¾",
    "PromptFileNotFound": "ðŸš«",
    "PromptLoadFailed": "â“",
    "PromptParseFailed": "âš ï¸",
    "PromptEvaluationFailed": "âŒ",
    "PromptComparisonResult": "ðŸ",
    "PromptComparisonNoMatch": "ðŸ§ªðŸ“„âŒ",
    "PromptAResponseGenerated": "ðŸ…°ï¸",
    "PromptBResponseGenerated": "ðŸ…±ï¸",
    "PromptABResponseGenerated": "ðŸ…°ï¸",
    "PromptQualityCompareStart": "âš–ï¸",
    "PromptTuningCompleted": "ðŸ§ªâœ¨",
    "PromptTuningSkipped": "â­ï¸",
    "PromptTuningExamples": "ðŸ“š",
    "TunedPromptStored": "ðŸ—ƒï¸",
    "TunedPromptGenerationFailed": "âŒ",
    "ComparisonPromptConstructed": "ðŸ› ï¸",
    "ComparisonResponseReceived": "ðŸ“©",
    "LLMCacheHit": "âœ…",
    "MRQTrainingStart": "So it's very",
    "MRQTrainingEpoch": "ðŸ“ˆ",
    "MRQTrainingComplete": "ðŸ",
    "MRQTraining": "ðŸ“ŠðŸ› ï¸",
    "MRQTrainingDataLoaded": "ðŸ§ ðŸ“¥",
    "MRQPipelineSuggested": "ðŸ§ ðŸ›¤ï¸",
    # goals
    "GoalCreated": "ðŸŽ¯ðŸ’¾",
    # Hypotheses Generation
    "GenerationAgent": "ðŸ§ª",
    "GeneratedHypotheses": "ðŸ’¡",
    "GenerationStart": "âœ¨",
    "GenerationStarted": "ðŸŽ¯",
    "DatasetLoading": "â³ðŸ“¦",
    "DatasetLoaded": "âœ…ðŸ“‚",
    "DPOGenerated": "ðŸ”ðŸ§ ",
    "TrainingStarted": "ðŸš€ðŸ“Š",
    "AdaptiveReasoningResponse": "ðŸ¤–ðŸª„",
    "GenerationCompleted": "âœ…",
    "HypothesisStored": "ðŸ’¾",
    "HypothesisStoreFailed": "âŒ",
    "HypothesisInserted": "ðŸ’¡ðŸ“¥",
    # Hypotheses Evaluation & Ranking
    "RankingAgent": "ðŸ†",
    "RankedHypotheses": "ðŸ…",
    "RankingStored": "ðŸ—ƒï¸",
    "RankingUpdated": "ðŸ”",
    "GoalContextOverride": "ðŸŽ¯",
    "DimensionEvaluated": "ðŸ“",
    "ScoreLinkedToRuleApplications": "ðŸ”—",
    "ScoreSavedToMemory": "ðŸ’¾",
    "HypothesisScoreComputed": "ðŸ§®",
    "NotEnoughHypothesesForRanking": "âš ï¸",
    "LLMJudgeResult": "âš–ï¸",
    "EvaluationCompleted": "ðŸ“Š",
    "ScoreComputed": "ðŸ§®ðŸ“Šâœ…",
    "ReviewScoreComputed": "ðŸ§‘â€âš–ï¸ðŸ“Š",
    "ReflectionScoreComputed": "ðŸªžðŸ“Šâœ…",
    "ScoreStored": "ðŸ’¾",
    # Evolution
    "EvolutionAgent": "ðŸ§¬",
    "EvolvingTopHypotheses": "ðŸ”„",
    "EvolvedHypotheses": "ðŸŒ±",
    "EvolvedParsedHypotheses": "ðŸ§¬",
    "EvolutionCompleted": "ðŸ¦¾",
    "EvolutionError": "âš ï¸",
    "AdaptiveModeDecision": "ðŸ§ âš–ï¸",
    "GraftingPair": "ðŸŒ¿",
    # Review & Reflection
    "ReflectionAgent": "ðŸªž",
    "ReflectionStart": "ðŸ¤”",
    "ReflectionStored": "ðŸ’¾",
    "ReflectionDeltaInserted": "ðŸ§©ðŸ“ˆ",
    "ReflectionDeltaLogged": "ðŸ”ðŸ“",
    "MetaReviewAgent": "ðŸ§ ",
    "MetaReviewInput": "ðŸ“‰",
    "MetaReviewSummary": "ðŸ“˜",
    "RawMetaReviewOutput": "ðŸ“œ",
    "GeneratedReviews": "ðŸ§¾",
    "ReviewStored": "ðŸ’¬",
    "SharpenedHypothesisSaved": "ðŸª“ðŸ’¾",
    "SharpenedGoalSaved": "ðŸª“ðŸ†",
    "IdeaSharpenedAndSaved": "ðŸ’¡ðŸª“ðŸ’¾",
    "SummaryLogged": "ðŸ“",
    "RefinedSkipped": "â­ï¸",
    "RefinedUpdated": "ðŸ”„",
    "CoTGenerated": "ðŸ§ ðŸ”—ðŸ“",
    # Refiner Agent
    "RefinerStart": "ðŸ”„",
    "RefinerPromptGenerated": "ðŸ’¡",
    "RefinerEvaluationPromptGenerated": "ðŸ’¬",
    "RefinerResponseGenerated": "ðŸ’¬",
    "RefinerEvaluationResponse": "ðŸ“Š",
    "RefinerHypothesesExtracted": "ðŸ”",
    "RefinerImprovementPromptLoaded": "ðŸ“œ",
    "RefinerNoHistoryFound": "ðŸš«",
    "RefinerError": "âŒ",
    # Literature & Research
    "LiteratureAgentInit": "ðŸ“š",
    "LiteratureQuery": "ðŸ“š",
    "LiteratureQueryFailed": "ðŸ“šâŒ",
    "LiteratureSearchCompleted": "ðŸ“šâœ…",
    "LiteratureSearchSkipped": "ðŸ“šâ­ï¸",
    "NoResultsFromWebSearch": "ðŸŒðŸš«",
    "ProximityGraphComputed": "ðŸ—ºï¸",
    "SearchQuery": "ðŸ”",
    "SearchingWeb": "ðŸŒ",
    "DatabaseHypothesesMatched": "ðŸ”",
    "SearchResult": "ðŸ”ŽðŸ“„",
    "LLMPromptGenerated_SearchQuery": "ðŸ§ ðŸ”",
    "LLMResponseReceived_SearchQuery": "ðŸ“¥ðŸ”",
    "LLMPromptGenerated_Summarize": "ðŸ§ ðŸ“„",
    "LLMResponseReceived_Summarize": "ðŸ“¥ðŸ“„",
    # Reporting
    "ReportGenerated": "ðŸ“Š",
    "GoalFetchedByText": "ðŸ“„ðŸ”",
    "GoalExists": "âœ”ï¸ðŸ“Œ",
    "BatchProcessingStart": "ðŸ“¥",
    # Rubric Patterns
    "RubricPatternsStored": "ðŸ“šðŸ§©ðŸ’¾",
    "PatternStatsStored": "ðŸ“ŠðŸ§©ðŸ’¾",
    "RubricClassified": "ðŸ“Œ",
    "DifficultySummary": "ðŸ“‹ðŸ§©",
    "SampleByDifficulty": "ðŸ§ªðŸ“š",
    "PreferencePairSaveError": "âŒðŸ’¾",
    "TrainingError": "ðŸ”§ðŸ’¥",
    "ClassificationStarted": "ðŸ”",
    "ClassificationCompleted": "ðŸ“‹",
    # SQL
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "SQLQuery": "ðŸ§®",
}
---END-OF-FILE---


"co_ai\logs\json_logger.py"
---START-OF-FILE---
import json
from datetime import datetime, timezone
from pathlib import Path

from co_ai.logs.icons_enum import get_event_icon


class JSONLogger:

    def __init__(self, log_path="logs/pipeline_log.jsonl"):
        self.log_path = Path(log_path)
        self.log_path.parent.mkdir(parents=True, exist_ok=True)

    def log(self, event_type: str, data: dict):
        icon = get_event_icon(event_type)
        print(f"{icon} [{event_type}] {str(data)[:100]}")

        log_entry = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "event_type": event_type,
            "data": data,
        }

        try:
            with self.log_path.open("a", encoding="utf-8") as f:
                json.dump(log_entry, f, default=str)
                f.write("\n")
        except (TypeError, ValueError) as e:
            print("âŒ [Logger] Failed to serialize log entry.")
            print(f"ðŸ› ï¸  Event Type: {event_type}")
            print(f"ðŸªµ  Error: {e}")
            print(f"ðŸ§±  Data: {repr(data)[:200]}")
---END-OF-FILE---


"co_ai\memory\__init__.py"
---START-OF-FILE---
"""Memory management and embedding tools"""
from .base import BaseStore
from .context_store import ContextStore
from .embedding_store import EmbeddingStore
from .goal_store import GoalStore
from .hypothesis_store import HypothesisStore
from .idea_store import IdeaStore
from .lookahead_store import LookaheadStore
from .memory_tool import MemoryTool
from .pattern_store import PatternStatStore
from .pipeline_run_store import PipelineRunStore
from .prompt_store import PromptStore
from .report_logger import ReportLogger
from .rule_application_store import RuleApplicationStore
from .score_store import ScoreStore
from .search_result_store import SearchResultStore
from .sharpening_store import SharpeningStore
from .symbolic_rule_store import SymbolicRuleStore
---END-OF-FILE---


"co_ai\memory\base.py"
---START-OF-FILE---
# co_ai/memory/base_store.py
from abc import ABC, abstractmethod


class BaseStore(ABC):

    def __init__(self, db, logger=None):
        self.db = db
        self.logger = logger

    @property
    @abstractmethod
    def name(self) -> str:
        pass

    def setup(self):
        """Optional: Setup logic for the store."""
        pass

    def teardown(self):
        """Optional: Cleanup logic for the store."""
        pass
---END-OF-FILE---


"co_ai\memory\context_store.py"
---START-OF-FILE---
# stores/context_store.py
import json
import os
from datetime import datetime, timezone
from typing import Optional

import yaml
from sqlalchemy.orm import Session

from co_ai.models.context_state import ContextStateORM


class ContextStore:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "context"
        self.dump_dir = logger.log_path if logger else None
        if self.dump_dir:
            self.dump_dir = os.path.dirname(self.dump_dir)

    def save(self, run_id: str, stage: str, context: dict, preferences: dict = None, extra_data: dict = None):
        """
        Saves the current pipeline context to database and optionally to disk.
        Increments version and marks it as current for this stage/run.
        """
        try:
            # Deactivate previous versions
            prev_versions = self.session.query(ContextStateORM).filter_by(run_id=run_id, stage_name=stage).all()
            for state in prev_versions:
                state.is_current = False

            # Get latest version number
            latest_version = max((s.version for s in prev_versions), default=0)
            new_version = latest_version + 1

            # Create new context state
            db_context = ContextStateORM(
                run_id=run_id,
                stage_name=stage,
                version=new_version,
                is_current=True,
                context=json.dumps(context),
                preferences=json.dumps(preferences) if preferences else None,
                extra_data=json.dumps(extra_data or {}),
                timestamp=datetime.now(timezone.utc)
            )

            self.session.add(db_context)
            self.session.flush()  # To get ID immediately

            if self.dump_dir:
                self._dump_to_yaml(run_id, stage, context)

            if self.logger:
                self.logger.log("ContextSaved", {
                    "run_id": run_id,
                    "stage": stage,
                    "version": new_version,
                    "timestamp": db_context.timestamp.isoformat(),
                    "is_current": True
                })

        except Exception as e:
            self.session.rollback()
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            if self.logger:
                self.logger.log("ContextSaveFailed", {"error": str(e)})
            raise

    def has_completed(self, run_id: str, stage_name: str) -> bool:
        """Check if this stage has already been run"""
        count = (
            self.session.query(ContextStateORM)
            .filter_by(run_id=run_id, stage_name=stage_name)
            .count()
        )
        return count > 0

    def load(self, run_id: str, stage: Optional[str] = None) -> dict:
        try:
            session = self.session if self.session.is_active else self.sessionmaker()

            if stage:
                states = (
                    session.query(ContextStateORM)
                    .filter_by(run_id=run_id, stage_name=stage)
                    .order_by(ContextStateORM.timestamp.asc())
                    .all()
                )
            else:
                states = session.query(ContextStateORM).filter_by(run_id=run_id).all()

            result = {}
            for state in states:
                result.update(json.loads(state.context))

            return result

        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            self.session.rollback()
            if self.logger:
                self.logger.log("ContextLoadFailed", {"error": str(e)})
            return {}

    def _dump_to_yaml(self, run_id: str, stage: str, context: dict):
        os.makedirs(self.dump_dir, exist_ok=True)
        timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H-%M-%S")
        filename = f"{run_id}_{stage}_{timestamp}.yaml"
        path = os.path.join(self.dump_dir, filename)

        try:
            with open(path, "w", encoding="utf-8") as f:
                yaml.dump(context, f, allow_unicode=True, sort_keys=False)

            if self.logger:
                self.logger.log("ContextYAMLDumpSaved", {"path": path})

        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            if self.logger:
                self.logger.log("ContextYAMLDumpFailed", {"error": str(e)})---END-OF-FILE---


"co_ai\memory\embedding_store.py"
---START-OF-FILE---
import hashlib

from co_ai.memory import BaseStore
from co_ai.tools import get_embedding
from co_ai.utils.lru_cache import SimpleLRUCache


class EmbeddingStore(BaseStore):
    def __init__(self, cfg, conn, db, logger=None, cache_size=10000):
        super().__init__(db, logger)
        self.cfg = cfg
        self.conn = conn
        self.name = "embedding"
        self._cache = SimpleLRUCache(max_size=cache_size)


    def __repr__(self):
        return f"<{self.name} connected={self.db is not None} cfg={self.cfg}>"

    def name(self) -> str:
        return "embedding"

    def get_or_create(self, text: str):
        text_hash = self.get_text_hash(text)

        cached = self._cache.get(text_hash)
        if cached:
            return cached

        try:
            with self.conn.cursor() as cur:
                cur.execute("SELECT embedding FROM embeddings WHERE text_hash  = %s", (text_hash,))
                row = cur.fetchone()
                if row:
                    return row[0]  # Force conversion to list of floats
        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            if self.logger:
                self.logger.log("EmbeddingFetchFailed", {"error": str(e)})

        embedding = get_embedding(text, self.cfg)
        try:
            with self.conn.cursor() as cur:
                cur.execute(
                    """
                    INSERT INTO embeddings (text, text_hash, embedding)
                    VALUES (%s, %s, %s)
                    ON CONFLICT (text_hash) DO NOTHING
                    RETURNING text_hash;
                """,
                    (text, text_hash, embedding),
                )
        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            if self.logger:
                self.logger.log("EmbeddingInsertFailed", {"error": str(e)})
        self._cache.set(text_hash, embedding)
        return embedding


    def search_related(self, query: str, top_k: int = 5):
        try:
            embedding = get_embedding(query, self.cfg)
            with self.conn.cursor() as cur:
                cur.execute(
                    """
                        SELECT 
                            h.text,
                            g.goal_text AS goal,
                            h.confidence,
                            h.review
                        FROM hypotheses h
                        JOIN goals g ON h.goal_id = g.id
                        ORDER BY h.embedding <-> %s
                        LIMIT %s;
                    """,
                    (embedding, top_k)
                )
                results = cur.fetchall()

            if self.logger:
                self.logger.log("HypothesesSearched", {
                    "query": query,
                    "top_k": top_k,
                    "result_count": len(results)
                })

            return results
        except Exception as e:
            if self.logger:
                self.logger.log("HypothesesSearchFailed", {
                    "error": str(e),
                    "query": query
                })
            else:
                print(f"[VectorMemory] Search failed: {e}")
            return []

    def get_text_hash(self, text: str) -> str:
        return hashlib.sha256(text.encode("utf-8")).hexdigest()---END-OF-FILE---


"co_ai\memory\goal_store.py"
---START-OF-FILE---
# stores/goal_store.py
from datetime import datetime, timezone

from sqlalchemy.exc import IntegrityError
from sqlalchemy.orm import Session

from co_ai.models.goal import GoalORM


class GoalStore:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "goals"
    
    def name(self) -> str:
        return "goals"

    def get_from_text(self, goal_text: str):
        return self.session.query(GoalORM).filter(GoalORM.goal_text == goal_text).first()

    def create(self, goal_dict: dict):
        try:
            new_goal = GoalORM(
                goal_text=goal_dict["goal_text"],
                goal_type=goal_dict.get("goal_type"),
                focus_area=goal_dict.get("focus_area"),
                strategy=goal_dict.get("strategy"),
                llm_suggested_strategy=goal_dict.get("llm_suggested_strategy"),
                source=goal_dict.get("source", "user"),
                created_at=goal_dict.get("created_at") or datetime.now(timezone.utc),
            )
            self.session.add(new_goal)
            self.session.commit()
            self.session.refresh(new_goal)

            if self.logger:
                self.logger.log("GoalCreated", {
                    "goal_id": new_goal.id,
                    "goal_text": new_goal.goal_text[:100],
                    "source": new_goal.source
                })

            return new_goal

        except IntegrityError:
            self.session.rollback()
            return self.get_by_text(goal_dict["goal_text"])

        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log("GoalCreateFailed", {"error": str(e)})
            raise

    def get_or_create(self, goal_dict: dict):
        """
        Returns existing goal or creates a new one.
        """
        goal_text = goal_dict.get("goal_text")
        if not goal_text:
            raise ValueError("Missing 'goal_text' in input")

        existing = self.get_from_text(goal_text)
        if existing:
            return existing

        return self.create(goal_dict)
---END-OF-FILE---


"co_ai\memory\hypothesis_store.py"
---START-OF-FILE---
# stores/hypothesis_store.py
from difflib import SequenceMatcher
from typing import Optional

import numpy as np
from sqlalchemy import text
from sqlalchemy.orm import Session

from co_ai.models.goal import GoalORM
from co_ai.models.hypothesis import HypothesisORM


class HypothesisStore:
    def __init__(self, session: Session, logger=None, embedding_store=None):
        self.session = session
        self.logger = logger
        self.embedding_store = embedding_store  # Optional embedding model
        self.name = "hypotheses"
    
    def name(self) -> str:
        return "hypotheses"

    def insert(self, hypothesis: HypothesisORM) -> int:
        """
        Inserts a new hypothesis into the database.
        Assumes goal and prompt are already resolved to IDs.
        """
        try:
            self.session.add(hypothesis)
            self.session.flush()  # To get ID before commit

            if self.logger:
                self.logger.log("HypothesisInserted", {
                    "hypothesis_id": hypothesis.id,
                    "goal_id": hypothesis.goal_id,
                    "strategy": hypothesis.strategy,
                    "length": len(hypothesis.text),
                    "timestamp": hypothesis.created_at.isoformat()
                })

            return hypothesis.id

        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log("HypothesisInsertFailed", {"error": str(e)})
            raise

    def update_review(self, hyp_id: int, review: str):
        """
        Updates the review field for a hypothesis.
        """
        hyp = self.session.query(HypothesisORM).get(hyp_id)
        if not hyp:
            raise ValueError(f"No hypothesis found with ID {hyp_id}")

        hyp.review = review
        self.session.commit()

        if self.logger:
            self.logger.log("ReviewStored", {
                "hypothesis_id": hyp_id,
                "review_snippet": (review or "")[:100]
            })

    def update_reflection(self, hyp_id: int, reflection: str):
        """
        Updates the reflection field for a hypothesis.
        """
        hyp = self.session.query(HypothesisORM).get(hyp_id)
        if not hyp:
            raise ValueError(f"No hypothesis found with ID {hyp_id}")

        hyp.reflection = reflection
        self.session.commit()

        if self.logger:
            self.logger.log("ReflectionStored", {
                "hypothesis_id": hyp_id,
                "reflection_snippet": (reflection or "")[:100]
            })

    def update_elo_rating(self, hyp_id: int, new_rating: float):
        """
        Updates the ELO rating of a hypothesis after pairwise comparison.
        """
        hyp = self.session.query(HypothesisORM).get(hyp_id)
        if not hyp:
            raise ValueError(f"No hypothesis found with ID {hyp_id}")

        hyp.elo_rating = new_rating
        self.session.commit()

        if self.logger:
            self.logger.log("HypothesisEloUpdated", {
                "hypothesis_id": hyp_id,
                "elo_rating": new_rating
            })

    def soft_delete(self, hyp_id: int):
        """
        Soft-deletes a hypothesis by setting enabled = False
        """
        hyp = self.session.query(HypothesisORM).get(hyp_id)
        if not hyp:
            raise ValueError(f"No hypothesis found with ID {hyp_id}")

        hyp.enabled = False
        self.session.commit()

        if self.logger:
            self.logger.log("HypothesisSoftDeleted", {"hypothesis_id": hyp_id})

    def get_by_goal(
        self, goal_text: str, limit: int = 10, source=None
    ) -> list[HypothesisORM]:
        """
        Returns all hypotheses for a given goal.
        """
        query = (
            self.session.query(HypothesisORM)
            .join(GoalORM)
            .filter(GoalORM.goal_text == goal_text)
        )

        if source:
            from co_ai.models import ScoreORM
            query = query.join(ScoreORM).filter(ScoreORM.source == source)

        return query.limit(limit).all()

    def get_latest(self, goal_text: str, limit: int = 10) -> list[HypothesisORM]:
        return self.session.query(HypothesisORM).join(GoalORM).filter(
            GoalORM.goal_text == goal_text
        ).order_by(HypothesisORM.created_at.desc()).limit(limit).all()

    def get_unreflected(self, goal_text: str, limit: int = 10) -> list[HypothesisORM]:
        return self.session.query(HypothesisORM).join(GoalORM).filter(
            GoalORM.goal_text == goal_text,
            HypothesisORM.reflection.is_(None)
        ).limit(limit).all()

    def get_unreviewed(self, goal_text: str, limit: int = 10) -> list[HypothesisORM]:
        return self.session.query(HypothesisORM).join(GoalORM).filter(
            GoalORM.goal_text == goal_text,
            HypothesisORM.review.is_(None)
        ).limit(limit).all()

    def get_from_text(self, query: str, threshold: float = 0.95) -> Optional[HypothesisORM]:
        """
        Finds exact or fuzzy match for hypothesis text.
        """
        result = self.session.query(HypothesisORM).filter(HypothesisORM.text == query).first()
        if result:
            return result

        # Fallback to similarity search if needed
        # This requires pg_trgm extension in PostgreSQL
        result = self.session.query(HypothesisORM).filter(
            HypothesisORM.text.ilike(f"%{query}%")
        ).first()

        if result and result.text:
            sim = SequenceMatcher(None, result.text, query).ratio()
            if sim >= threshold:
                return result

        return None

    def get_by_id(self, hyp_id: int) -> Optional[HypothesisORM]:
        return self.session.get(HypothesisORM, hyp_id)

    def get_all(self, limit: int = 100) -> list[HypothesisORM]:
        return self.session.query(HypothesisORM).order_by(HypothesisORM.created_at.desc()).limit(limit).all()
    
    def get_similar(self, query: str, limit: int = 3) -> list[str]:
        """
        Get top N hypotheses similar to the given prompt using semantic similarity.

        Args:
            query (str): New hypothesis or idea
            limit (int): Number of similar items to return

        Returns:
            list: Top N similar hypotheses
        """
        try:
            query_embedding = self.embedding_store.get_or_create(query)

            results = []
            with self.embedding_store.conn.cursor() as cur:
                cur.execute(
                    "SELECT text FROM hypotheses ORDER BY embedding <-> %s LIMIT %s",
                    (np.array(query_embedding), limit),
                )
                results = [row[0] for row in cur.fetchall()]

            if self.logger:
                self.logger.log("SimilarHypothesesFound", {
                    "query": query[:100],
                    "matches": [r[:100] for r in results]
                })

            return results

        except Exception as e:
            if self.logger:
                self.logger.log("SimilarHypothesesSearchFailed", {"error": str(e)})
            return []---END-OF-FILE---


"co_ai\memory\idea_store.py"
---START-OF-FILE---
# stores/idea_store.py
from co_ai.memory import BaseStore
from co_ai.models.idea import IdeaORM


class IdeaStore(BaseStore):
    def __init__(self, session, logger):
        super().__init__(session, logger)
        self.name = "ideas"

    def name(self) -> str:
        return "ideas"

    def add_idea(self, idea_data: dict) -> IdeaORM:
        """
        Add a single idea to the database.
        """
        idea = IdeaORM(**idea_data)
        self.db.add(idea)
        self.db.commit()
        self.db.refresh(idea)
        return idea

    def bulk_add_ideas(self, ideas_data: list[dict]) -> list[IdeaORM]:
        """
        Add multiple ideas at once.
        """
        ideas = [IdeaORM(**data) for data in ideas_data]
        self.db.bulk_save_objects(ideas)
        self.db.commit()
        return ideas

    def get_by_goal_id(self, goal_id: int) -> list[IdeaORM]:
        """
        Retrieve all ideas associated with a specific goal.
        """
        return self.db.query(IdeaORM).filter(IdeaORM.goal_id == goal_id).all()

    def get_top_ranked_ideas(self, limit: int = 5) -> list[IdeaORM]:
        """
        Get top-ranked ideas based on score or other criteria.
        (This assumes you have a scoring system stored in extra_data or another table)
        """
        # Example: Filter by novelty + feasibility scores from extra_data
        return self.db.query(IdeaORM).order_by(
            IdeaORM.extra_data["novelty_score"].desc(),
            IdeaORM.extra_data["feasibility_score"].desc()
        ).limit(limit).all()

    def get_by_focus_area_and_strategy(self, focus_area: str, strategy: str) -> list[IdeaORM]:
        """
        Retrieve ideas filtered by domain and strategy.
        """
        return self.db.query(IdeaORM).filter(
            IdeaORM.focus_area == focus_area,
            IdeaORM.strategy == strategy
        ).all()

    def get_by_source(self, source: str) -> list[IdeaORM]:
        """
        Retrieve ideas by their origin (e.g., 'llm', 'survey_agent', 'evolved').
        """
        return self.db.query(IdeaORM).filter(IdeaORM.source == source).all()

    def delete_by_goal_id(self, goal_id: int) -> None:
        """
        Delete all ideas linked to a given goal.
        """
        self.db.query(IdeaORM).filter(IdeaORM.goal_id == goal_id).delete()
        self.db.commit()

    def clear_all(self) -> None:
        """
        Clear all ideas â€” useful for testing.
        """
        self.db.query(IdeaORM).delete()
        self.db.commit()---END-OF-FILE---


"co_ai\memory\lookahead_store.py"
---START-OF-FILE---
# stores/lookahead_store.py
import json
from datetime import datetime, timezone
from typing import List, Optional

from sqlalchemy.orm import Session

from co_ai.models.lookahead import LookaheadORM


class LookaheadStore:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "lookahead"
    
    def name(self) -> str:
        return "lookahead"

    def insert(self, goal_id: int, result: LookaheadORM):
        """
        Inserts a new lookahead result into the database.
        Assumes goal already exists.
        """
        try:
            # Build ORM object
            db_lookahead = LookaheadORM(
                goal_id=goal_id,
                agent_name=result.agent_name,
                model_name=result.model_name,
                input_pipeline=result.input_pipeline,
                suggested_pipeline=result.suggested_pipeline,
                rationale=result.rationale,
                reflection=result.reflection,
                backup_plans=json.dumps(result.backup_plans) if result.backup_plans else None,
                extra_data=json.dumps(result.extra_data or {}),
                run_id=result.run_id,
                created_at=result.created_at or datetime.now(timezone.utc),
            )

            self.session.add(db_lookahead)
            self.session.flush()  # To get ID immediately

            if self.logger:
                self.logger.log(
                    "LookaheadInserted",
                    {
                        "goal_id": goal_id,
                        "agent": result.agent_name,
                        "model": result.model_name,
                        "pipeline": result.input_pipeline,
                        "suggested_pipeline": result.suggested_pipeline,
                        "rationale_snippet": (result.rationale or "")[:100],
                    },
                )

            return db_lookahead.id

        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log("LookaheadInsertFailed", {"error": str(e)})
            raise

    def list_all(self, limit: int = 100) -> List[LookaheadORM]:
        """Returns all stored lookaheads, converted back to dataclass"""
        db_results = self.session.query(LookaheadORM).order_by(LookaheadORM.created_at.desc()).limit(limit).all()
        return [self._orm_to_dataclass(result) for result in db_results]

    def get_by_goal_id(self, goal_id: int) -> List[LookaheadORM]:
        results = (
            self.session.query(LookaheadORM)
            .filter_by(goal_id=goal_id)
            .order_by(LookaheadORM.created_at.desc())
            .all()
        )
        return [self._orm_to_dataclass(r) for r in results]

    def get_by_run_id(self, run_id: str) -> Optional[LookaheadORM]:
        result = self.session.query(LookaheadORM).filter_by(run_id=run_id).first()
        return self._orm_to_dataclass(result) if result else None

    def _orm_to_dataclass(self, row: LookaheadORM) -> LookaheadORM:
        return LookaheadORM(
            goal=row.goal_id,
            agent_name=row.agent_name,
            model_name=row.model_name,
            input_pipeline=row.input_pipeline,
            suggested_pipeline=row.suggested_pipeline,
            rationale=row.rationale,
            reflection=row.reflection,
            backup_plans=json.loads(row.backup_plans) if row.backup_plans else [],
            metadata=json.loads(row.extra_data) if row.extra_data else {},
            run_id=row.run_id,
            created_at=row.created_at,
        )---END-OF-FILE---


"co_ai\memory\memory_tool.py"
---START-OF-FILE---
from typing import Any, Optional

import psycopg2
from pgvector.psycopg2 import register_vector
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy.orm import Session, sessionmaker

from co_ai.logs import JSONLogger
from co_ai.memory.context_store import ContextStore
from co_ai.memory.embedding_store import EmbeddingStore
from co_ai.memory.goal_store import GoalStore
from co_ai.memory.hypothesis_store import HypothesisStore
from co_ai.memory.idea_store import IdeaStore
from co_ai.memory.lookahead_store import LookaheadStore
from co_ai.memory.method_plan_store import MethodPlanStore
from co_ai.memory.mrq_store import MRQStore
from co_ai.memory.pattern_store import PatternStatStore
from co_ai.memory.pipeline_run_store import PipelineRunStore
from co_ai.memory.prompt_store import PromptStore
from co_ai.memory.reflection_delta_store import ReflectionDeltaStore
from co_ai.memory.rule_application_store import RuleApplicationStore
from co_ai.memory.rule_effect_store import RuleEffectStore
from co_ai.memory.score_store import ScoreStore
from co_ai.memory.search_result_store import SearchResultStore
from co_ai.memory.sharpening_store import SharpeningStore
from co_ai.memory.symbolic_rule_store import SymbolicRuleStore
from co_ai.memory.prompt_program_store import PromptProgramStore
from co_ai.models.base import engine  # From your SQLAlchemy setup


class MemoryTool:
    def __init__(self, cfg, logger: Optional[JSONLogger] = None):
        self.cfg = cfg
        self.logger = logger
        self._stores = {}  # name -> Store instance

        # Create a new session
        self.session_maker = sessionmaker(bind=engine)
        self.session: Session = self.session_maker()

        # Create connection
        conn = psycopg2.connect(
            dbname=self.cfg.get("db").get("name"),
            user=self.cfg.get("db").get("user"),
            password=self.cfg.get("db").get("password"),
            host=self.cfg.get("db").get("host"),
            port=self.cfg.get("db").get("port"),
        )
        conn.autocommit = True
        register_vector(conn)  # Register pgvector extension

        # Register stores
        self.register_store(GoalStore(self.session, logger))
        embedding_store = EmbeddingStore(self.cfg, conn, self.session, logger)
        self.register_store(embedding_store)
        self.register_store(HypothesisStore(self.session, logger, embedding_store))
        self.register_store(PromptStore(self.session, logger))
        self.register_store(ScoreStore(self.session, logger))
        self.register_store(PipelineRunStore(self.session, logger))
        self.register_store(LookaheadStore(self.session, logger))
        self.register_store(ContextStore(self.session, logger))
        self.register_store(ReflectionDeltaStore(self.session, logger))
        self.register_store(PatternStatStore(self.session, logger))
        self.register_store(SearchResultStore(self.session, logger))
        self.register_store(IdeaStore(self.session, logger))
        self.register_store(MethodPlanStore(self.session, logger))
        self.register_store(MRQStore(cfg, self.session, logger))
        self.register_store(SharpeningStore(self.session, logger))
        self.register_store(SymbolicRuleStore(self.session, logger))
        self.register_store(RuleEffectStore(self.session, logger))
        self.register_store(RuleApplicationStore(self.session, logger))
        self.register_store(PromptProgramStore(self.session, logger))


        # Register extra stores if defined in config
        if cfg.get("extra_stores"):
            for store_class in cfg.get("extra_stores", []):
                self.register_store(store_class(self.session, logger))

    def register_store(self, store):
        store_name = getattr(store, "name", store.__class__.__name__)
        if store_name in self._stores:
            raise ValueError(f"A store named '{store_name}' is already registered.")
        self._stores[store_name] = store

        if self.logger:
            self.logger.log("StoreRegistered", {"store": store_name})

    def get(self, name: str) -> Optional[Any]:
        return self._stores.get(name)

    def __getattr__(self, name: str):
        if name in self._stores:
            return self._stores[name]
        raise AttributeError(f"'MemoryTool' has no attribute '{name}'")

    def commit(self):
        """Commit any pending changes"""
        try:
            self.session.commit()
        except SQLAlchemyError as e:
            self.session.rollback()
            if self.logger:
                self.logger.log("SessionRollback", {"error": str(e)})
            raise

    def close(self):
        """Close session at end of run"""
        try:
            self.session.close()
        except SQLAlchemyError as e:
            if self.logger:
                self.logger.log("SessionCloseFailed", {"error": str(e)})
            self.session = self.session_maker()  # Reopen session on failure

    def begin_nested(self):
        """Start nested transaction (for safe rollback during complex ops)"""
        return self.session.begin_nested()

    def refresh_session(self):
        """Closes current session and creates a fresh one"""
        try:
            self.session.rollback()
            self.session.close()
        finally:
            self.session = self.session_maker()
            if self.logger:
                self.logger.log("SessionRefreshed", {"new_session_id": id(self.session)})---END-OF-FILE---


"co_ai\memory\method_plan_store.py"
---START-OF-FILE---
# stores/method_plan_store.py
from sqlalchemy.orm import Session

from co_ai.memory import BaseStore
from co_ai.models.method_plan import MethodPlanORM


class MethodPlanStore(BaseStore):
    def __init__(self, session, logger):
        super().__init__(session, logger)
        self.name = "method_plans"

    def name(self) -> str:
        return "method_plans"

    def add_method_plan(self, plan_data: dict) -> MethodPlanORM:
        """
        Adds a new method plan to the database.

        Args:
            plan_data (dict): Must include all fields from MethodPlanORM

        Returns:
            MethodPlanORM: The saved ORM object
        """
        required_fields = ["idea_text"]
        missing = [f for f in required_fields if plan_data.get(f) is None]

        if missing:
            self.logger.log(
                "MissingRequiredFields",
                {"missing_fields": missing, "raw_input": plan_data},
            )
            raise ValueError(
                f"Cannot save method plan. Missing required fields: {missing}"
            )

        plan = MethodPlanORM(**plan_data)
        self.db.add(plan)
        self.db.commit()
        self.db.refresh(plan)
        return plan

    def get_by_idea_text(self, idea_text: str) -> list[MethodPlanORM]:
        """
        Retrieves all method plans generated from a specific idea

        Args:
            idea_text (str): Text of the idea used to generate this method

        Returns:
            List of MethodPlanORM objects
        """
        return (
            self.db.query(MethodPlanORM)
            .filter(MethodPlanORM.idea_text.ilike(f"%{idea_text}%"))
            .all()
        )

    def get_by_goal_id(self, goal_id: int) -> list[MethodPlanORM]:
        """
        Retrieves all method plans linked to a specific research goal

        Args:
            goal_id (int): GoalORM.id

        Returns:
            List of MethodPlanORM objects
        """
        return (
            self.db.query(MethodPlanORM).filter(MethodPlanORM.goal_id == goal_id).all()
        )

    def get_top_scoring(self, limit: int = 5) -> list[MethodPlanORM]:
        """
        Get top N method plans ranked by composite score

        Returns:
            List of MethodPlanORM objects
        """
        return (
            self.db.query(MethodPlanORM)
            .order_by(
                (
                    MethodPlanORM.score_novelty * 0.3
                    + MethodPlanORM.score_feasibility * 0.2
                    + MethodPlanORM.score_impact * 0.3
                    + MethodPlanORM.score_alignment * 0.2
                ).desc()
            )
            .limit(limit)
            .all()
        )

    def update_method_plan(self, plan_id: int, updates: dict) -> MethodPlanORM:
        """
        Updates an existing method plan with new values

        Args:
            plan_id (int): ID of the plan to update
            updates (dict): Fields to update (score_novelty, code_plan, etc.)

        Returns:
            Updated MethodPlanORM object
        """
        plan = self.db.query(MethodPlanORM).get(plan_id)
        if not plan:
            raise ValueError(f"No method plan found with id {plan_id}")

        for key, value in updates.items():
            setattr(plan, key, value)

        self.db.commit()
        self.db.refresh(plan)
        return plan

    def delete_by_goal_id(self, goal_id: int) -> None:
        """
        Delete all method plans associated with a given goal

        Args:
            goal_id (int): GoalORM.id
        """
        self.db.query(MethodPlanORM).filter(MethodPlanORM.goal_id == goal_id).delete()
        self.db.commit()

    def clear_all(self) -> None:
        """
        Clear all method plans â€” useful for testing
        """
        self.db.query(MethodPlanORM).delete()
        self.db.commit()---END-OF-FILE---


"co_ai\memory\mrq_store.py"
---START-OF-FILE---
# stores/mrq_store.py
import json
from datetime import datetime, timezone

from sqlalchemy import text
from sqlalchemy.orm import Session

from co_ai.models import (MRQMemoryEntryORM, MRQPreferencePairORM,
                          ReflectionDeltaORM)


class MRQStore:
    def __init__(self, cfg: dict, session: Session, logger=None):
        self.db = session
        self.logger = logger
        self.name = "mrq"
        self.cfg = cfg

    def log_evaluations(self):
        return self.cfg.get("log_evaluations", True)

    def add(
        self,
        goal: str,
        strategy: str,
        prompt: str,
        response: str,
        reward: float,
        metadata: dict = None,
    ):
        """
        Adds a new entry to MRQ memory for symbolic learning or training.
        """
        try:
            db_entry = MRQMemoryEntryORM(
                goal=goal,
                strategy=strategy,
                prompt=prompt,
                response=response,
                reward=reward,
                embedding=None,  # optional: compute from prompt/response
                features=None,  # optional: extract features from metadata
                source="manual",
                run_id=metadata.get("run_id") if metadata else None,
                metadata_=json.dumps(metadata or {}),
                created_at=datetime.now(timezone.utc),
            )

            self.db.add(db_entry)
            self.db.flush()  # Get ID before commit

            if self.logger:
                self.logger.log(
                    "MRQMemoryEntryInserted",
                    {
                        "goal_snippet": goal[:100],
                        "prompt_snippet": prompt[:100],
                        "strategy": strategy,
                        "reward": reward,
                        "timestamp": db_entry.created_at.isoformat(),
                    },
                )

            return db_entry.id

        except Exception as e:
            self.db.rollback()
            if self.logger:
                self.logger.log("MRQMemoryInsertFailed", {"error": str(e)})
            raise

    def get_similar_prompt(self, prompt: str, top_k: int = 5) -> list:
        """
        Gets similar prompts based on text match.
        Future: can use vector similarity instead of trigram search.
        """
        try:
            results = (
                self.db.query(MRQMemoryEntryORM)
                .filter(MRQMemoryEntryORM.prompt.ilike(f"%{prompt}%"))
                .limit(top_k)
                .all()
            )

            return results

        except Exception as e:
            if self.logger:
                self.logger.log("MRQSimilarPromptSearchFailed", {"error": str(e)})
            return []

    def get_by_strategy(self, strategy: str, limit: int = 100) -> list:
        """Returns all entries generated using a specific strategy."""
        return (
            self.db.query(MRQMemoryEntryORM)
            .filter_by(strategy=strategy)
            .limit(limit)
            .all()
        )

    def get_all(self, limit: int = 100) -> list:
        """Returns most recent MRQ memory entries."""
        return (
            self.db.query(MRQMemoryEntryORM)
            .order_by(MRQMemoryEntryORM.created_at.desc())
            .limit(limit)
            .all()
        )

    def train_from_reflection_deltas(self):
        """Train ranker from reflection deltas (symbolic_ranker example)"""
        deltas = self.db.query(ReflectionDeltaORM).all()
        examples = []

        for d in deltas:
            a = d.pipeline_a
            b = d.pipeline_b
            score_a = d.score_a
            score_b = d.score_b

            if not isinstance(a, list) or not isinstance(b, list):
                continue
            if score_a is None or score_b is None:
                continue
            if abs(score_a - score_b) < 0.05:
                continue  # Skip small differences

            label = "b" if score_b > score_a else "a"
            examples.append(
                {
                    "goal_text": d.goal.goal_text,
                    "pipeline_a": a,
                    "pipeline_b": b,
                    "score_a": score_a,
                    "score_b": score_b,
                    "label": label,
                }
            )

        self.training_data = examples
        self.trained_ranker = self.symbolic_ranker()

        if self.logger:
            self.logger.log("MRQTrainingDataLoaded", {"count": len(examples)})

    def symbolic_ranker(self):
        """Simple rule-based ranker used until we train a learned one"""

        def score_pipeline(pipeline: list):
            base_score = len(pipeline) * 0.3
            if "verifier" in pipeline:
                base_score += 1.5
            if "reviewer" in pipeline:
                base_score += 1.2
            if "retriever" in pipeline:
                base_score += 1.0
            if "cot_generator" in pipeline:
                base_score += 0.8
            return base_score

        return score_pipeline

    def get_training_pairs(
        self, goal: str, limit: int = 100, agent_name="generation"
    ) -> list[dict]:
        try:
            sql = text("""
                WITH top_h AS (
                    SELECT DISTINCT ON (p.id)
                        p.id AS prompt_id,
                        g.goal_text AS goal,
                        p.prompt_text,
                        h.text AS output_a,
                        h.elo_rating AS rating_a
                    FROM prompts p
                    JOIN goals g ON p.goal_id = g.id
                    JOIN hypotheses h ON h.prompt_id = p.id
                    WHERE h.enabled = TRUE
                    AND h.goal_id = g.id
                    AND p.agent_name = :agent_name
                    ORDER BY p.id, h.elo_rating DESC
                ),
                bottom_h AS (
                    SELECT DISTINCT ON (p.id)
                        p.id AS prompt_id,
                        h.text AS output_b,
                        h.elo_rating AS rating_b
                    FROM prompts p
                    JOIN hypotheses h ON h.prompt_id = p.id
                    JOIN goals g ON p.goal_id = g.id
                    WHERE h.enabled = TRUE
                    AND h.goal_id = g.id
                    AND p.agent_name = :agent_name
                    ORDER BY p.id, h.elo_rating ASC
                )
                SELECT 
                    top_h.prompt_id,
                    top_h.goal,
                    top_h.prompt_text,
                    top_h.output_a,
                    top_h.rating_a,
                    bottom_h.output_b,
                    bottom_h.rating_b
                FROM top_h
                JOIN bottom_h ON top_h.prompt_id = bottom_h.prompt_id
                WHERE top_h.rating_a != bottom_h.rating_b
                LIMIT :limit;
            """)

            result = self.db.execute(
                sql, {"goal": goal, "agent_name": agent_name, "limit": limit}
            )
            rows = result.fetchall()

            return [
                {
                    "prompt": row[2],
                    "output_a": row[3],
                    "output_b": row[5],
                    "preferred": "a" if row[4] > row[6] else "b",
                    "rating_a": row[4],
                    "rating_b": row[6],
                }
                for row in rows
            ]

        except Exception as e:
            self.db.rollback()
            if self.logger:
                self.logger.log(
                    "GetMRQTrainingPairsFailed", {"error": str(e), "goal": goal}
                )
            return []

    def add_preference_pair(
        self,
        goal: str,
        prompt: str,
        output_a: str,
        output_b: str,
        preferred: str,
        fmt_a: str,
        fmt_b: str,
        difficulty: str,
        source: str = "arm_dataloader",
        run_id: str = None
    ):
        """
        Save preference pair to database with precomputed embeddings.
        Args:
            goal: Task name or group key (e.g., "arm_dpo")
            prompt: Input question or instruction
            output_a: First response (chosen or rejected)
            output_b: Second response
            preferred: Either "a" or "b"
            prompt_emb: Precomputed embedding of the prompt
            output_a_emb: Precomputed embedding of output_a
            output_b_emb: Precomputed embedding of output_b
        """
        try:
            entry = MRQPreferencePairORM(
                goal=goal,
                prompt=prompt,
                output_a=output_a,
                output_b=output_b,
                preferred=preferred,
                fmt_a=fmt_a,
                fmt_b=fmt_b,
                difficulty=difficulty,
                source=source,
                run_id=run_id,
            )
            self.db.add(entry)
            self.db.commit()
        except Exception as e:
            self.db.rollback()
            raise RuntimeError(f"Failed to save preference pair: {str(e)}")
        finally:
            self.db.close()

    def get_training_preferece_pairs(self, goal: str, limit: int = 1000) -> list[dict]:
        try:
            query = self.db.query(MRQPreferencePairORM).filter(
                MRQPreferencePairORM.goal == goal
            )
            results = query.limit(limit).all()
            return [
                {
                    "prompt": r.prompt,
                    "output_a": r.output_a,
                    "output_b": r.output_b,
                    "preferred": r.preferred,
                    "fmt_a": r.fmt_a,
                    "fmt_b": r.fmt_b,
                }
                for r in results
            ]
        except Exception as e:
            raise RuntimeError(
                f"Failed to load preference pairs for goal '{goal}': {str(e)}"
            )
        finally:
            self.db.close()
---END-OF-FILE---


"co_ai\memory\pattern_store.py"
---START-OF-FILE---
# stores/pattern_stat_store.py
from datetime import datetime

from co_ai.models.pattern_stat import PatternStatORM


class PatternStatStore:
    def __init__(self, session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "pattern_stats"

    def insert(self, stats: list[PatternStatORM]):
        """Insert multiple pattern stats at once"""
        try:
            self.session.bulk_save_objects(stats)
            self.session.commit()

            if self.logger:
                self.logger.log("PatternStatsStored", {
                    "goal_id": stats[0].goal_id,
                    "hypothesis_id": stats[0].hypothesis_id,
                    "agent": stats[0].agent_name,
                    "model": stats[0].model_name,
                    "count": len(stats),
                    "timestamp": datetime.utcnow().isoformat()
                })

        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log("PatternStatsInsertFailed", {"error": str(e)})
            raise---END-OF-FILE---


"co_ai\memory\pipeline_run_store.py"
---START-OF-FILE---
# stores/pipeline_run_store.py
import json
from datetime import datetime
from typing import Optional

from sqlalchemy.orm import Session

from co_ai.models.pipeline_run import PipelineRunORM


class PipelineRunStore:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "pipeline_runs"

    def insert(self, run_dict: dict) -> int:
        """
        Inserts a new pipeline run record into the database.
        
        :param run_dict: Dictionary containing fields like run_id, goal_id, pipeline, etc.
        :return: The inserted record's ID
        """
        try:
            # Convert dictionary to ORM object
            db_run = PipelineRunORM(
                run_id=run_dict["run_id"],
                # goal_id=run_dict["goal_id"],
                pipeline=run_dict.get("pipeline"),
                strategy=run_dict.get("strategy"),
                model_name=run_dict.get("model_name"),
                run_config=json.dumps(run_dict.get("run_config")) if run_dict.get("run_config") else None,
                lookahead_context=json.dumps(run_dict.get("lookahead_context")) if run_dict.get("lookahead_context") else None,
                symbolic_suggestion=json.dumps(run_dict.get("symbolic_suggestion")) if run_dict.get("symbolic_suggestion") else None,
                extra_data=json.dumps(run_dict.get("extra_data")) if run_dict.get("extra_data") else None,
                created_at=run_dict.get("created_at") or datetime.utcnow()
            )

            self.session.add(db_run)
            self.session.flush()  # Get ID before commit
            run_id = db_run.id

            if self.logger:
                self.logger.log("PipelineRunInserted", {
                    "run_id": db_run.run_id,
                    "goal_id": db_run.goal_id,
                    "pipeline": db_run.pipeline,
                    "strategy": db_run.strategy,
                    "model": db_run.model_name,
                    "timestamp": db_run.created_at if db_run.created_at else None
                })

            return run_id

        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log("PipelineRunInsertFailed", {"error": str(e)})
            raise

    def get_by_run_id(self, run_id: str) -> Optional[PipelineRunORM]:
        """
        Fetches a single pipeline run by its unique run_id.
        """
        result = self.session.query(PipelineRunORM).filter(PipelineRunORM.run_id == run_id).first()
        return result

    def get_by_goal_id(self, goal_id: int) -> list[PipelineRunORM]:
        """
        Fetches all pipeline runs associated with a given goal.
        """
        return self.session.query(PipelineRunORM).filter(PipelineRunORM.goal_id == goal_id).all()

    def get_all(self, limit: int = 100) -> list[PipelineRunORM]:
        """
        Returns the most recent pipeline runs up to a limit.
        """
        return self.session.query(PipelineRunORM).order_by(PipelineRunORM.created_at.desc()).limit(limit).all()

    def find(self, filters: dict) -> list[PipelineRunORM]:
        """
        Generic search method for pipeline runs.

        :param filters: Dictionary of filter conditions
        :return: Matching PipelineRun instances
        """
        query = self.session.query(PipelineRunORM)

        if "goal_id" in filters:
            query = query.filter(PipelineRunORM.goal_id == filters["goal_id"])
        if "strategy" in filters:
            query = query.filter(PipelineRunORM.strategy == filters["strategy"])
        if "model_name" in filters:
            query = query.filter(PipelineRunORM.model_name == filters["model_name"])
        if "since" in filters:
            query = query.filter(PipelineRunORM.created_at >= filters["since"])

        return query.order_by(PipelineRunORM.created_at.desc()).all()---END-OF-FILE---


"co_ai\memory\prompt_program_store.py"
---START-OF-FILE---
from typing import List, Optional
from sqlalchemy.orm import Session
from co_ai.models.prompt_program import PromptProgramORM  # adjust if path differs


class PromptProgramStore:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "prompt_programs"
        self.table_name = "prompt_programs"

    def add_prompt(self, prompt: PromptProgramORM) -> PromptProgramORM:
        self.session.add(prompt)
        self.session.commit()
        self.session.refresh(prompt)
        return prompt

    def get_by_id(self, prompt_id: str) -> Optional[PromptProgramORM]:
        return self.session.query(PromptProgramORM).filter_by(id=prompt_id).first()

    def get_all_prompts(self) -> List[PromptProgramORM]:
        return self.session.query(PromptProgramORM).order_by(PromptProgramORM.version.desc()).all()

    def get_prompts_for_goal(self, goal_text: str) -> List[PromptProgramORM]:
        return (
            self.session.query(PromptProgramORM)
            .filter(PromptProgramORM.goal == goal_text)
            .order_by(PromptProgramORM.version.desc())
            .all()
        )

    def get_top_prompts(self, goal_text: str, min_score: float = 0.0, top_k: int = 5) -> List[PromptProgramORM]:
        return (
            self.session.query(PromptProgramORM)
            .filter(
                PromptProgramORM.goal == goal_text,
                PromptProgramORM.score >= min_score,
            )
            .order_by(PromptProgramORM.score.desc().nullslast())
            .limit(top_k)
            .all()
        )

    def get_prompt_lineage(self, prompt_id: str) -> List[PromptProgramORM]:
        prompt = self.get_by_id(prompt_id)
        if not prompt:
            return []
        lineage = [prompt]
        while prompt.parent_id:
            prompt = self.get_by_id(prompt.parent_id)
            if prompt:
                lineage.insert(0, prompt)
            else:
                break
        return lineage

    def get_latest_prompt(self, goal_text: str) -> Optional[PromptProgramORM]:
        return (
            self.session.query(PromptProgramORM)
            .filter(PromptProgramORM.goal == goal_text)
            .order_by(PromptProgramORM.version.desc())
            .first()
        )
---END-OF-FILE---


"co_ai\memory\prompt_store.py"
---START-OF-FILE---
# stores/prompt_store.py
import json
from typing import Optional

from sqlalchemy import text
from sqlalchemy.orm import Session

from co_ai.models.goal import GoalORM
from co_ai.models.prompt import PromptORM
from sqlalchemy.dialects.postgresql import dialect

class PromptStore:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "prompt"

    def get_or_create_goal(self, goal_text: str, goal_type: str = None,
                           focus_area: str = None, strategy: str = None,
                           source: str = "user") -> GoalORM:
        """
        Returns existing goal or creates a new one.
        """
        try:
            # Try to find by text
            goal = self.session.query(GoalORM).filter_by(goal_text=goal_text).first()
            if not goal:
                # Create new
                goal = GoalORM(
                    goal_text=goal_text,
                    goal_type=goal_type,
                    focus_area=focus_area,
                    strategy=strategy,
                    llm_suggested_strategy=None,
                    source=source
                )
                self.session.add(goal)
                self.session.flush()  # Get ID before commit

                if self.logger:
                    self.logger.log("GoalCreated", {
                        "goal_id": goal.id,
                        "goal_text": goal_text[:100],
                        "source": source
                    })

            return goal

        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log("GoalGetOrCreateFailed", {"error": str(e)})
            raise

    def save(self, goal: dict, agent_name: str, prompt_key: str, prompt_text: str,
             response: Optional[str] = None, strategy: str = "default", pipeline_run_id: Optional[int] = None,
             extra_data: dict = None, version: int = 1):
        """
        Saves a prompt to the database and marks it as current for its key/agent.
        """
        try:
            goal_text = goal.get("goal_text", "")
            goal_type=goal.get("goal_type")
            # Get or create the associated goal
            goal_orm = self.get_or_create_goal(goal_text=goal_text, goal_type=goal_type)

            # Deactivate previous versions of this prompt key/agent combo
            self.session.query(PromptORM).filter_by(
                agent_name=agent_name,
                prompt_key=prompt_key
            ).update({"is_current": False})

            # Build ORM object
            db_prompt = PromptORM(
                goal_id=goal_orm.id,
                pipeline_run_id=pipeline_run_id,
                agent_name=agent_name,
                prompt_key=prompt_key,
                prompt_text=prompt_text,
                response_text=response,
                strategy=strategy,
                version=version,
                extra_data=json.dumps(extra_data or {})
            )

            self.session.add(db_prompt)
            self.session.flush()  # Get ID immediately

            if self.logger:
                self.logger.log("PromptStored", {
                    "prompt_id": db_prompt.id,
                    "prompt_key": prompt_key,
                    "goal_id": goal_orm.id,
                    "agent": agent_name,
                    "strategy": strategy,
                    "length": len(prompt_text),
                    "timestamp": db_prompt.timestamp.isoformat()
                })

            return db_prompt.id

        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log(
                    "PromptStoreFailed", {"error": str(e), "prompt_key": prompt_key}
                )
            raise

    def get_from_text(
        self,
        prompt_text: str
    ) -> Optional[PromptORM]:
        """
        Retrieve a prompt from the DB based on its exact prompt_text.
        Optionally filter by agent_name and/or strategy.
        """
        try:
            query = self.session.query(PromptORM).filter(
                PromptORM.prompt_text == prompt_text
            )

            prompt = query.order_by(PromptORM.timestamp.desc()).first()

            if self.logger:
                self.logger.log(
                    "PromptLookup",
                    {
                        "matched": bool(prompt),
                        "text_snippet": prompt_text[:100],
                    },
                )

            return prompt

        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log(
                    "PromptLookupFailed",
                    {"error": str(e), "text_snippet": prompt_text[:100]},
                )
            return None

    def get_id_from_response(
        self,
        response_text: str
    ) -> Optional[PromptORM]:
        """
        Retrieve a prompt from the DB based on its exact prompt_text.
        Optionally filter by agent_name and/or strategy.
        """
        try:
            query = self.session.query(PromptORM).filter(
                PromptORM.response_text == response_text
            )

            prompt = query.order_by(PromptORM.timestamp.desc()).first()

            if self.logger:
                self.logger.log(
                    "PromptLookup",
                    {
                        "matched": bool(prompt),
                        "text_snippet": response_text[:100],
                    },
                )

            return prompt.id

        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log(
                    "PromptLookupFailed",
                    {"error": str(e)},
                )
            return None

    def find_matching(self, agent_name, prompt_text, strategy=None):
        query = self.session.query(PromptORM).filter_by(
            agent_name=agent_name,
            prompt_text=prompt_text
        )
        if strategy:
            query = query.filter_by(strategy=strategy)

        return [p.to_dict() for p in query.limit(10).all()]

    def get_prompt_training_set(self, goal: str, limit: int = 500) -> list[dict]:
        try:
            sql = text("""
                SELECT 
                    p.id,
                    g.goal_text AS goal,
                    p.prompt_text,
                    p.prompt_key,
                    p.timestamp,
                    h.text AS hypothesis_text,
                    h.elo_rating,
                    h.review
                FROM goals g
                JOIN prompts p ON p.goal_id = g.id
                JOIN hypotheses h ON h.prompt_id = p.id AND h.goal_id = g.id
                WHERE g.goal_text = :goal
                AND h.enabled = TRUE
                ORDER BY p.id, h.elo_rating DESC, h.updated_at DESC
                LIMIT :limit
            """)
            print("\nðŸ” Final SQL Query:")
            print(sql.compile(dialect=dialect(), compile_kwargs={"literal_binds": True}).string)

            result = self.session.execute(sql, {
                'goal': goal,
                'limit': limit
            })

            rows = result.fetchall()
            return [
                {
                    "id": row[0],
                    "goal": row[1],
                    "prompt_text": row[2],
                    "prompt_key": row[3],
                    "timestamp": row[4],
                    "hypothesis_text": row[5],
                    "elo_rating": row[6],
                    "review": row[7],
                }
                for row in rows
            ]

        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log("GetLatestPromptsFailed", {
                    "error": str(e),
                    "goal": goal
                })
            return []
---END-OF-FILE---


"co_ai\memory\reflection_delta_store.py"
---START-OF-FILE---
from typing import Optional

from co_ai.memory.base import BaseStore
from co_ai.models.reflection_delta import ReflectionDeltaORM


class ReflectionDeltaStore(BaseStore):
    def __init__(self, db, logger=None):
        super().__init__(db, logger)
        self.name = "reflection_deltas"

    def __repr__(self):
        return f"<{self.name} connected={self.db is not None}>"

    def name(self) -> str:
        return "reflection_deltas"

    def insert(self, delta: ReflectionDeltaORM) -> int:
        try:
            self.db.add(delta)
            self.db.flush()
            self.db.commit()

            if self.logger:
                self.logger.log("ReflectionDeltaInserted", {
                    "delta_id": delta.id,
                    "goal_id": delta.goal_id,
                    "run_id_a": delta.run_id_a,
                    "run_id_b": delta.run_id_b,
                    "score_a": delta.score_a,
                    "score_b": delta.score_b,
                    "score_delta": delta.score_delta,
                    "strategy_diff": delta.strategy_diff,
                    "model_diff": delta.model_diff,
                })

            return delta.id

        except Exception as e:
            self.db.rollback()
            if self.logger:
                self.logger.log("ReflectionDeltaInsertFailed", {"error": str(e)})
            raise

    def get_by_goal_id(self, goal_id: int) -> list[ReflectionDeltaORM]:
        try:
            return self.db.query(ReflectionDeltaORM).filter_by(goal_id=goal_id).order_by(ReflectionDeltaORM.created_at.desc()).all()
        except Exception as e:
            if self.logger:
                self.logger.log("ReflectionDeltasFetchFailed", {"error": str(e)})
            return []

    def get_by_run_ids(self, run_id_a: str, run_id_b: str) -> Optional[ReflectionDeltaORM]:
        try:
            return self.db.query(ReflectionDeltaORM).filter_by(run_id_a=run_id_a, run_id_b=run_id_b).first()
        except Exception as e:
            if self.logger:
                self.logger.log("ReflectionDeltaFetchFailed", {"error": str(e)})
            return None

    def get_all(self, limit: int = 100) -> list[ReflectionDeltaORM]:
        try:
            return self.db.query(ReflectionDeltaORM).order_by(ReflectionDeltaORM.created_at.desc()).limit(limit).all()
        except Exception as e:
            if self.logger:
                self.logger.log("ReflectionDeltasFetchFailed", {"error": str(e)})
            return []

    def find(self, filters: dict) -> list[ReflectionDeltaORM]:
        try:
            query = self.db.query(ReflectionDeltaORM)

            if "goal_id" in filters:
                query = query.filter(ReflectionDeltaORM.goal_id == filters["goal_id"])

            if "run_id_a" in filters and "run_id_b" in filters:
                query = query.filter(
                    ReflectionDeltaORM.run_id_a == filters["run_id_a"],
                    ReflectionDeltaORM.run_id_b == filters["run_id_b"]
                )

            if "score_delta_gt" in filters:
                query = query.filter(ReflectionDeltaORM.score_delta > filters["score_delta_gt"])

            if "strategy_diff" in filters:
                query = query.filter(ReflectionDeltaORM.strategy_diff == filters["strategy_diff"])

            return query.order_by(ReflectionDeltaORM.created_at.desc()).all()

        except Exception as e:
            if self.logger:
                self.logger.log("ReflectionDeltasFetchFailed", {"error": str(e)})
            return []
---END-OF-FILE---


"co_ai\memory\report_logger.py"
---START-OF-FILE---
from co_ai.memory import BaseStore


class ReportLogger(BaseStore):
    def __init__(self, db, logger=None):
        super().__init__(db, logger)
        self.name = "report"

    def __repr__(self):
        return f"<{self.name} connected={self.db is not None}>"

    def name(self) -> str:
        return "report"

    def log(self, run_id, goal, summary, path):
        try:
            with self.db.cursor() as cur:
                cur.execute(
                    """
                    INSERT INTO reports (run_id, goal, summary, path)
                    VALUES (%s, %s, %s, %s)
                    """,
                    (run_id, goal, summary, path)
                )
        except Exception as e:
            print(f"âŒ Exception: {type(e).__name__}: {e}")
            if self.logger:
                self.logger.log("ReportLogFailed", {"error": str(e)})---END-OF-FILE---


"co_ai\memory\rule_application_store.py"
---START-OF-FILE---
from typing import List, Optional

from sqlalchemy import and_, desc
from sqlalchemy.orm import Session

from co_ai.models.rule_application import RuleApplicationORM


class RuleApplicationStore:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "rule_applications"
        self.table_name = "rule_applications"

    def add(self, application: RuleApplicationORM) -> RuleApplicationORM:
        try:
            self.session.add(application)
            self.session.commit()
            self.session.refresh(application)
            if self.logger:
                self.logger.log("RuleApplicationAdded", {"rule_application_id": application.id})
            return application
        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log("RuleApplicationAddFailed", {"error": str(e)})
            raise

    def get_by_id(self, application_id: int) -> Optional[RuleApplicationORM]:
        return self.session.query(RuleApplicationORM).get(application_id)

    def get_all(self) -> List[RuleApplicationORM]:
        return self.session.query(RuleApplicationORM).order_by(desc(RuleApplicationORM.created_at)).all()

    def get_by_goal(self, goal_id: int) -> List[RuleApplicationORM]:
        return (
            self.session.query(RuleApplicationORM)
            .filter(RuleApplicationORM.goal_id == goal_id)
            .order_by(desc(RuleApplicationORM.created_at))
            .all()
        )

    def get_by_hypothesis(self, hypothesis_id: int) -> List[RuleApplicationORM]:
        return (
            self.session.query(RuleApplicationORM)
            .filter(RuleApplicationORM.hypothesis_id == hypothesis_id)
            .order_by(desc(RuleApplicationORM.created_at))
            .all()
        )

    def get_by_pipeline_run(self, pipeline_run_id: int) -> List[RuleApplicationORM]:
        return (
            self.session.query(RuleApplicationORM)
            .filter(RuleApplicationORM.pipeline_run_id == pipeline_run_id)
            .order_by(desc(RuleApplicationORM.applied_at))
            .all()
        )

    def get_latest_for_run(self, pipeline_run_id: int) -> Optional[RuleApplicationORM]:
        return (
            self.session.query(RuleApplicationORM)
            .filter(RuleApplicationORM.pipeline_run_id == pipeline_run_id)
            .order_by(desc(RuleApplicationORM.applied_at))
            .first()
        )

    def get_for_goal_and_hypothesis(self, goal_id: int, hypothesis_id: int) -> List[RuleApplicationORM]:
        return (
            self.session.query(RuleApplicationORM)
            .filter(
                RuleApplicationORM.goal_id == goal_id,
                RuleApplicationORM.hypothesis_id == hypothesis_id,
            )
            .order_by(desc(RuleApplicationORM.applied_at))
            .all()
        )
---END-OF-FILE---


"co_ai\memory\rule_effect_store.py"
---START-OF-FILE---
from datetime import datetime, timezone
from typing import Dict, List, Optional

from sqlalchemy.orm import Session

from co_ai.models.rule_application import RuleApplicationORM


class RuleEffectStore:
    def __init__(self, db: Session, logger=None):
        self.db = db
        self.logger = logger
        self.name = "rule_effects"
        self.table_name = "rule_applications"

    def insert(
        self,
        rule_id: int,
        goal_id: int,
        pipeline_run_id: Optional[int] = None,
        hypothesis_id: Optional[int] = None,
        result_score: Optional[float] = None,
        change_type: Optional[str] = None,
        agent_name: Optional[str] = None,
        notes: Optional[str] = None,
        details: Optional[Dict] = None,
        stage_details: Optional[Dict] = None,
        context_hash: Optional[str] = None,
    ) -> RuleApplicationORM:
        """Insert a new rule application record into the database."""
        try:
            application = RuleApplicationORM(
                rule_id=rule_id,
                goal_id=goal_id,
                pipeline_run_id=pipeline_run_id,
                hypothesis_id=hypothesis_id,
                post_score=result_score,
                change_type=change_type,
                agent_name=agent_name,
                notes=notes,
                details=details,
                stage_details=stage_details,
                context_hash=context_hash,
            )
            self.db.add(application)
            self.db.commit()
            self.db.refresh(application)

            if self.logger:
                self.logger.log("RuleApplicationLogged", application.to_dict())

            return application

        except Exception as e:
            self.db.rollback()
            if self.logger:
                self.logger.log("RuleApplicationError", {"error": str(e)})
            raise

    def get_by_rule(self, rule_id: int) -> List[RuleApplicationORM]:
        """Retrieve all applications for a given rule."""
        return self.db.query(RuleApplicationORM).filter_by(rule_id=rule_id).all()

    def get_recent(self, limit: int = 50) -> List[RuleApplicationORM]:
        """Get the most recent rule applications."""
        return (
            self.db.query(RuleApplicationORM)
            .order_by(RuleApplicationORM.created_at.desc())
            .limit(limit)
            .all()
        )

    def get_feedback_summary(self, rule_id: int) -> Dict[str, int]:
        """Return a count of feedback labels for a specific rule."""
        results = (
            self.db.query(RuleApplicationORM.result_label)
            .filter(RuleApplicationORM.rule_id == rule_id)
            .all()
        )
        summary = {}
        for (label,) in results:
            if label:
                summary[label] = summary.get(label, 0) + 1
        return summary


    def get_by_run_and_goal(self, run_id: int, goal_id: int) -> List[RuleApplicationORM]:
        """
        Retrieve all rule applications for a specific pipeline run and goal.

        Args:
            run_id (int): The ID of the pipeline run.
            goal_id (int): The ID of the goal.

        Returns:
            List[RuleApplicationORM]: Matching rule applications.
        """
        if not run_id or not goal_id:
            if self.logger:
                self.logger.log("InvalidInputForRuleFetch", {
                    "reason": "Missing run_id or goal_id",
                    "run_id": run_id,
                    "goal_id": goal_id
                })
            return []

        try:
            applications = (
                self.db.query(RuleApplicationORM)
                .filter(
                    RuleApplicationORM.pipeline_run_id == int(run_id),
                    RuleApplicationORM.goal_id == int(goal_id)
                )
                .all()
            )

            if self.logger and len(applications) > 0:
                self.logger.log("RuleApplicationsFetched", {
                    "run_id": run_id,
                    "goal_id": goal_id,
                    "count": len(applications)
                })

            return applications

        except Exception as e:
            if self.logger:
                self.logger.log("RuleApplicationFetchError", {
                    "error": str(e),
                    "run_id": run_id,
                    "goal_id": goal_id
                })
            return []---END-OF-FILE---


"co_ai\memory\score_store.py"
---START-OF-FILE---
# stores/score_store.py
import json
from typing import Optional

from sqlalchemy.orm import Session

from co_ai.models import RuleApplicationORM
from co_ai.models.goal import GoalORM
from co_ai.models.score import ScoreORM
from co_ai.models.score_rule_link import ScoreRuleLinkORM


class ScoreStore:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "scores"
        self.table_name = "scores"

    def insert(self, score: ScoreORM):
        """
        Inserts a new score into the database.
        Accepts a dictionary (e.g., from Score dataclass).
        """
        try:
            self.session.add(score)
            self.session.flush()  # To get ID immediately

            if self.logger:
                self.logger.log(
                    "ScoreStored",
                    {
                        "score_id": score.id,
                        "goal_id": score.goal_id,
                        "hypothesis_id": score.hypothesis_id,
                        "agent": score.agent_name,
                        "model": score.model_name,
                        "scores": score.scores,
                        "timestamp": score.created_at.isoformat(),
                    },
                )

            # Link score to rule application if possible
            if score.pipeline_run_id and score.goal_id:
                rule_apps = (
                    self.session.query(RuleApplicationORM)
                    .filter_by(pipeline_run_id=score.pipeline_run_id, goal_id=score.goal_id)
                    .all()
                )
                for ra in rule_apps:
                    link = ScoreRuleLinkORM(score_id=score.id, rule_application_id=ra.id)
                    self.session.add(link)
                self.logger.log(
                    "ScoreLinkedToRuleApplications",
                    {
                        "score_id": score.id,
                        "linked_rule_application_ids": [ra.id for ra in rule_apps],
                    },
                )

            self.session.refresh(score)
            self.session.commit()
            return score.id

        except Exception as e:
            self.session.rollback()
            if self.logger:
                self.logger.log("ScoreInsertFailed", {"error": str(e)})
            raise

    def get_by_goal_id(self, goal_id: int) -> list[dict]:
        """Returns all scores associated with a specific goal."""
        results = self.session.query(ScoreORM).join(GoalORM).filter(GoalORM.id == goal_id).all()
        return [self._orm_to_dict(r) for r in results]

    def get_by_goal_type(self, goal_type: str) -> list[dict]:
        """Returns all scores associated with a specific goal."""
        results = self.session.query(ScoreORM).join(GoalORM).filter(GoalORM.goal_type == goal_type).all()
        return [self._orm_to_dict(r) for r in results]


    def get_by_hypothesis_id(
        self,
        hypothesis_id: int,
        source: Optional[str] = None
    ) -> list[dict]:
        """Returns all scores associated with a specific hypothesis, optionally filtered by evaluator source."""
        query = self.session.query(ScoreORM).filter(ScoreORM.hypothesis_id == hypothesis_id)
        
        if source:
            query = query.filter(ScoreORM.evaluator_name == source)

        results = query.all()
        return [self._orm_to_dict(r) for r in results]

    def get_by_run_id(self, run_id: str) -> list[dict]:
        """Returns all scores associated with a specific pipeline run."""
        results = self.session.query(ScoreORM).filter(ScoreORM.run_id == run_id).all()
        return [self._orm_to_dict(r) for r in results]

    def get_by_evaluator(self, evaluator_name: str) -> list[dict]:
        """Returns all scores produced by a specific evaluator (LLM, MRQ, etc.)"""
        results = self.session.query(ScoreORM).filter(ScoreORM.evaluator_name == evaluator_name).all()
        return [self._orm_to_dict(r) for r in results]

    def get_by_strategy(self, strategy: str) -> list[dict]:
        """Returns all scores generated using a specific reasoning strategy."""
        results = self.session.query(ScoreORM).filter(ScoreORM.strategy == strategy).all()
        return [self._orm_to_dict(r) for r in results]

    def get_all(self, limit: int = 100) -> list[dict]:
        """Returns the most recent scores up to a limit."""
        results = self.session.query(ScoreORM).order_by(ScoreORM.created_at.desc()).limit(limit).all()
        return [self._orm_to_dict(r) for r in results]

    def _orm_to_dict(self, row: ScoreORM) -> dict:
        """Converts an ORM object back to a dictionary format"""
        return {
            "id": row.id,
            "goal_id": row.goal_id,
            "hypothesis_id": row.hypothesis_id,
            "agent_name": row.agent_name,
            "model_name": row.model_name,
            "evaluator_name": row.evaluator_name,
            "score_type": row.score_type,
            "score": row.score,
            "score_text": row.score_text,
            "strategy": row.strategy,
            "reasoning_strategy": row.reasoning_strategy,
            "rationale": row.rationale,
            "reflection": row.reflection,
            "review": row.review,
            "meta_review": row.meta_review,
            "pipeline_run_id": row.pipeline_run_id,
            "extra_data": (
                row.extra_data if isinstance(row.extra_data, dict) else json.loads(row.extra_data)
            ) if row.extra_data else {},
            "created_at": row.created_at,
        }
    
    def get_rules_for_score(self, score_id: int) -> list[int]:
        links = (
            self.session.query(ScoreRuleLinkORM.rule_application_id)
            .filter_by(score_id=score_id)
            .all()
        )
        return [rid for (rid,) in links]---END-OF-FILE---


"co_ai\memory\search_result_store.py"
---START-OF-FILE---
# stores/search_result_store.py
from datetime import datetime
from typing import Dict, List, Optional

from co_ai.memory import BaseStore
from co_ai.models.search_result import SearchResultORM


class SearchResultStore(BaseStore):
    def __init__(self, session, logger):
        super().__init__(session, logger)
        self.name = "search_results"

    def name(self) -> str:
        return "search_results"

    def add_result(
        self,
        *,
        query: str,
        source: str,
        result_type: str,
        title: str,
        summary: str,
        url: str,
        author: Optional[str] = None,
        published_at: Optional[datetime] = None,
        tags: Optional[List[str]] = None,
        goal_id: Optional[int] = None,
        parent_goal: Optional[str] = None,
        strategy: Optional[str] = None,
        focus_area: Optional[str] = None,
        extra_data: Optional[Dict] = None
    ) -> SearchResultORM:
        """
        Add a single search result to the database.
        """
        result = SearchResultORM(
            query=query,
            source=source,
            result_type=result_type,
            title=title,
            summary=summary,
            url=url,
            author=author,
            published_at=published_at,
            tags=tags,
            goal_id=goal_id,
            parent_goal=parent_goal,
            strategy=strategy,
            focus_area=focus_area,
            extra_data=extra_data
        )
        self.session.add(result)
        self.session.commit()
        self.session.refresh(result)
        return result

    def bulk_add_results(self, results: List[Dict]) -> List[SearchResultORM]:
        """
        Add multiple search results at once.
        Each dict should contain the required fields.
        """
        orm_objects = [SearchResultORM(**result) for result in results]
        self.db.bulk_save_objects(orm_objects)
        self.db.commit()
        return orm_objects

    def get_by_goal_id(self, goal_id: int) -> List[SearchResultORM]:
        """
        Retrieve all search results associated with a specific goal.
        """
        return self.db.query(SearchResultORM).filter(
            SearchResultORM.goal_id == goal_id
        ).all()

    def get_by_strategy_and_focus(self, strategy: str, focus_area: str) -> List[SearchResultORM]:
        """
        Get results filtered by strategy and focus area.
        """
        return self.db.query(SearchResultORM).filter(
            SearchResultORM.strategy == strategy,
            SearchResultORM.focus_area == focus_area
        ).all()

    def get_by_source_and_type(self, source: str, result_type: str) -> List[SearchResultORM]:
        """
        Get results filtered by source and type (e.g., arxiv/paper).
        """
        return self.db.query(SearchResultORM).filter(
            SearchResultORM.source == source,
            SearchResultORM.result_type == result_type
        ).all()

    def delete_by_goal_id(self, goal_id: int) -> None:
        """
        Delete all search results linked to a given goal.
        """
        self.db.query(SearchResultORM).filter(
            SearchResultORM.goal_id == goal_id
        ).delete()
        self.db.commit()

    def clear_all(self) -> None:
        """
        Delete all records â€” useful for testing.
        """
        self.db.query(SearchResultORM).delete()
        self.db.commit()---END-OF-FILE---


"co_ai\memory\sharpening_store.py"
---START-OF-FILE---
# stores/sharpening_store.py
from sqlalchemy.orm import Session

from co_ai.models.sharpening_prediction import SharpeningPredictionORM


class SharpeningStore:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "sharpening"

    def insert_sharpening_prediction(self, prediction_dict: dict, goal: dict):
        """
        Inserts a new sharpening comparison from A/B hypothesis testing
        """
        prediction = SharpeningPredictionORM(**prediction_dict)
        prediction.goal_id = goal.get("id")  # Ensure correct goal linkage

        self.session.add(prediction)
        self.session.commit()
        self.session.refresh(prediction)

        return prediction.id---END-OF-FILE---


"co_ai\memory\symbolic_rule_store.py"
---START-OF-FILE---
from typing import List

import yaml
from sqlalchemy import and_, or_
from sqlalchemy.orm import Session

from co_ai.constants import PIPELINE
from co_ai.models.score import ScoreORM
from co_ai.models.symbolic_rule import SymbolicRuleORM


class SymbolicRuleStore:
    def __init__(self, session: Session, logger=None):
        self.session = session
        self.logger = logger
        self.name = "symbolic_rules"
        self.table_name = "symbolic_rules"

    def add_rule(self, rule: SymbolicRuleORM):
        self.session.add(rule)
        self.session.commit()
        self.session.refresh(rule)
        return rule

    def get_all_rules(self) -> List[SymbolicRuleORM]:
        return self.session.query(SymbolicRuleORM).all()

    def get_rules_for_goal(self, goal) -> List[SymbolicRuleORM]:
        return (
            self.session.query(SymbolicRuleORM)
            .filter(SymbolicRuleORM.goal_id == goal.id)
            .all()
        )

    def get_applicable_rules(
        self, goal: dict, pipeline_run_id: int = None, config: dict = {}
    ):
        match_priority = config.get(
            "match_priority", ["goal_id", "pipeline_run_id", "metadata"]
        )
        metadata_mode = config.get("metadata_match_mode", "partial")
        allow_fallback = config.get("allow_fallback", True)

        filters = []

        if "goal_id" in match_priority and goal.get("id"):
            filters.append(SymbolicRuleORM.goal_id == goal["id"])

        if "pipeline_run_id" in match_priority and pipeline_run_id:
            filters.append(SymbolicRuleORM.pipeline_run_id == pipeline_run_id)

        if "metadata" in match_priority and allow_fallback:
            goal_type = goal.get("goal_type")
            goal_category = goal.get("goal_category")
            difficulty = goal.get("difficulty")

            if metadata_mode == "exact":
                filters.append(
                    and_(
                        SymbolicRuleORM.goal_type == goal_type,
                        SymbolicRuleORM.goal_category == goal_category,
                        SymbolicRuleORM.difficulty == difficulty,
                    )
                )
            elif metadata_mode == "partial":
                filters += [
                    and_(
                        SymbolicRuleORM.goal_type == goal_type,
                        SymbolicRuleORM.goal_category == None,
                        SymbolicRuleORM.difficulty == None,
                    ),
                    and_(
                        SymbolicRuleORM.goal_category == goal_category,
                        SymbolicRuleORM.goal_type == None,
                        SymbolicRuleORM.difficulty == None,
                    ),
                    and_(
                        SymbolicRuleORM.difficulty == difficulty,
                        SymbolicRuleORM.goal_type == None,
                        SymbolicRuleORM.goal_category == None,
                    ),
                ]

        if not filters:
            return []

        query = self.session.query(SymbolicRuleORM).filter(or_(*filters))
        return query.all()

    def find_matching_rules(self, goal) -> List[SymbolicRuleORM]:
        return (
            self.session.query(SymbolicRuleORM)
            .filter(
                SymbolicRuleORM.goal_id == goal.id
                # Add logic here if you want partial matches by goal_type, etc.
            )
            .order_by(SymbolicRuleORM.score.desc().nullslast())
            .all()
        )

    def update_rule_score(self, rule_id: int):
        scores = (
            self.session.query(ScoreORM.score)
            .filter(ScoreORM.symbolic_rule_id == rule_id)
            .all()
        )
        scores = [s[0] for s in scores if s[0] is not None]

        if scores:
            avg_score = sum(scores) / len(scores)
            rule = self.session.query(SymbolicRuleORM).get(rule_id)
            rule.score = avg_score
            self.session.commit()
            return avg_score
        return None

    def get_top_rules(self, top_k=10) -> List[SymbolicRuleORM]:
        return (
            self.session.query(SymbolicRuleORM)
            .order_by(SymbolicRuleORM.score.desc().nullslast())
            .limit(top_k)
            .all()
        )

    def track_pipeline_stage(self, stage_dict: dict, context: dict):
        # Only create if not already exists
        goal = context.get("goal")
        goal_id = goal.get("id")
        pipeline_run_id = context.get("pipeline_run_id")
        agent_name = stage_dict.get("name", "default_agent")
        rule_filter = {"agent_name": agent_name, "goal_id": goal_id}
        context_hash = SymbolicRuleORM.compute_context_hash(stage_dict, rule_filter)
        existing = (
            self.session.query(SymbolicRuleORM)
            .filter_by(
                goal_id=goal_id, context_hash=context_hash, agent_name=agent_name
            )
            .first()
        )

        if not existing:
            rule = SymbolicRuleORM(
                goal_id=goal_id,
                pipeline_run_id=pipeline_run_id,
                agent_name=agent_name,
                target="pipeline",
                attributes=stage_dict,
                filter=rule_filter,
                goal_type=goal.get("goal_type"),
                goal_category=goal.get("goal_category") or goal.get("focus_area"),
                difficulty=goal.get("difficulty"),
                context_hash=context_hash,
                source="pipeline_stage",
            )
            self.add_rule(rule)

    def load_from_yaml(self, path: str):
        with open(path, "r") as f:
            data = yaml.safe_load(f)

        for rule in data.get("rules", []):
            exists = (
                self.session.query(SymbolicRuleORM)
                .filter_by(
                    goal_type=rule.get("goal_type"),
                    agent_name=rule.get("agent_name"),
                    rule_text=rule.get("rule_text"),
                )
                .first()
            )
            if not exists:
                new_rule = SymbolicRuleORM(**rule)
                self.session.add(new_rule)
        self.session.commit()

    def get_all(self) -> list[SymbolicRuleORM]:
        try:
            rules = self.session.query(SymbolicRuleORM).order_by(SymbolicRuleORM.created_at.desc()).all()
            if self.logger:
                self.logger.log("SymbolicRulesFetched", {"count": len(rules)})
            return rules
        except Exception as e:
            if self.logger:
                self.logger.log("SymbolicRulesFetchError", {"error": str(e)})
            return []

    def get_by_id(self, rule_id: int):
        try:
            return self.session.query(SymbolicRuleORM).filter_by(id=rule_id).first()
        except Exception as e:
            if self.logger:
                self.logger.log("SymbolicRuleGetByIdError", {"rule_id": rule_id, "error": str(e)})
            return None---END-OF-FILE---


"co_ai\models\__init__.py"
---START-OF-FILE---
from .context_state import ContextStateORM
from .goal import GoalORM
from .hypothesis import HypothesisORM
from .idea import IdeaORM
from .lookahead import LookaheadORM
from .method_plan import MethodPlanORM
from .mrq_memory_entry import MRQMemoryEntryORM
from .mrq_preference_pair import MRQPreferencePairORM
from .pattern_stat import PatternStatORM
from .pipeline_run import PipelineRunORM
from .prompt import PromptORM
from .reflection_delta import ReflectionDeltaORM
from .rule_application import RuleApplicationORM
from .score import ScoreORM
from .score_rule_link import ScoreRuleLinkORM
from .search_result import SearchResultORM
from .sharpening_prediction import SharpeningPredictionORM
from .sharpening_result import SharpeningResultORM
from .symbolic_rule import SymbolicRuleORM
from .score_dimension import ScoreDimensionORM
---END-OF-FILE---


"co_ai\models\base.py"
---START-OF-FILE---
from sqlalchemy import create_engine
from sqlalchemy.orm import declarative_base, sessionmaker

# Replace with your actual DB URL
engine = create_engine("postgresql://co:co@localhost:5432/co")
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()---END-OF-FILE---


"co_ai\models\context_state.py"
---START-OF-FILE---
# models/context_state.py
from datetime import datetime, timezone

from sqlalchemy import JSON, Boolean, Column, DateTime, Integer, String, Text

from co_ai.models.base import Base


class ContextStateORM(Base):
    __tablename__ = "context_states"

    id = Column(Integer, primary_key=True)
    run_id = Column(String, nullable=False)
    stage_name = Column(String, nullable=False)
    version = Column(Integer, nullable=False)
    is_current = Column(Boolean, default=True)
    context = Column(JSON, nullable=False)  # Stored as JSONB or TEXT
    preferences = Column(JSON)
    extra_data = Column(JSON)
    timestamp = Column(DateTime, default=lambda: datetime.now(timezone.utc))---END-OF-FILE---


"co_ai\models\goal.py"
---START-OF-FILE---
# models/goal.py
from datetime import datetime

from sqlalchemy import Column, DateTime, Integer, String
from sqlalchemy.orm import relationship

from .base import Base


class GoalORM(Base):
    __tablename__ = "goals"

    id = Column(Integer, primary_key=True)
    goal_text = Column(String, nullable=False)
    goal_type = Column(String)
    focus_area = Column(String)
    strategy = Column(String)
    llm_suggested_strategy = Column(String)
    source = Column(String, default="user")
    difficulty = Column(String, default="medium")
    goal_category = Column(String, default="analyze")

    created_at = Column(DateTime, default=datetime.utcnow)

    prompts = relationship("PromptORM", back_populates="goal")
    hypotheses = relationship("HypothesisORM", back_populates="goal")
    pipeline_runs = relationship("PipelineRunORM", back_populates="goal")
    scores = relationship("ScoreORM", back_populates="goal")
    lookaheads = relationship("LookaheadORM", back_populates="goal")
    reflection_deltas = relationship("ReflectionDeltaORM", back_populates="goal")
    ideas = relationship("IdeaORM", back_populates="goal")
    method_plans = relationship("MethodPlanORM", back_populates="goal")
    sharpening_predictions = relationship(
        "SharpeningPredictionORM", back_populates="goal"
    )
    symbolic_rules = relationship("SymbolicRuleORM", back_populates="goal")

    rule_applications = relationship(
        "RuleApplicationORM", back_populates="goal", cascade="all, delete-orphan"
    )

    def __repr__(self):
        return f"<GoalORM(id={self.id}, goal_text='{self.goal_text[:50]}')>"

    def to_dict(self):
        return {
            "id": self.id,
            "goal_text": self.goal_text,
            "goal_type": self.goal_type,
            "focus_area": self.focus_area,
            "strategy": self.strategy,
            "llm_suggested_strategy": self.llm_suggested_strategy,
            "source": self.source,
            "created_at": self.created_at.isoformat() if self.created_at else None,
        }
---END-OF-FILE---


"co_ai\models\hypothesis.py"
---START-OF-FILE---
# models/hypothesis.py
from datetime import datetime

from sqlalchemy import (JSON, Boolean, Column, DateTime, Float, ForeignKey,
                        Integer, String)
from sqlalchemy.orm import relationship

from .base import Base


class HypothesisORM(Base):
    __tablename__ = "hypotheses"

    id = Column(Integer, primary_key=True)
    text = Column(String, nullable=False)
    goal_id = Column(Integer, ForeignKey("goals.id"))
    prompt_id = Column(Integer, ForeignKey("prompts.id"))
    pipeline_run_id = Column(Integer, ForeignKey("pipeline_runs.id"))
    source_hypothesis_id = Column(Integer, ForeignKey("hypotheses.id"))
    strategy = Column(String)
    confidence = Column(Float, default=0.0)
    review = Column(String)
    reflection = Column(String)
    elo_rating = Column(Float, default=750.0)
    embedding = Column(JSON)  # Use pgvector later for better support
    features = Column(JSON)   # For structured metadata
    source = Column(String)
    pipeline_signature = Column(String)
    enabled = Column(Boolean, default=True)
    version = Column(Integer, default=1)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    # Relationships
    goal = relationship("GoalORM", back_populates="hypotheses")
    prompt = relationship("PromptORM", back_populates="hypotheses")
    source_hypothesis = relationship("HypothesisORM", remote_side=[id])
    scores = relationship("ScoreORM", back_populates="hypothesis")
    pipeline_run = relationship("PipelineRunORM", back_populates="hypotheses")

    rule_applications = relationship(
        "RuleApplicationORM",
        back_populates="hypothesis",
        cascade="all, delete-orphan",
    )

    def to_dict(self, include_related=False):
        return {
            "id": self.id,
            "text": self.text,
            "goal_id": self.goal_id,
            "prompt_id": self.prompt_id,
            "strategy": self.strategy,
            "confidence": self.confidence,
            "review": self.review,
            "reflection": self.reflection,
            "elo_rating": self.elo_rating,
            "embedding": self.embedding,
            "features": self.features,
            "source_hypothesis_id": self.source_hypothesis_id,
            "source": self.source,
            "pipeline_signature": self.pipeline_signature,
            "pipeline_id": self.pipeline_run_id,
            "enabled": self.enabled,
            "version": self.version,
            "created_at": self.created_at.isoformat() if self.created_at else None,
            "updated_at": self.updated_at.isoformat() if self.updated_at else None,
            # Optional related objects
            "goal": self.goal.to_dict() if include_related and self.goal else None,
            "prompt": self.prompt.to_dict()
            if include_related and self.prompt
            else None,
            "scores": [s.to_dict() for s in self.scores]
            if include_related and self.scores
            else None,
        }

    @staticmethod
    def from_dict(data: dict):
        return HypothesisORM(
            id=data.get("id"),
            text=data.get("text", ""),
            goal_id=data.get("goal_id"),
            prompt_id=data.get("prompt_id"),
            strategy=data.get("strategy"),
            confidence=data.get("confidence", 0.0),
            review=data.get("review"),
            reflection=data.get("reflection"),
            elo_rating=data.get("elo_rating", 750.0),
            embedding=data.get("embedding"),
            features=data.get("features"),
            source_hypothesis_id=data.get("source_hypothesis_id"),
            source=data.get("source"),
            pipeline_signature=data.get("pipeline_signature"),
            pipeline_id=data.get("pipeline_id"),
            enabled=data.get("enabled", True),
            version=data.get("version", 1),
            created_at=data.get("created_at"),
            updated_at=data.get("updated_at"),
        )
---END-OF-FILE---


"co_ai\models\idea.py"
---START-OF-FILE---
# models/idea.py
from datetime import datetime

from sqlalchemy import JSON, Column, DateTime, ForeignKey, Integer, String
from sqlalchemy.orm import relationship

from .base import Base


class IdeaORM(Base):
    __tablename__ = "ideas"

    id = Column(Integer, primary_key=True, autoincrement=True)
    idea_text = Column(String, nullable=False)
    parent_goal = Column(String)
    focus_area = Column(String)
    strategy = Column(String)
    source = Column(String)
    origin = Column(String)
    extra_data = Column(JSON)
    goal_id = Column(Integer, ForeignKey("goals.id"), nullable=True)
    created_at = Column(DateTime, default=datetime.utcnow)

    # Relationship to GoalORM
    goal = relationship("GoalORM", back_populates="ideas")

    def to_dict(self):
        return {
            "id": self.id,
            "idea_text": self.idea_text,
            "parent_goal": self.parent_goal,
            "focus_area": self.focus_area,
            "strategy": self.strategy,
            "source": self.source,
            "origin": self.origin,
            "extra_data": self.extra_data or {},  # Avoid NoneType issues
            "goal_id": self.goal_id,
            "created_at": self.created_at.isoformat() if self.created_at else None
        }---END-OF-FILE---


"co_ai\models\lookahead.py"
---START-OF-FILE---
# models/lookahead.py
from datetime import datetime

from sqlalchemy import Column, DateTime, ForeignKey, Integer, String, Text
from sqlalchemy.orm import relationship

from co_ai.models.base import Base


class LookaheadORM(Base):
    __tablename__ = "lookaheads"

    id = Column(Integer, primary_key=True)

    # Goal info
    goal_id = Column(Integer, ForeignKey("goals.id"), nullable=False)

    # Agent metadata
    agent_name = Column(String, nullable=False)
    model_name = Column(String, nullable=False)

    # Pipeline information
    input_pipeline = Column(String)  # Store as JSON string if needed
    suggested_pipeline = Column(String)  # Same here
    rationale = Column(Text)
    reflection = Column(Text)
    backup_plans = Column(Text)  # List[str] stored as JSON or newline-separated
    extra_data = Column("metadata", Text)  # Renamed to avoid conflict with SQLAlchemy
    run_id = Column(String)

    created_at = Column(DateTime, default=datetime.utcnow)

    goal = relationship("GoalORM", back_populates="lookaheads")
---END-OF-FILE---


"co_ai\models\method_plan.py"
---START-OF-FILE---
# models/method_plan.py
from datetime import datetime, timezone

from sqlalchemy import (JSON, Boolean, Column, DateTime, Float, ForeignKey,
                        Integer, String)
from sqlalchemy.orm import relationship

from .base import Base


class MethodPlanORM(Base):
    __tablename__ = "method_plans"

    id = Column(Integer, primary_key=True)

    # Idea source
    idea_text = Column(String, nullable=False)
    idea_id = Column(Integer, ForeignKey("ideas.id"), nullable=True)
    goal_id = Column(Integer, ForeignKey("goals.id"), nullable=False)

    # Research design fields
    research_objective = Column(String, nullable=False)
    key_components = Column(JSON)  # List of technical components or modules
    experimental_plan = Column(String)
    hypothesis_mapping = Column(JSON)  # e.g., {"H1": "handled by introspection module"}
    search_strategy = Column(String)
    knowledge_gaps = Column(String)
    next_steps = Column(String)

    # Supporting metadata
    task_description = Column(String)
    baseline_method = Column(String)
    literature_summary = Column(String)
    code_plan = Column(String)  # Optional pseudocode or starter code
    focus_area = Column(String)  # e.g., chemistry, nlp, cv, meta_learning
    strategy = Column(String)  # e.g., graph_attention_with_positional_embeddings

    # Scoring system
    score_novelty = Column(Float)
    score_feasibility = Column(Float)
    score_impact = Column(Float)
    score_alignment = Column(Float)

    # Evolution tracking
    evolution_level = Column(Integer, default=0)
    parent_plan_id = Column(Integer, ForeignKey("method_plans.id"), nullable=True)
    is_refinement = Column(Boolean, default=False)

    # Timestamps
    created_at = Column(DateTime, default=datetime.now(timezone.utc))

    # Relationships
    goal = relationship("GoalORM", back_populates="method_plans")
    parent_plan = relationship("MethodPlanORM", remote_side=[id], backref="refinements")

    def to_dict(self, include_relationships: bool = False) -> dict:
        result = {
            "id": self.id,
            "idea_text": self.idea_text,
            "idea_id": self.idea_id,
            "goal_id": self.goal_id,
            "research_objective": self.research_objective,
            "key_components": self.key_components,
            "experimental_plan": self.experimental_plan,
            "hypothesis_mapping": self.hypothesis_mapping,
            "search_strategy": self.search_strategy,
            "knowledge_gaps": self.knowledge_gaps,
            "next_steps": self.next_steps,
            "baseline_method": self.baseline_method,
            "literature_summary": self.literature_summary,
            "code_plan": self.code_plan,
            "focus_area": self.focus_area,
            "strategy": self.strategy,
            "score_novelty": self.score_novelty,
            "score_feasibility": self.score_feasibility,
            "score_impact": self.score_impact,
            "score_alignment": self.score_alignment,
            "evolution_level": self.evolution_level,
            "parent_plan_id": self.parent_plan_id,
            "is_refinement": self.is_refinement,
            "created_at": self.created_at.isoformat() if self.created_at else None
        }

        if include_relationships and self.goal:
            result["goal"] = self.goal.to_dict()
            result["refinements"] = [r.to_dict() for r in self.refinements]

        return result---END-OF-FILE---


"co_ai\models\mrq_memory_entry.py"
---START-OF-FILE---
# models/mrq_memory_entry.py
from datetime import datetime, timezone

from sqlalchemy import JSON, Column, DateTime, Float, Integer, String, Text
from sqlalchemy.dialects.postgresql import ARRAY, REAL

from co_ai.models.base import Base


class MRQMemoryEntryORM(Base):
    __tablename__ = "mrq_memory"

    id = Column(Integer, primary_key=True)
    goal = Column(Text, nullable=False)
    strategy = Column(String, nullable=False)
    prompt = Column(Text, nullable=False)
    response = Column(Text, nullable=False)
    reward = Column(Float, nullable=False)

    # Optional: Use these if storing embeddings
    embedding = Column(ARRAY(REAL))  # Or use pgvector.ARRAY(Float)
    features = Column(JSON)   # Additional extracted features

    source = Column(String)   # e.g., manual, agent, refinement
    run_id = Column(String)
    metadata_ = Column("metadata", JSON)
    created_at = Column(DateTime, default=datetime.now(timezone.utc))---END-OF-FILE---


"co_ai\models\mrq_preference_pair.py"
---START-OF-FILE---
from datetime import datetime, timezone

from sqlalchemy import JSON, Column, DateTime, Integer, Text

from co_ai.models.base import Base


class MRQPreferencePairORM(Base):
    __tablename__ = "mrq_preference_pairs"

    id = Column(Integer, primary_key=True)

    goal = Column(Text, nullable=False)
    prompt = Column(Text, nullable=False)

    output_a = Column(Text, nullable=False)
    output_b = Column(Text, nullable=False)
    preferred = Column(Text, nullable=False)  # 'a' or 'b'

    fmt_a = Column(Text)  # e.g., direct, short_cot, code, long_cot
    fmt_b = Column(Text)
    
    difficulty = Column(Text)
    
    features = Column(JSON)  # Optional: extra metadata
    run_id = Column(Text)
    source = Column(Text)  # e.g., arm_dataloader, user, 
    
    created_at = Column(DateTime, default=datetime.now(timezone.utc))


    def to_dict(self):
        return {
            "id": self.id,
            "goal": self.goal,
            "prompt": self.prompt,
            "output_a": self.output_a,
            "output_b": self.output_b,
            "preferred": self.preferred,
            "fmt_a": self.fmt_a,
            "fmt_b": self.fmt_b,
            "difficulty": self.difficulty,
            "features": self.features or {},
            "run_id": self.run_id,
            "source": self.source,
            "created_at": self.created_at.isoformat() if self.created_at else None,
        }
---END-OF-FILE---


"co_ai\models\pattern_stat.py"
---START-OF-FILE---
# models/pattern_stat.py
from datetime import datetime, timezone

from sqlalchemy import Column, DateTime, Float, ForeignKey, Integer, String

from co_ai.models.base import Base


class PatternStatORM(Base):
    __tablename__ = "cot_pattern_stats"

    id = Column(Integer, primary_key=True)

    # Foreign keys
    goal_id = Column(Integer, ForeignKey("goals.id"), nullable=False)
    hypothesis_id = Column(Integer, ForeignKey("hypotheses.id"), nullable=False)

    # Agent/Model Info
    model_name = Column(String, nullable=False)
    agent_name = Column(String, nullable=False)

    # Rubric Dimension + Label
    dimension = Column(String, nullable=False)  # e.g., "Inference Style"
    label = Column(String, nullable=False)      # e.g., "Deductive"
    confidence_score = Column(Float)           # Optional numeric score

    # Timestamps
    created_at = Column(DateTime, default=datetime.now(timezone.utc))---END-OF-FILE---


"co_ai\models\pipeline_run.py"
---START-OF-FILE---
# models/pipeline_run.py
from datetime import datetime, timezone

from sqlalchemy import JSON, Column, DateTime, ForeignKey, Integer, String
from sqlalchemy.orm import relationship

from .base import Base


class PipelineRunORM(Base):
    __tablename__ = "pipeline_runs"

    id = Column(Integer, primary_key=True)
    run_id = Column(String, unique=True, nullable=False)
    goal_id = Column(Integer, ForeignKey("goals.id"), nullable=False)
    pipeline = Column(JSON)  # Stored as JSONB or TEXT[]
    strategy = Column(String)
    model_name = Column(String)
    run_config = Column(JSON)
    lookahead_context = Column(JSON)
    symbolic_suggestion = Column(JSON)
    extra_data = Column(JSON)
    created_at = Column(DateTime, default=datetime.now(timezone.utc))

    goal = relationship("GoalORM", back_populates="pipeline_runs")
    prompts = relationship("PromptORM", back_populates="pipeline_run", cascade="all, delete-orphan")
    hypotheses = relationship("HypothesisORM", back_populates="pipeline_run")
    symbolic_rules = relationship("SymbolicRuleORM", back_populates="pipeline_run")

    rule_applications = relationship(
        "RuleApplicationORM",
        back_populates="pipeline_run",
        cascade="all, delete-orphan",
    )
    scores = relationship(
        "ScoreORM", back_populates="pipeline_run", cascade="all, delete-orphan"
    )---END-OF-FILE---


"co_ai\models\prompt_program.py"
---START-OF-FILE---
from sqlalchemy import Column, String, Integer, Float, Text, ForeignKey, JSON
from sqlalchemy.orm import declarative_base, relationship
import uuid

Base = declarative_base()

def generate_uuid():
    return str(uuid.uuid4())

class PromptProgramORM(Base):
    __tablename__ = "prompt_programs"

    id = Column(String, primary_key=True, default=generate_uuid)
    pipeline_run_id = Column(Integer, ForeignKey("pipeline_runs.id", ondelete="SET NULL"), nullable=True)
    prompt_id = Column(Integer, ForeignKey("prompts.id", ondelete="SET NULL"), nullable=True)
    goal = Column(Text, nullable=False)
    template = Column(Text, nullable=False)
    inputs = Column(JSON, default={})
    version = Column(Integer, default=1)
    parent_id = Column(String, ForeignKey("prompt_programs.id"), nullable=True)
    strategy = Column(String, default="default")
    prompt_text = Column(Text, nullable=True)
    hypothesis = Column(Text, nullable=True)
    score = Column(Float, nullable=True)
    rationale = Column(Text, nullable=True)
    mutation_type = Column(String, nullable=True)
    execution_trace = Column(Text, nullable=True)
    extra_data = Column(JSON, default={})

 
    parent = relationship("PromptProgramORM", remote_side=[id], backref="children")
    prompt = relationship("PromptORM", backref="prompt_programs")
    pipeline_run = relationship("PipelineRunORM", back_populates="prompt_programs")

    def to_dict(self):
        return {
            "id": self.id,
            "goal": self.goal,
            "template": self.template,
            "inputs": self.inputs,
            "version": self.version,
            "parent_id": self.parent_id,
            "prompt_id": self.prompt_id,
            "propipeline_run_idmpt_id": self.pipeline_run_id,
            "strategy": self.strategy,
            "prompt_text": self.prompt_text,
            "hypothesis": self.hypothesis,
            "score": self.score,
            "rationale": self.rationale,
            "mutation_type": self.mutation_type,
            "execution_trace": self.execution_trace,
            "extra_data": self.extra_data,
        }
---END-OF-FILE---


"co_ai\models\prompt.py"
---START-OF-FILE---
# models/prompt.py
from datetime import datetime, timezone

from sqlalchemy import (JSON, Boolean, Column, DateTime, ForeignKey, Integer,
                        String, Text)
from sqlalchemy.orm import relationship

from co_ai.models.base import Base


class PromptORM(Base):
    __tablename__ = "prompts"

    id = Column(Integer, primary_key=True)

    # Agent and prompt metadata
    goal_id = Column(Integer, ForeignKey("goals.id"))
    pipeline_run_id = Column(Integer, ForeignKey("pipeline_runs.id", ondelete="SET NULL"), nullable=True)
    agent_name = Column(String, nullable=False)
    prompt_key = Column(String, nullable=False)  # e.g., generation_goal_aligned.txt
    prompt_text = Column(Text, nullable=False)
    response_text = Column(Text)  # Optional â€” if storing model output too
    source = Column(String)  # e.g., manual, dsp_refinement, feedback_injection
    strategy = Column(String)  # e.g., goal_aligned, out_of_the_box
    version = Column(Integer, default=1)
    is_current = Column(Boolean, default=False)
    extra_data = Column(JSON)
    timestamp = Column(DateTime, default=datetime.now(timezone.utc))

    goal = relationship("GoalORM", back_populates="prompts")
    hypotheses = relationship("HypothesisORM", back_populates="prompt")
    symbolic_rules = relationship("SymbolicRuleORM", back_populates="prompt")
    pipeline_run = relationship("PipelineRunORM", back_populates="prompts")

    def to_dict(self, include_relationships: bool = False) -> dict:
        data = {
            "id": self.id,
            "agent_name": self.agent_name,
            "prompt_key": self.prompt_key,
            "prompt_text": self.prompt_text,
            "response_text": self.response_text,
            "goal_id": self.goal_id,
            "source": self.source,
            "strategy": self.strategy,
            "version": self.version,
            "is_current": self.is_current,
            "extra_data": self.extra_data,
            "timestamp": self.timestamp.isoformat() if self.timestamp else None,
        }

        if include_relationships:
            data["goal"] = self.goal.to_dict() if self.goal else None
            data["hypotheses"] = [h.to_dict() for h in self.hypotheses] if self.hypotheses else []

        return data
---END-OF-FILE---


"co_ai\models\reflection_delta.py"
---START-OF-FILE---
# models/reflection_delta.py
from datetime import datetime, timezone

from sqlalchemy import (JSON, Boolean, Column, DateTime, Float, ForeignKey,
                        Integer, String)
from sqlalchemy.orm import relationship

from .base import Base


class ReflectionDeltaORM(Base):
    __tablename__ = "reflection_deltas"

    id = Column(Integer, primary_key=True)
    goal_id = Column(Integer, ForeignKey("goals.id"))
    run_id_a = Column(String, nullable=False)
    run_id_b = Column(String, nullable=False)
    score_a = Column(Float)
    score_b = Column(Float)
    score_delta = Column(Float)
    pipeline_a = Column(JSON)
    pipeline_b = Column(JSON)
    pipeline_diff = Column(JSON)
    strategy_diff = Column(Boolean, default=False)
    model_diff = Column(Boolean, default=False)
    rationale_diff = Column(JSON)
    created_at = Column(DateTime, default=datetime.now(timezone.utc))

    goal = relationship("GoalORM", back_populates="reflection_deltas")---END-OF-FILE---


"co_ai\models\rule_application.py"
---START-OF-FILE---
# models/rule_application.py

from datetime import datetime, timezone

from sqlalchemy import (JSON, Column, DateTime, Float, ForeignKey, Integer,
                        String, Text)
from sqlalchemy.orm import relationship

from .base import Base


class RuleApplicationORM(Base):
    __tablename__ = "rule_applications"

    id = Column(Integer, primary_key=True)
    rule_id = Column(Integer, ForeignKey("symbolic_rules.id", ondelete="CASCADE"))
    goal_id = Column(Integer, ForeignKey("goals.id", ondelete="CASCADE"))
    pipeline_run_id = Column(Integer, ForeignKey("pipeline_runs.id", ondelete="CASCADE"))
    hypothesis_id = Column(Integer, ForeignKey("hypotheses.id"), nullable=True)
    context_hash = Column(String, index=True)

    applied_at = Column(DateTime, default=datetime.now(timezone.utc))
    agent_name = Column(String, nullable=True)
    change_type = Column(String, nullable=True)  # e.g., pipeline_override, hint, etc.
    details = Column(JSON, nullable=True)        # Can store {old:..., new:...}
    stage_details = Column(JSON, nullable=True)

    post_score = Column(Float, nullable=True)
    pre_score = Column(Float, nullable=True)
    delta_score = Column(Float, nullable=True)
    evaluator_name = Column(String, nullable=True)
    rationale = Column(Text, nullable=True)
    notes = Column(Text, nullable=True)

    # Relationships (optional, if you want to access related objects)
    goal = relationship("GoalORM", back_populates="rule_applications")
    rule = relationship("SymbolicRuleORM", back_populates="applications")
    pipeline_run = relationship("PipelineRunORM", back_populates="rule_applications")
    hypothesis = relationship("HypothesisORM", back_populates="rule_applications")


    def to_dict(self) -> dict:
        return {
            "id": self.id,
            "rule_id": self.rule_id,
            "goal_id": self.goal_id,
            "pipeline_run_id": self.pipeline_run_id,
            "hypothesis_id": self.hypothesis_id,
            "applied_at": self.applied_at.isoformat() if self.applied_at else None,
            "agent_name": self.agent_name,
            "change_type": self.change_type,
            "details": self.details,
            "post_score": self.post_score,
            "pre_score": self.pre_score,
            "delta_score": self.delta_score,
            "evaluator_name": self.evaluator_name,
            "rationale": self.rationale,
            "notes": self.notes
        }
---END-OF-FILE---


"co_ai\models\score_dimension.py"
---START-OF-FILE---
# co_ai/db/orm/score_dimension.py
from sqlalchemy import Column, Integer, String, Float, Text, JSON
from sqlalchemy.orm import declarative_base, relationship

Base = declarative_base()

class ScoreDimensionORM(Base):
    __tablename__ = "score_dimensions"

    id = Column(Integer, primary_key=True)
    name = Column(String, unique=True, nullable=False)  # e.g., 'clarity'
    stage = Column(String, nullable=True)               # e.g., 'review', 'reflection'
    prompt_template = Column(Text, nullable=False)      # Template with {goal}, {hypothesis}, etc.
    weight = Column(Float, default=1.0)

    # Optional relationships or metadata fields
    notes = Column(Text, nullable=True)
    extra_data = Column(JSON, nullable=True)  # Flexible config extension

    # def to_score_dimension(self, parser_fn):
    #     from mrq.scoring import ScoreDimension
    #     return ScoreDimension(
    #         name=self.name,
    #         prompt_template=self.prompt_template,
    #         weight=self.weight,
    #         parser=parser_fn,
    #     )
    
    def to_dict(self):
        return {
            "id": self.id,
            "name": self.name,
            "stage": self.stage,
            "prompt_template": self.prompt_template,
            "weight": self.weight,
            "notes": self.notes,
            "extra_data": self.extra_data or {},
        }
    ---END-OF-FILE---


"co_ai\models\score_rule_link.py"
---START-OF-FILE---
from sqlalchemy import Column, DateTime, ForeignKey, Integer, func
from sqlalchemy.orm import relationship

from co_ai.models.base import Base


class ScoreRuleLinkORM(Base):
    __tablename__ = "score_rule_links"

    id = Column(Integer, primary_key=True)
    score_id = Column(Integer, ForeignKey("scores.id", ondelete="CASCADE"), nullable=False)
    rule_application_id = Column(Integer, ForeignKey("rule_applications.id", ondelete="CASCADE"), nullable=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
---END-OF-FILE---


"co_ai\models\score.py"
---START-OF-FILE---
# models/score.py
from datetime import datetime, timezone

from sqlalchemy import (JSON, Column, DateTime, Float, ForeignKey, Integer,
                        String, Text)
from sqlalchemy.orm import relationship

from co_ai.models.base import Base


class ScoreORM(Base):
    __tablename__ = "scores"

    id = Column(Integer, primary_key=True)
    goal_id = Column(Integer, ForeignKey("goals.id"))
    hypothesis_id = Column(Integer, ForeignKey("hypotheses.id"))
    symbolic_rule_id = Column(Integer, ForeignKey("symbolic_rules.id"), nullable=True)
    pipeline_run_id = Column(Integer, ForeignKey("pipeline_runs.id"), nullable=True)
    agent_name = Column(String, nullable=False)
    model_name = Column(String, nullable=False)
    evaluator_name = Column(String, nullable=False)
    strategy = Column(String)
    reasoning_strategy = Column(String)
    
    scores = Column(JSON, default={})
    
    extra_data = Column(JSON)
    created_at = Column(DateTime, default=datetime.now(timezone.utc))

    goal = relationship("GoalORM", back_populates="scores")
    hypothesis = relationship("HypothesisORM", back_populates="scores")
    symbolic_rule = relationship("SymbolicRuleORM", back_populates="scores")
    pipeline_run = relationship("PipelineRunORM", back_populates="scores")

    def to_dict(self, include_relationships: bool = False) -> dict:
        data = {
            "id": self.id,
            "goal_id": self.goal_id,
            "hypothesis_id": self.hypothesis_id,
            "symbolic_rule_id": self.symbolic_rule_id,
            "pipeline_run_id": self.pipeline_run_id,
            "agent_name": self.agent_name,
            "model_name": self.model_name,
            "evaluator_name": self.evaluator_name,
            "score_type": self.score_type,
            "score": self.score,
            "score_text": self.score_text,
            "strategy": self.strategy,
            "dimensions": self.dimensions,
            "reasoning_strategy": self.reasoning_strategy,
            "rationale": self.rationale,
            "reflection": self.reflection,
            "review": self.review,
            "meta_review": self.meta_review,
            "extra_data": self.extra_data,
            "created_at": self.created_at.isoformat() if self.created_at else None,
        }

        if include_relationships:
            data["goal"] = (
                self.goal.to_dict()
                if self.goal and hasattr(self.goal, "to_dict")
                else None
            )
            data["hypothesis"] = (
                self.hypothesis.to_dict()
                if self.hypothesis and hasattr(self.hypothesis, "to_dict")
                else None
            )

        return data
---END-OF-FILE---


"co_ai\models\search_result.py"
---START-OF-FILE---
# models/search_result.py
from datetime import datetime, timezone

from sqlalchemy import (ARRAY, JSON, Column, DateTime, ForeignKey, Integer,
                        String, Text)

from .base import Base


class SearchResultORM(Base):
    __tablename__ = "search_results"

    id = Column(Integer, primary_key=True)
    query = Column(Text, nullable=False)
    source = Column(String, nullable=False)
    result_type = Column(String)
    title = Column(Text)
    summary = Column(Text)
    url = Column(Text)
    author = Column(String)
    published_at = Column(DateTime)
    tags = Column(ARRAY(String))
    goal_id = Column(Integer, ForeignKey("goals.id"))
    parent_goal = Column(Text)
    strategy = Column(String)
    focus_area = Column(String)
    extra_data = Column(JSON)
    created_at = Column(DateTime, default=datetime.now(timezone.utc))

    # ðŸ” New Fields Below â€” For Knowledge Refinement & Idea Linking
    key_concepts = Column(ARRAY(String))
    technical_insights = Column(ARRAY(String))
    relevance_score = Column(Integer)  # 1â€“10 score for how relevant this is to the goal
    novelty_score = Column(Integer)  # Estimated novelty vs. prior knowledge
    related_ideas = Column(ARRAY(String))  # List of idea IDs or descriptions
    refined_summary = Column(Text)  # A concise, processed summary for downstream agents
    extracted_methods = Column(
        ARRAY(String)
    )  # Techniques or methods described in the result
    domain_knowledge_tags = Column(
        ARRAY(String)
    )  # e.g., "self-modifying", "graph transformer"
    critique_notes = Column(Text)  # Feedback from evaluator agent (Mr Q), if any

    def to_dict(self, include_relationships: bool = False) -> dict:
        return {
            "id": self.id,
            "query": self.query,
            "source": self.source,
            "result_type": self.result_type,
            "title": self.title,
            "summary": self.summary,
            "url": self.url,
            "author": self.author,
            "published_at": self.published_at.isoformat()
            if self.published_at
            else None,
            "tags": self.tags,
            "goal_id": self.goal_id,
            "parent_goal": self.parent_goal,
            "strategy": self.strategy,
            "focus_area": self.focus_area,
            "key_concepts": self.key_concepts,
            "technical_insights": self.technical_insights,
            "relevance_score": self.relevance_score,
            "novelty_score": self.novelty_score,
            "related_ideas": self.related_ideas,
            "extracted_methods": self.extracted_methods,
            "critique_notes": self.critique_notes,
            "extra_data": self.extra_data,
            "created_at": self.created_at.isoformat() if self.created_at else None,
        }
---END-OF-FILE---


"co_ai\models\sharpening_prediction.py"
---START-OF-FILE---
# models/sharpening_prediction.py
from datetime import datetime, timezone

from sqlalchemy import Column, DateTime, Float, ForeignKey, Integer, String
from sqlalchemy.orm import relationship

from co_ai.models.base import Base


class SharpeningPredictionORM(Base):
    __tablename__ = "sharpening_predictions"

    id = Column(Integer, primary_key=True)

    # Goal context
    goal_id = Column(Integer, ForeignKey("goals.id"), nullable=False)

    # Prompt that led to the comparison
    prompt_text = Column(String, nullable=False)

    # Original and evolved outputs
    output_a = Column(String, nullable=False)
    output_b = Column(String, nullable=False)

    # Evaluation results
    preferred = Column(String)  # 'a' or 'b'
    predicted = Column(String)  # 'a' or 'b'

    value_a = Column(Float, nullable=False)
    value_b = Column(Float, nullable=False)

    created_at = Column(DateTime, default=datetime.now(timezone.utc))
    
    # Relationships
    goal = relationship("GoalORM", back_populates="sharpening_predictions")

    def to_dict(self, include_relationships: bool = False) -> dict:
        """
        Convert this ORM instance to a dictionary representation.

        Args:
            include_relationships (bool): Whether to include related objects like GoalORM

        Returns:
            dict: Dictionary of all fields
        """
        result = {
            "id": self.id,
            "goal_id": self.goal_id,
            "prompt_text": self.prompt_text,
            "output_a": self.output_a,
            "output_b": self.output_b,
            "preferred": self.preferred,
            "predicted": self.predicted,
            "value_a": self.value_a,
            "value_b": self.value_b,
            "created_at": self.created_at.isoformat() if self.created_at else None
        }

        if include_relationships:
            result["goal"] = self.goal.to_dict() if self.goal else None

        return result---END-OF-FILE---


"co_ai\models\sharpening_result.py"
---START-OF-FILE---
from datetime import datetime

from sqlalchemy import Boolean, Column, DateTime, Float, String, Text

from co_ai.models.base import \
    Base  # Adjust this import to match your actual Base


class SharpeningResultORM(Base):
    __tablename__ = 'sharpening_results'

    id = Column(String, primary_key=True)  # Optional: Add UUID or auto-increment ID
    goal = Column(Text, nullable=False)
    prompt = Column(Text, nullable=False)
    template = Column(Text, nullable=False)
    original_output = Column(Text, nullable=False)
    sharpened_output = Column(Text, nullable=False)
    preferred_output = Column(Text, nullable=False)
    winner = Column(String, nullable=False)
    improved = Column(Boolean, nullable=False)
    comparison = Column(Text, nullable=False)
    score_a = Column(Float, nullable=False)
    score_b = Column(Float, nullable=False)
    score_diff = Column(Float, nullable=False)
    best_score = Column(Float, nullable=False)
    prompt_template = Column(Text, nullable=True)
    created_at = Column(DateTime, default=datetime.utcnow)

    def to_dict(self, include_nulls=False):
        data = {
            "id": self.id,
            "goal": self.goal,
            "prompt": self.prompt,
            "template": self.template,
            "original_output": self.original_output,
            "sharpened_output": self.sharpened_output,
            "preferred_output": self.preferred_output,
            "winner": self.winner,
            "improved": self.improved,
            "comparison": self.comparison,
            "score_a": self.score_a,
            "score_b": self.score_b,
            "score_diff": self.score_diff,
            "best_score": self.best_score,
            "prompt_template": self.prompt_template,
            "created_at": self.created_at.isoformat() if self.created_at else None,
        }
        if not include_nulls:
            data = {k: v for k, v in data.items() if v is not None}
        return data---END-OF-FILE---


"co_ai\models\symbolic_rule.py"
---START-OF-FILE---
import hashlib
import json
from datetime import datetime

from sqlalchemy import (JSON, Column, DateTime, Float, ForeignKey, Integer,
                        String, Text)
from sqlalchemy.orm import relationship

from .base import Base


class SymbolicRuleORM(Base):
    __tablename__ = "symbolic_rules"

    id = Column(Integer, primary_key=True)

    # General metadata
    source = Column(String)  # e.g., 'manual', 'lookahead', 'pipeline_stage'
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    # New design: generalized rules
    target = Column(String, nullable=False)  # e.g., 'goal', 'agent', 'prompt', 'pipeline', 'hypothesis'
    attributes = Column(JSON)  # What to apply, e.g., {"difficulty": "hard"}
    filter = Column(JSON)  # How to match, e.g., {"goal_type": "research", "strategy": "reasoning"}
    context_hash = Column(String, index=True)  # Hash of (filters + attributes)


    goal_type = Column(String)
    goal_category = Column(String)
    difficulty = Column(String)
    focus_area = Column(String)

    # Optional linkage (for legacy/rule provenance/debugging)
    goal_id = Column(Integer, ForeignKey("goals.id"), nullable=True)
    pipeline_run_id = Column(Integer, ForeignKey("pipeline_runs.id"), nullable=True)
    prompt_id = Column(Integer, ForeignKey("prompts.id"), nullable=True)
    agent_name = Column(String, nullable=True)

    # Optional scoring/description
    score = Column(Float)
    rule_text = Column(Text)

    # Relationships
    goal = relationship("GoalORM", back_populates="symbolic_rules", lazy="joined")
    pipeline_run = relationship("PipelineRunORM", back_populates="symbolic_rules", lazy="joined")
    prompt = relationship("PromptORM", back_populates="symbolic_rules", lazy="joined")
    scores = relationship("ScoreORM", back_populates="symbolic_rule")
    applications = relationship(
        "RuleApplicationORM", back_populates="rule", cascade="all, delete-orphan"
    )

    def __str__(self):
        return (
            f"<SymbolicRuleORM id={self.id} "
            f"target={self.target} "
            f"filter={json.dumps(self.filter, sort_keys=True)} "
            f"attributes={json.dumps(self.attributes, sort_keys=True)} "
            f"context_hash={self.context_hash[:8]}... "
            f"source={self.source} "
            f"agent={self.agent_name}>"
        )

    @staticmethod
    def compute_context_hash(filters: dict, attributes: dict) -> str:
        merged = {"filters": filters or {}, "attributes": attributes or {}}
        canonical_str = json.dumps(merged, sort_keys=True)
        return hashlib.sha256(canonical_str.encode("utf-8")).hexdigest()

    def to_dict(self, include_relationships=False):
        return {
            "id": self.id,
            "target": self.target,
            "filter": self.filter,
            "attributes": self.attributes,
            "context_hash": self.context_hash,
            "source": self.source,
            "goal_id": self.goal_id,
            "pipeline_run_id": self.pipeline_run_id,
            "prompt_id": self.prompt_id,
            "agent_name": self.agent_name,
            "score": self.score,
            "rule_text": self.rule_text,
            "created_at": self.created_at.isoformat() if self.created_at else None,
            "updated_at": self.updated_at.isoformat() if self.updated_at else None,
        }
---END-OF-FILE---


"co_ai\parsers\__init__.py"
---START-OF-FILE---
"""Parser utils"""
from .utils import extract_hypotheses
---END-OF-FILE---


"co_ai\parsers\utils.py"
---START-OF-FILE---
import re


def extract_hypotheses(text: str):
    # First attempt: Try precise regex-based extraction
    pattern = re.compile(
        r"(# Hypothesis\s+\d+\s*\n(?:.*?))(?:\n(?=# Hypothesis\s+\d+)|\Z)",
        re.IGNORECASE | re.DOTALL,
    )
    matches = list(pattern.finditer(text))

    if matches:
        return [match.group(1).strip() for match in matches]

    # Fallback (if needed)
    split_parts = re.split(r"\bHypothesis\s+\d+\b", text, flags=re.IGNORECASE)
    if len(split_parts) <= 1:
        return [text]

    hypotheses = []
    for i, part in enumerate(split_parts[1:], start=1):
        cleaned = part.strip()
        if cleaned:
            hypotheses.append(f"Hypothesis {i} {cleaned}")

    return hypotheses
---END-OF-FILE---


"co_ai\prompts\__init__.py"
---START-OF-FILE---
from .prompt_loader import PromptLoader
from .strategy_resolver import StrategyResolver
---END-OF-FILE---


"co_ai\prompts\prompt_loader.py"
---START-OF-FILE---
import os

from jinja2 import Template

from co_ai.constants import (DEFAULT, FILE, NAME, PROMPT_DIR, PROMPT_FILE,
                             PROMPT_MODE, STRATEGY)


def get_text_from_file(file_path: str) -> str:
    """Reads and returns stripped text from a file."""
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read().strip()


class PromptLoader:
    def __init__(self, memory=None, logger=None):
        self.memory = memory
        self.logger = logger

    def load_prompt(self, config: dict, context: dict) -> str:
        """
        Load and render a prompt based on the strategy defined in the agent config.
        Supports: file, template, tuning, or static.
        """
        prompt_type = config.get(PROMPT_MODE, FILE)
        prompts_dir = context.get(PROMPT_DIR, "prompts")

        if not os.path.isdir(prompts_dir):
            raise FileNotFoundError(f"Prompt directory not found: {prompts_dir}")

        merged = self._merge_context(config, context)

        try:
            if prompt_type == "static":
                prompt_text = config.get("prompt_text")
                if not prompt_text:
                    raise ValueError("Missing 'prompt_text' in static config.")
                return Template(prompt_text).render(**merged)

            if prompt_type == "tuning":
                agent_name = config.get(NAME, "default")
                return self._load_best_version(agent_name, context.get("goal", ""), merged)

            return self._load_from_file(merged)

        except Exception as e:
            print(f"âŒ Exception:  {type(e).__name__}: {e}")
            if self.logger:
                self.logger.log("PromptLoadFailed", {
                    "agent": config.get(NAME, DEFAULT),
                    "error": str(e),
                    "config_used": config
                })
            return self._fallback_prompt(context.get("goal", ""))

    def from_file(self, file_name: str, config: dict, context: dict) -> str:
        """Manually load and render a prompt file."""
        path = self.get_file_path(file_name, config, context)
        prompt_text = get_text_from_file(path)
        merged = self._merge_context(config, context)
        return Template(prompt_text).render(**merged)

    @staticmethod
    def get_file_path(file_name: str, cfg: dict, context: dict) -> str:
        """Builds full prompt file path."""
        prompts_dir = context.get(PROMPT_DIR, "prompts")
        filename = file_name if file_name.endswith(".txt") else f"{file_name}.txt"
        return os.path.join(prompts_dir, cfg.get("name", "default"), filename)

    def _load_from_file(self, config: dict) -> str:
        """Loads and renders a prompt file based on config."""
        prompt_dir = config.get(PROMPT_DIR, "prompts")
        file_key = config.get(PROMPT_FILE) or config.get(STRATEGY) or DEFAULT
        file_name = f"{file_key}.txt" if not file_key.endswith(".txt") else file_key
        path = os.path.join(prompt_dir, config.get(NAME, "default"), file_name)

        if not os.path.exists(path):
            if self.logger:
                self.logger.log("PromptFileNotFound", {"path": path, "agent": config.get(NAME, DEFAULT)})
            raise FileNotFoundError(f"Prompt file not found: {path}")

        try:
            prompt_text = get_text_from_file(path)
            return Template(prompt_text).render(**self._merge_context(config, {}))
        except KeyError as ke:
            if self.logger:
                self.logger.log("PromptFormattingError", {
                    "missing_key": str(ke),
                    "path": path
                })
            return prompt_text  # Fallback: return raw

    def _load_best_version(self, agent_name: str, goal: str, config: dict) -> str:
        """Load a tuned version of the prompt if available."""
        best_prompt = self.memory.prompt.get_best_prompt_for_agent(
            agent_name=agent_name,
            strategy=config.get(STRATEGY, DEFAULT),
            goal=goal
        )
        if best_prompt:
            return best_prompt["prompt_text"]

        if self.logger:
            self.logger.log("UsingFallbackPrompt", {"reason": "no_tuned_prompt_found"})

        return self._load_from_file(config)

    def _fallback_prompt(self, goal: str = "") -> str:
        """Minimal backup prompt if nothing else works."""
        return f"Generate hypothesis for goal: {goal or '[unspecified goal]'}"

    @staticmethod
    def _merge_context(config: dict, context: dict) -> dict:
        """Merges agent config and pipeline context."""
        return {**context, **config}
---END-OF-FILE---


"co_ai\prompts\strategy_resolver.py"
---START-OF-FILE---
import os

from co_ai.constants import GOAL


class StrategyResolver:
    def __init__(self, config):
        self.base_prompt_dir = getattr(config, "prompt_dir", "prompts/general_reasoner")
        self.default_strategy = getattr(config, "strategy", "cot")
        self.agent_name = getattr(config, "name", "general_reasoner")

    def resolve(self, context):
        """
        Determines the reasoning strategy and prompt path based on the context and config.
        Context should include goal metadata such as type, strategy, etc.

        Parameters:
            context (dict): Contains the goal object and optionally agent metadata.

        Returns:
            (str, str): Tuple of (strategy, prompt_file_path)
        """
        goal = context.get(GOAL).get("goal_text")
        is_reasoning_agent = context.get("agent_type", "reasoning") == "reasoning"

        if is_reasoning_agent:
            strategy = goal.get("reasoning_strategy", self.default_strategy)
        else:
            strategy = self.default_strategy

        filename = f"{strategy}_{self.agent_name}.j2"
        full_path = os.path.join(self.base_prompt_dir, filename)

        if not os.path.isfile(full_path):
            raise FileNotFoundError(f"Prompt template not found for strategy '{strategy}': {full_path}")

        return strategy, full_path
---END-OF-FILE---


"co_ai\reports\__init__.py"
---START-OF-FILE---
from .formatter import ReportFormatter
from .general_reason import save_markdown_report
---END-OF-FILE---


"co_ai\reports\formatter.py"
---START-OF-FILE---
import re
from datetime import datetime, timezone
from pathlib import Path


class ReportFormatter:
    def __init__(self, output_dir="reports"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def format_report(self, context):
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")

        item = context.get("goal")
        if isinstance(item, str):
            goal = item
        else:
            goal = item.get("goal_text", "Error No Goal")

        safe_goal = sanitize_goal_for_filename(goal)
        file_name = f'{safe_goal}_{timestamp}_report.md'
        file_path = self.output_dir / file_name

        content = f"""# ðŸ§ª AI Co-Research Summary Report

**ðŸ—‚ï¸ Run ID:** `{context.get("run_id", "Error No Run_id")}`  
**ðŸŽ¯ Goal:** *{goal}*  
**ðŸ“… Timestamp:** {timestamp}

---

### ðŸ”¬ Literature:
{self._format_list(context.get("literature", []))}



### ðŸ”¬ Hypotheses Generated:
{self._format_list([h if isinstance(h, str) else h.get("text", "") for h in context.get("hypotheses", [])])}
OK so 


---

### ðŸªž Reflections:
{self._format_reflections(context.get("reflections", []))}


---

### ðŸ§  Persona Reviews:
{self._format_reviews(context.get("reviews", []))}

---

### ðŸ§¬ Evolution Outcome:
- {len(context.get("evolved", []))} hypotheses evolved.

---

### ðŸ“˜ Meta-Review Summary:
> {context.get("meta_review", "")}


### ðŸ“˜ Feedback:
{context.get("feedback", "")}

### ðŸ“˜ DB Matches:
{context.get("proximity", {}).get("database_matches", [])}


---
"""

        file_path.write_text(content, encoding="utf-8")
        return str(file_path)

    def _format_list(self, items):
        return "\n".join(f"1. **{item.strip()}**" for item in items)

    def _format_reviews(self, reviews):
        if not reviews:
            return "No reviews recorded."
        formatted = []
        for r in reviews:
            persona = r.get("persona", "Unknown")
            review = r.get("review", "")
            formatted.append(f"**{persona}:**\n> {review}")
        return "\n\n".join(formatted)

    def _format_reflections(self, reflections):
        if not reflections:
            return "No reflections recorded."
        formatted = []
        for r in reflections:
            reflection = r.get("reflection", "")
            formatted.append(f"**Reflection:**\n> {reflection}")
        return "\n\n".join(formatted)



def sanitize_goal_for_filename(goal: str, length:int=40) -> str:
    """
    Converts a goal string into a safe filename:
    - Replaces non-alphanumeric characters with underscores
    - Truncates to 100 characters
    - Appends a UTC timestamp
    """
    safe = re.sub(r'[^a-zA-Z0-9]', '_', goal)  # Replace non-alphanumeric
    safe = safe[:length]                                       # Limit to (len) characters
    return safe
---END-OF-FILE---


"co_ai\reports\general_reason.py"
---START-OF-FILE---
from datetime import datetime, timezone
from pathlib import Path


def save_markdown_report(result: dict, out_dir: str = "./reports"):
    goal = result["goal"]["goal_text"]
    run_id = result.get("run_id", datetime.now(timezone.utc).isoformat())
    timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d_%H-%M")

    report_md = f"""# General Reasoner Run Report

**Run ID:** {run_id}  
**Timestamp:** {timestamp}  
**Goal:** {goal}

---

## ðŸ§  Hypotheses

"""
    for hyp in result["hypotheses"]:
        strategy = hyp.get("strategy", hyp["features"].get("strategy", "unknown"))
        report_md += f"""### Strategy: `{strategy}`  
{hyp['text']}

---
"""

    report_md += "\n## ðŸ§ª Judgments\n\n"
    for score in result["scoring"]:
        strategy_a = score["hypothesis_a"][:50].replace('\n', ' ') + "..."
        strategy_b = score["hypothesis_b"][:50].replace('\n', ' ') + "..."
        winner = score["winner"]
        reason = score["reason"]
        report_md += f"""- **Winner:** `{winner}`  
  - A: {strategy_a}  
  - B: {strategy_b}  
  - Reason: {reason}\n\n"""

    # Optional: Add per-strategy summary
    from collections import defaultdict
    score_stats = defaultdict(list)
    for score in result["scoring"]:
        score_stats[score["winner"]].append(score.get("score_b", 0))  # use score_b as winner

    report_md += "\n## ðŸ“Š Strategy Performance\n\n"
    report_md += "| Strategy | Judged Wins | Avg Score |\n|----------|--------------|------------|\n"
    for strategy, scores in score_stats.items():
        avg_score = round(sum(scores) / len(scores), 2)
        report_md += f"| {strategy} | {len(scores)} | {avg_score} |\n"

    # Save the markdown
    Path(out_dir).mkdir(parents=True, exist_ok=True)
    out_path = Path(out_dir) / f"general_reasoner_report_{timestamp}.md"
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(report_md)

    print(f"âœ… Markdown report saved to {out_path}")
---END-OF-FILE---


"co_ai\rules\__init__.py"
---START-OF-FILE---
from .symbolic_rule_applier import SymbolicRuleApplier
from .rule_tuner import RuleTuner---END-OF-FILE---


"co_ai\rules\rule_tuner.py"
---START-OF-FILE---

class RuleTuner:
    """
    Provides utilities to modify or tune symbolic rules based on performance analytics.
    """

    def __init__(self, memory, logger):
        self.memory = memory
        self.logger = logger

    def increase_priority(self, rule_id: int, amount: float = 0.1) -> float:
        """
        Increases the priority of the rule by a given amount. If no priority is set, defaults to 1.0.
        """
        rule = self.memory.symbolic_rules.get(rule_id)
        if not rule:
            self.logger.log("RuleNotFound", {"rule_id": rule_id})
            return None

        current_priority = rule.attributes.get("priority", 1.0)
        new_priority = round(float(current_priority) + amount, 4)
        rule.attributes["priority"] = new_priority

        self.memory.symbolic_rules.update(rule)
        self.logger.log("RulePriorityUpdated", {
            "rule_id": rule_id,
            "old_priority": current_priority,
            "new_priority": new_priority,
        })

        return new_priority

    def decrease_priority(self, rule_id: int, amount: float = 0.1) -> float:
        """
        Decreases the priority of the rule by a given amount (min 0.0).
        """
        rule = self.memory.symbolic_rules.get(rule_id)
        if not rule:
            self.logger.log("RuleNotFound", {"rule_id": rule_id})
            return None

        current_priority = rule.attributes.get("priority", 1.0)
        new_priority = max(0.0, round(float(current_priority) - amount, 4))
        rule.attributes["priority"] = new_priority

        self.memory.symbolic_rules.update(rule)
        self.logger.log("RulePriorityUpdated", {
            "rule_id": rule_id,
            "old_priority": current_priority,
            "new_priority": new_priority,
        })

        return new_priority
---END-OF-FILE---


"co_ai\rules\symbolic_rule_applier.py"
---START-OF-FILE---
import hashlib
import json
from pathlib import Path
from typing import Any, Dict

import yaml

from co_ai.memory.symbolic_rule_store import SymbolicRuleORM


class SymbolicRuleApplier:
    def __init__(self, cfg, memory, logger):
        self.cfg = cfg
        self.memory = memory
        self.logger = logger
        self.enabled = cfg.get("symbolic", {}).get("enabled", False)
        self.rules = self._load_rules()

    def apply(self, context: dict) -> dict:
        if not self.enabled:
            return context

        goal = context.get("goal", {})
        pipeline_run_id = context.get("pipeline_run_id")
        current_pipeline = context.get("pipeline", [])

        matching_rules = [r for r in self.rules if self._matches_metadata(r, goal)]

        if not matching_rules:
            self.logger.log("NoSymbolicRulesApplied", {"goal_id": goal.get("id")})
            return context

        self.logger.log("SymbolicRulesFound", {"count": len(matching_rules)})

        for rule in matching_rules:
            if rule.rule_text and "pipeline:" in rule.rule_text:
                suggested_pipeline = (
                    rule.rule_text.split("pipeline:")[-1].strip().split(",")
                )
                suggested_pipeline = [
                    s.strip() for s in suggested_pipeline if s.strip()
                ]
                if suggested_pipeline:
                    self.logger.log(
                        "PipelineUpdatedBySymbolicRule",
                        {
                            "from": current_pipeline,
                            "to": suggested_pipeline,
                            "rule_id": rule.id,
                        },
                    )
                    context["pipeline"] = suggested_pipeline
                    context["pipeline_updated_by_symbolic_rule"] = True

            if rule.source == "lookahead" and rule.goal_type:
                context["symbolic_hint"] = f"use_{rule.goal_type.lower()}_strategy"

        return context

    from datetime import datetime

    from co_ai.models import RuleApplicationORM

    def apply_to_agent(self, cfg: Dict, context: Dict) -> Dict:
        if not self.enabled:
            return cfg

        goal = context.get("goal", {})
        pipeline_run_id = context.get("pipeline_run_id")
        agent_name = cfg.get("name")

        matching_rules = [
            r
            for r in self.rules
            if r.agent_name == agent_name and self._matches_metadata(r, goal)
        ]

        if not matching_rules:
            self.logger.log(
                "NoSymbolicAgentRulesApplied",
                {
                    "agent": agent_name,
                    "goal_id": goal.get("id"),
                },
            )
            return cfg

        self.logger.log(
            "SymbolicAgentRulesFound",
            {
                "agent": agent_name,
                "goal_id": goal.get("id"),
                "count": len(matching_rules),
            },
        )

        for rule in matching_rules:
            # Apply new-style attributes
            if rule.attributes:
                for key, value in rule.attributes.items():
                    if key in cfg:
                        self.logger.log(
                            "SymbolicAgentOverride",
                            {
                                "agent": agent_name,
                                "key": key,
                                "old_value": cfg[key],
                                "new_value": value,
                                "rule_id": rule.id,
                            },
                        )
                    else:
                        self.logger.log(
                            "SymbolicAgentNewKey",
                            {
                                "agent": agent_name,
                                "key": key,
                                "value": value,
                                "rule_id": rule.id,
                            },
                        )
                    cfg[key] = value

            # Apply legacy rule_text (optional, for backward compatibility)
            if rule.rule_text:
                entries = [e.strip() for e in rule.rule_text.split(",") if e.strip()]
                for entry in entries:
                    if ":" in entry:
                        key, value = [s.strip() for s in entry.split(":", 1)]
                        if key in cfg:
                            self.logger.log(
                                "SymbolicAgentOverride",
                                {
                                    "agent": agent_name,
                                    "key": key,
                                    "old_value": cfg[key],
                                    "new_value": value,
                                    "rule_id": rule.id,
                                },
                            )
                        else:
                            self.logger.log(
                                "SymbolicAgentNewKey",
                                {
                                    "agent": agent_name,
                                    "key": key,
                                    "value": value,
                                    "rule_id": rule.id,
                                },
                            )
                        cfg[key] = value

            # Record the application of this rule
            self.memory.rule_effects.insert(
                goal_id=goal.get("id"),
                agent_name=agent_name,
                rule_id=rule.id,
                pipeline_run_id=pipeline_run_id,
                details=rule.to_dict(),
                stage_details=cfg,
            )

        return cfg


    def apply_prompt_rules(
            self, agent_name: str, prompt_cfg: dict, context: dict
        ) -> dict:
        """
        Applies prompt-level symbolic rules to the prompt config before generation.

        Returns the updated prompt_cfg.
        """
        goal = context.get("goal", {})
        applicable_rules = [
            rule
            for rule in self.rules
            if rule.agent_name == agent_name
            # and self._matches_filter(rule.filter, goal)
        ]

        if not applicable_rules:
            self.logger.log("NoPromptRulesFound", {"agent": agent_name})
            return prompt_cfg

        for rule in applicable_rules:
            for key, value in rule.attributes.items():
                self.logger.log(
                    "PromptAttributeOverride",
                    {
                        "agent": agent_name,
                        "key": key,
                        "old_value": prompt_cfg.get(key),
                        "new_value": value,
                        "rule_id": rule.id,
                        "emoji": "ðŸ› ï¸",
                    },
                )
                self.set_nested(prompt_cfg, key, value)

            # Optional: record the rule application
            self.memory.rule_effects.insert(
                rule_id=rule.id,
                goal_id=goal.get("id"),
                pipeline_run_id=context.get("pipeline_run_id"),
                details=prompt_cfg,
            )

        return prompt_cfg

    def set_nested(self, cfg: dict, dotted_key: str, value):
        keys = dotted_key.split(".")
        d = cfg
        for k in keys[:-1]:
            if k not in d or not isinstance(d[k], dict):
                d[k] = {}
            d = d[k]
        d[keys[-1]] = value

    def apply_to_prompt(self, cfg: Dict, context: Dict) -> Dict:
        if not self.enabled:
            return cfg

        goal = context.get("goal", {})
        pipeline_run_id = context.get("pipeline_run_id")
        prompt_name = cfg.get("prompt_key", "unknown_prompt")

        matching_rules = [
            r for r in self.rules
            if r.target == "prompt" and self._matches_filter(r.filter, goal)
        ]

        if not matching_rules:
            self.logger.log("NoSymbolicPromptRulesApplied", {
                "prompt": prompt_name,
                "goal_id": goal.get("id"),
            })
            return cfg

        self.logger.log("SymbolicPromptRulesFound", {
            "prompt": prompt_name,
            "goal_id": goal.get("id"),
            "count": len(matching_rules),
        })

        for rule in matching_rules:
            for key, value in rule.attributes.items():
                if key in cfg:
                    self.logger.log("SymbolicPromptOverride", {
                        "prompt": prompt_name,
                        "key": key,
                        "old_value": cfg[key],
                        "new_value": value,
                        "rule_id": rule.id,
                    })
                else:
                    self.logger.log("SymbolicPromptNewKey", {
                        "prompt": prompt_name,
                        "key": key,
                        "value": value,
                        "rule_id": rule.id,
                    })
                cfg[key] = value

            # Track the application of the prompt-level rule
            self.memory.rule_effects.insert(
                rule_id=rule.id,
                goal_id=goal.get("id"),
                pipeline_run_id=pipeline_run_id,
                agent_name=cfg.get("name", "prompt"),
                context_hash=self.compute_context_hash(context),
                run_id=context.get("run_id"),
            )

        return cfg

    def _matches_filter(self, filter_dict: dict, target_obj: dict) -> bool:
        """Generic matcher for symbolic rule filters"""
        for key, value in filter_dict.items():
            target_value = target_obj.get(key)
            if isinstance(value, list):
                if target_value not in value:
                    return False
            else:
                if target_value != value:
                    return False
        return True


    def track_pipeline_stage(self, stage_dict: dict, context: dict):
        self.memory.symbolic_rules.track_pipeline_stage(stage_dict, context)

    def _load_rules(self):
        rules = []
        symbolic_dict = self.cfg.get("symbolic", {})
        if symbolic_dict.get("rules_file"):
            rules += self._load_rules_from_yaml(symbolic_dict.rules_file)
        if symbolic_dict.get("enable_db_rules", True):
            rules += self.memory.symbolic_rules.get_all_rules()
        return rules

    def _load_rules_from_yaml(self, path: str) -> list:
        if not Path(path).exists():
            self.logger.log("SymbolicRuleYAMLNotFound", {"path": path})
            return []

        with open(path, "r", encoding="utf-8") as f:
            raw = yaml.safe_load(f)

        rules_list = raw.get("rules", raw)

        rules = []
        existing_rules = {
            r.rule_text for r in self.memory.symbolic_rules.get_all_rules()
        }
        for item in rules_list:
            if isinstance(item, dict) and item.get("rule_text") not in existing_rules:
                rules.append(SymbolicRuleORM(**item))
            else:
                self.logger.log(
                    "DuplicateSymbolicRuleSkipped", {"rule_text": item.get("rule_text")}
                )
        return rules

    def _matches_metadata(self, rule: SymbolicRuleORM, goal: Dict[str, Any]) -> bool:
        if rule.goal_id and rule.goal_id != goal.get("id"):
            return False
        if rule.goal_type and rule.goal_type != goal.get("goal_type"):
            return False
        if rule.goal_category and rule.goal_category != goal.get("goal_category"):
            return False
        if rule.difficulty and rule.difficulty != goal.get("difficulty"):
            return False
        if hasattr(goal, "focus_area") and rule.goal_category:
            if rule.goal_category != goal.get("focus_area"):
                return False
        return True

    @staticmethod
    def compute_context_hash(context_dict: dict) -> str:
        canonical_str = json.dumps(context_dict, sort_keys=True)
        return hashlib.sha256(canonical_str.encode("utf-8")).hexdigest()
---END-OF-FILE---


"co_ai\scoring\__init__.py"
---START-OF-FILE---
from .base_score import BaseScore
from .proximity import ProximityScore
from .reflection import ReflectionScore
from .review import ReviewScore
---END-OF-FILE---


"co_ai\scoring\base_score.py"
---START-OF-FILE---
from abc import ABC, abstractmethod

from co_ai.models import ScoreORM


class BaseScore(ABC):
    name: str = "unnamed"
    default_value: float = 0.0

    def __init__(self, cfg, memory, logger, evaluator_name=None):
        self.memory = memory
        self.logger = logger
        self.agent_name = cfg.get("name")
        self.model_name = cfg.get("model", {}).get("name")
        self.evaluator_name = evaluator_name or self.name

    @abstractmethod
    def compute(self, hypothesis: dict, context:dict) -> float:
        pass

    def get_score(self, hypothesis: dict, context: dict) -> float:
        # 1. If already cached on object
        if hypothesis.get(f"{self.name}_score"):
            return hypothesis[f"{self.name}_score"]

        # 2. Compute and attach
        score = self.compute(hypothesis, context)
        hypothesis[f"{self.name}_score"] = score

        # 3. Store in scores table
        if self.memory:
            # Optional dimensions dict (can be overridden in subclass)
            dimensions = getattr(self, "dimensions", None)

            s = ScoreORM(
                goal_id=hypothesis.get("goal_id"),
                hypothesis_id=hypothesis.get("id"),
                agent_name=self.agent_name,
                model_name=self.model_name,
                evaluator_name=self.evaluator_name,
                score_type=self.name,
                score=score,
                dimensions=dimensions,
                pipeline_run_id=context.get("pipeline_run_id"),
            )
            try:
                self.memory.scores.insert(s)
                self.memory.commit()  # Ensure session commit happens
            except Exception as e:
                self.memory.refresh_session()
                score = self.default_value
                self.logger.log("ScoreInsertFailed", {"error": str(e)})

        # 4. Log
        self.logger.log("ScoreComputed", {
            "type": self.name,
            "score": score,
            "hypothesis_id": hypothesis.get("id")
        })

        return score
---END-OF-FILE---


"co_ai\scoring\proximity.py"
---START-OF-FILE---
# co_ai/scoring/proximity.py
import re

from co_ai.scoring.base_score import BaseScore


class ProximityScore(BaseScore):
    name = "proximity"
    default_value = 0.0

    def compute(self, hypothesis: dict, context: dict) -> float:
        analysis = hypothesis.get("proximity_analysis")
        if not analysis:
            return self.default_value

        try:
            themes = self._extract_block(analysis, "Common Themes Identified")
            grafts = self._extract_block(analysis, "Grafting Opportunities")
            directions = self._extract_block(analysis, "Strategic Directions")
            score = self._heuristic_score(themes, grafts, directions)
            justification = self._generate_justification(themes, grafts, directions)

            structured = {
                "themes": themes,
                "graft_suggestions": grafts,
                "strategic_directions": directions,
                "score": score,
                "justification": justification,
            }

            hypothesis["proximity_structured"] = structured

            self._store_score(
                hypothesis,
                context,
                score_type="proximity_usefulness",
                score_data={
                    "score": score,
                    "rationale": justification,
                    "themes": themes,
                    "grafts": grafts,
                    "directions": directions,
                }
            )

            return score

        except Exception as e:
            self.logger.log("ProximityScoreParseFailed", {
                "error": str(e),
                "snippet": analysis[:300],
            })
            return self.default_value

    def _extract_block(self, text: str, section_title: str) -> list:
        pattern = rf"# {re.escape(section_title)}\n((?:- .+\n?)*)"
        match = re.search(pattern, text)
        if not match:
            return []
        block = match.group(1).strip()
        return [line.strip("- ").strip() for line in block.splitlines() if line.strip()]

    def _heuristic_score(self, themes, grafts, directions) -> float:
        """
        Simple scoring heuristic based on the number of insights generated.
        """
        return min(100.0, 10 * len(themes) + 10 * len(grafts) + 20 * len(directions))

    def _generate_justification(self, themes, grafts, directions) -> str:
        return (
            f"Identified {len(themes)} themes, {len(grafts)} grafting suggestions, "
            f"and {len(directions)} strategic directions."
        )
---END-OF-FILE---


"co_ai\scoring\reflection.py"
---START-OF-FILE---
import re

from co_ai.constants import REFLECTION
from co_ai.models import ScoreORM
from co_ai.scoring.base_score import BaseScore


class ReflectionScore(BaseScore):
    name = REFLECTION
    default_value = 0.0

    def __init__(self, cfg, memory, logger=None):
        super().__init__(cfg, memory, logger)
        self.scores = {}

    def compute(self, hypothesis: dict[str, any], context: dict[str, any]) -> float:
        """
        Compute a composite score from multiple reflection dimensions.
        """
        reflection_text = hypothesis.get(REFLECTION)
        if not reflection_text:
            return self.default_value

        try:
            self.scores = self._extract_scores(reflection_text)
            if not self.scores:
                return self.default_value

            # Cache full dict for later analysis
            hypothesis["reflection_scores"] = self.scores

            # Save each dimension to score store
            for dimension, data in self.scores.items():
                self._store_score(hypothesis, context, dimension, data)


            # Save composite score with dimensions included
            composite = self._composite_score(self.scores)
            dimensions = {k: v["score"] for k, v in self.scores.items()}

            composite_score_obj = ScoreORM(
                goal_id=hypothesis.get("goal_id"),
                hypothesis_id=hypothesis.get("id"),
                agent_name=self.agent_name,
                model_name=self.model_name,
                evaluator_name="ReflectionScore",
                score_type="reflection_composite",
                score=composite,
                rationale="Composite score from structured reflection.",
                dimensions=dimensions,  # âœ… This is new
                pipeline_run_id=context.get("pipeline_run_id"),
                metadata={"source": "structured_reflection"},
            )
            self.memory.scores.insert(composite_score_obj)


            # Return correctness if available, otherwise average
            return self.scores.get("correctness", {}).get("score") or composite

        except Exception as e:
            self.logger.log("ReflectionScoreParseFailed", {
                "hypothesis_id": hypothesis.get("id"),
                "error": str(e),
                "reflection_snippet": reflection_text[:300],
            })
            return self.default_value

    def _extract_scores(self, text: str) -> dict:
        """
        Extracts structured scores and rationales from reflection text.
        Returns a dictionary of dimension â†’ {score, rationale}
        """
        pattern = r"#\s*(?P<dimension>\w+)\s*Assessment\s*Score:\s*(?P<score>\d+\.?\d*)\s*Reasoning:\s*(?P<rationale>.*?)(?=\n#|\Z)"
        matches = re.finditer(pattern, text, re.DOTALL)

        results = {}
        for m in matches:
            dimension = m.group("dimension").lower()
            try:
                score = float(m.group("score"))
            except ValueError:
                continue
            rationale = m.group("rationale").strip()
            results[dimension] = {"score": score, "rationale": rationale}

        # Include self-reward if found
        reward_match = re.search(r"# Self-Reward Score\s*Score\s*\[?(\d{1,3})\]?", text)
        if reward_match:
            results["reward"] = {"score": float(reward_match.group(1)), "rationale": "Self-reward score"}

        return results

    def _composite_score(self, scores: dict) -> float:
        if not scores:
            return self.default_value
        return sum(item["score"] for item in scores.values()) / len(scores)

    def _store_score(self, hypothesis: dict, context: dict, dimension: str, data: dict):
        score_obj = ScoreORM(
            goal_id=hypothesis.get("goal_id"),
            hypothesis_id=hypothesis.get("id"),
            agent_name=self.agent_name,
            model_name=self.model_name,
            evaluator_name="ReflectionScore",
            score_type=dimension,
            score=data["score"],
            rationale=data.get("rationale", ""),
            pipeline_run_id=context.get("pipeline_run_id"),
            metadata={"source": "structured_reflection"},
        )
        self.memory.scores.insert(score_obj)

    def get_score(self, hypothesis: dict, context: dict) -> float:
        """
        Alias for compute(...) â€” used for consistency with other score classes.
        """
        return self.compute(hypothesis, context)
---END-OF-FILE---


"co_ai\scoring\review.py"
---START-OF-FILE---
import re

from co_ai.constants import REVIEW
from co_ai.models import ScoreORM
from co_ai.scoring.base_score import BaseScore


class ReviewScore(BaseScore):
    name = REVIEW
    default_value = 0.0

    def __init__(self, cfg, memory, logger=None):
        super().__init__(cfg, memory, logger)
        self.scores = {}

    def compute(self, hypothesis: dict, context: dict) -> float:
        review_text = hypothesis.get(REVIEW)
        if not review_text:
            return self.default_value

        try:
            self.scores = self._extract_scores(review_text)
            if not self.scores:
                return self.default_value

            # Cache full dict on hypothesis
            hypothesis["review_scores"] = self.scores

            # Save each dimension score individually
            for dimension, data in self.scores.items():
                if dimension == "overall":
                    score_type = "overall"
                    score = data.get("score", 0.0)
                    rationale = data.get("summary", "")
                else:
                    score_type = dimension
                    score = data.get("score", 0.0)
                    rationale = data.get("rationale", "")
                self._store_score(hypothesis, context, score_type, score, rationale)

            # Save composite score with all dimensions in one object
            dimensions = {k: v.get("score", 0.0) for k, v in self.scores.items()}
            composite_score = self.scores.get("overall", {}).get("score", self.default_value)

            composite_score_obj = ScoreORM(
                goal_id=hypothesis.get("goal_id"),
                hypothesis_id=hypothesis.get("id"),
                agent_name=self.agent_name,
                model_name=self.model_name,
                evaluator_name="ReviewScore",
                score_type="review_composite",
                score=composite_score,
                rationale=self.scores.get("overall", {}).get("summary", "Composite from structured review."),
                dimensions=dimensions,
                pipeline_run_id=context.get("pipeline_run_id"),
                metadata={"source": "structured_review"},
            )
            self.memory.scores.insert(composite_score_obj)

            return composite_score

        except Exception as e:
            self.logger.log("ReviewScoreParseFailed", {
                "hypothesis_id": hypothesis.get("id"),
                "error": str(e),
                "review_snippet": review_text[:300],
            })
            return self.default_value

    def _extract_scores(self, text: str) -> dict:
        """
        Extract review scores from structured plain text review output.
        Expected fields:
          - correctness
          - originality
          - clarity
          - relevance
          - overall_score
          - summary
          - suggested_improvements (as list)
        """
        scores = {}

        def extract_int(tag):
            match = re.search(rf"{tag}:\s*(\d+)", text)
            return int(match.group(1)) if match else None

        def extract_text(tag):
            match = re.search(rf"{tag}:\s*(.*?)(?=\n\w+:|\noverall_score:|$)", text, re.DOTALL)
            return match.group(1).strip() if match else ""

        for dim in ["correctness", "originality", "clarity", "relevance"]:
            score = extract_int(dim)
            rationale = extract_text(f"{dim}_rationale")
            if score is not None:
                scores[dim] = {"score": score, "rationale": rationale}

        overall = extract_int("overall_score")
        summary = extract_text("summary")
        improvements = re.findall(r"-\s*(.+)", text.split("suggested_improvements:")[-1]) if "suggested_improvements" in text else []

        scores["overall"] = {
            "score": overall if overall is not None else self.default_value,
            "summary": summary,
            "suggested_improvements": improvements,
        }

        return scores

    def _store_score(self, hypothesis: dict, context: dict, score_type: str, score: float, rationale: str):
        score_obj = ScoreORM(
            goal_id=hypothesis.get("goal_id"),
            hypothesis_id=hypothesis.get("id"),
            agent_name=self.agent_name,
            model_name=self.model_name,
            evaluator_name="ReviewScore",
            score_type=score_type,
            score=score,
            rationale=rationale,
            pipeline_run_id=context.get("pipeline_run_id"),
            metadata={"source": "structured_review"},
        )
        self.memory.scores.insert(score_obj)

    def get_score(self, hypothesis: dict, context: dict) -> float:
        return self.compute(hypothesis, context)
---END-OF-FILE---


"co_ai\tools\__init__.py"
---START-OF-FILE---
"""Tools for inspecting or visualizing pipeline outputs"""
from .arxiv_tool import search_arxiv
from .embedding_tool import get_embedding
from .huggingface_tool import search_huggingface_datasets
from .view_ranking_trace import fetch_ranking_trace, plot_elo_evolution
from .web_search import WebSearchTool
---END-OF-FILE---


"co_ai\tools\arxiv_tool.py"
---START-OF-FILE---
import arxiv


def search_arxiv(queries: list[str], max_results: int = 5) -> list[dict]:
    results = []
    for query in queries:
        search = arxiv.Search(query=query, max_results=max_results)
        for r in search.results():
            results.append({
                "query": query,
                "title": r.title,
                "summary": r.summary,
                "authors": [a.name for a in r.authors],
                "url": r.pdf_url
            })
    return results
---END-OF-FILE---


"co_ai\tools\cos_sim_tool.py"
---START-OF-FILE---
from typing import List, Tuple

import numpy as np


def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """Compute cosine similarity between two vectors."""
    dot = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    return dot / (norm1 * norm2 + 1e-8)  # Avoid division by zero


def get_top_k_similar(
    query: str,
    documents: List[str],
    memory,
    top_k: int = 5
) -> List[Tuple[str, float]]:
    """
    Compute similarity between query and each document, return top_k most similar.
    
    Args:
        query: The input query text.
        documents: A list of document strings.
        get_embedding: Callable that takes a string and returns a vector (np.ndarray).
        top_k: Number of top results to return.
    
    Returns:
        List of (document, similarity_score) tuples.
    """
    query_vec = memory.embedding.get_or_create(query)
    doc_vecs = [memory.embedding.get_or_create(doc) for doc in documents]

    similarities = [cosine_similarity(query_vec, vec) for vec in doc_vecs]
    scored = list(zip(documents, similarities))
    scored.sort(key=lambda x: x[1], reverse=True)

    return scored[:top_k]
---END-OF-FILE---


"co_ai\tools\ddgs_web_search.py"
---START-OF-FILE---
import asyncio

from duckduckgo_search import DDGS


class DDGSWebSearchTool:

    def search(self, query: str, max_results: int =5):
        return DDGS().text(query, max_results=max_results)

    async def search2(self, query: str, max_results: int = 5) -> list[str]:
        def _run_search():
            results = []
            with DDGS() as ddgs:
                for r in ddgs.text(query, max_results=max_results):
                    title = r.get("title", "").strip()
                    body = r.get("body", "").strip()
                    href = r.get("href", "")
                    results.append(f"{title}: {body}\n{href}")
            return results

        try:
            return await asyncio.to_thread(_run_search)
        except Exception as e:
            print(f"âŒ Exception [WebSearchTool] Search error: {type(e).__name__}: {e}")
            return [f"Search failed: {str(e)}"]
---END-OF-FILE---


"co_ai\tools\embedding_tool.py"
---START-OF-FILE---
# co_ai/tools/embedding_tool.py
from collections import OrderedDict

import requests


# Simple in-memory LRU cache
class EmbeddingCache:
    def __init__(self, max_size=10000):
        self.cache = OrderedDict()
        self.max_size = max_size

    def get(self, key):
        if key in self.cache:
            # Move to the end to mark as recently used
            self.cache.move_to_end(key)
            return self.cache[key]
        return None

    def set(self, key, value):
        self.cache[key] = value
        self.cache.move_to_end(key)
        if len(self.cache) > self.max_size:
            self.cache.popitem(last=False)  # Remove least recently used item

embedding_cache = EmbeddingCache(max_size=10000)

def get_embedding(text: str, cfg):
    """
    Get an embedding from Ollama using the configured model.

    Args:
        text (str): The input text to embed.
        cfg (dict)): Configuration containing 'model' and optionally 'endpoint'.

    Returns:
        list[float]: The embedding vector.
    """
    cached = embedding_cache.get(text)
    if cached is not None:
        print("ðŸ” Using cached embedding")
        return cached

    model = cfg.get("embeddings", {}).get("model", "mxbai-embed-large")
    endpoint = cfg.get("embeddings", {}).get("endpoint", "http://localhost:11434/api/embeddings")
    response = requests.post(
        endpoint,
        json={"model": model, "prompt": text},
    )
    response.raise_for_status()
    return response.json().get("embedding")
---END-OF-FILE---


"co_ai\tools\huggingface_tool.py"
---START-OF-FILE---
from huggingface_hub import HfApi


def search_huggingface_datasets(queries: list[str], max_results: int = 5) -> list[dict]:
    api = HfApi()
    results = []

    for query in queries:
        try:
            matches = api.list_datasets(search=query, limit=max_results)
            for ds in matches:
                results.append({
                    "name": ds.id,
                    "description": ds.cardData.get("description", "No description available") if ds.cardData else "No card data"
                })
        except Exception as e:
            results.append({
                "name": query,
                "description": f"Error searching: {str(e)}"
            })

    return results
---END-OF-FILE---


"co_ai\tools\view_ranking_trace.py"
---START-OF-FILE---
# tools/view_ranking_trace.py
from collections import defaultdict

import matplotlib.pyplot as plt
import psycopg2
from tabulate import tabulate

DB_CONFIG = dict(
    dbname="co",
    user="co",
    password="co",
    host="localhost"
)

def fetch_ranking_trace(run_id=None):
    with psycopg2.connect(**DB_CONFIG) as conn:
        with conn.cursor() as cur:
            if run_id:
                cur.execute("""
                    SELECT winner, loser, explanation, created_at
                    FROM ranking_trace
                    WHERE run_id = %s
                    ORDER BY created_at
                """, (run_id,))
            else:
                cur.execute("""
                    SELECT winner, loser, explanation, created_at
                    FROM ranking_trace
                    ORDER BY created_at DESC LIMIT 50
                """)
            return cur.fetchall()

def fetch_elo_scores(run_id=None):
    with psycopg2.connect(**DB_CONFIG) as conn:
        with conn.cursor() as cur:
            if run_id:
                cur.execute("""
                    SELECT hypothesis, score, created_at
                    FROM elo_ranking_log
                    WHERE run_id = %s
                    ORDER BY created_at
                """, (run_id,))
            else:
                cur.execute("""
                    SELECT hypothesis, score, created_at
                    FROM elo_ranking_log
                    ORDER BY created_at DESC LIMIT 50
                """)
            return cur.fetchall()

def display_top_ranked(run_id=None):
    scores = fetch_elo_scores(run_id)
    latest_scores = {}
    for hypo, score, ts in scores:
        latest_scores[hypo] = score
    sorted_scores = sorted(latest_scores.items(), key=lambda x: x[1], reverse=True)
    print("\nTop-Ranked Hypotheses:\n")
    print(tabulate(sorted_scores, headers=["hypotheses", "ELO Score"], tablefmt="grid"))

def plot_elo_evolution(run_id=None):
    scores = fetch_elo_scores(run_id)
    time_series = defaultdict(list)
    for hypo, score, ts in scores:
        time_series[hypo].append((ts, score))

    plt.figure(figsize=(10, 6))
    for hypo, points in time_series.items():
        times, scores = zip(*points)
        plt.plot(times, scores, label=hypo[:40] + ("..." if len(hypo) > 40 else ""))

    plt.xlabel("Time")
    plt.ylabel("ELO Score")
    plt.title("ELO Evolution Over Time")
    plt.legend(loc="best")
    plt.tight_layout()
    plt.xticks(rotation=45)
    plt.show()

def main():
    run_id = input("Enter run_id (or leave blank for latest): ").strip() or None
    print("\n--- Ranking Trace ---")
    rows = fetch_ranking_trace(run_id)
    print(tabulate(rows, headers=["Winner", "Loser", "Explanation", "Time"], tablefmt="grid"))

    print("\n--- Top-Ranked Hypotheses ---")
    display_top_ranked(run_id)

    print("\n--- Plotting ELO Evolution ---")
    plot_elo_evolution(run_id)

if __name__ == "__main__":
    main()
---END-OF-FILE---


"co_ai\tools\web_search.py"
---START-OF-FILE---
import asyncio

import httpx
import requests
from bs4 import BeautifulSoup
from readability import Document

from co_ai.utils.file_utils import write_text_to_file


class WebSearchTool:
    def __init__(self, cfg: dict, logger):
        self.base_url = f'{cfg.get("instance_url", "localhost:8080")}/search'
        self.max_results = cfg.get("max_results", 15)
        self.fetch_page = cfg.get("fetch_page", False)
        self.categories = cfg.get("categories", "general")
        self.language = cfg.get("language", "en")
        self.logger = logger

    async def search(self, query: str, max_results: int = 15) -> list[str] | None:
        max_results = max_results or self.max_results

        params = {
            "q": query,
            "categories": "general",
            "language": self.language,
            "formats": ["html", "json"]
        }

        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                resp = await client.get(self.base_url, params=params)
                resp.raise_for_status()
                html = resp.text

        except Exception as e:
            print(f"âŒ Exception:  {type(e).__name__}: {e}")
            return None

        return self.parse_searxng_results(html, max_results)

    from bs4 import BeautifulSoup

    def parse_searxng_results(self, html: str, max_results:int=20):
        soup = BeautifulSoup(html, "html.parser")
        results = []

        for i, article in enumerate(soup.find_all("article", class_="result")):
            if i > max_results:
                continue
            link_tag = article.find("a", class_="url_header")
            href = link_tag["href"] if link_tag else None

            title_tag = article.find("h3")
            title = title_tag.get_text(strip=True) if title_tag else None

            snippet_tag = article.find("p", class_="content")
            snippet = snippet_tag.get_text(strip=True) if snippet_tag else None

            cleand_page = ""
            if self.fetch_page:
                cleand_page = self.fetch_html(href)

            if href and title:
                results.append(
                    {
                        "title": title,
                        "url": href,
                        "snippet": snippet,
                        "page": cleand_page,
                    }
                )

        return results

    import requests

    def fetch_html(self, url: str) -> str | None:
        headers = {"User-Agent": "Mozilla/5.0"}
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            if self.logger:
                self.logger.log("FetchHTMLFailed", {"url": url, "error": str(e)})
            return None  # or return ""

    def fetch_and_parse_readable(self, url:str):
        html = self.fetch_html(url)
        title, clean_text = self.extract_main_text(html)
        return {"url": url, "title": title, "text": clean_text}


    def extract_main_text(self, html):
        doc = Document(html)
        title = doc.short_title()
        summary_html = doc.summary()

        # Use BeautifulSoup to clean text
        soup = BeautifulSoup(summary_html, 'html.parser')
        clean_text = soup.get_text(separator='\n', strip=True)
        return title, clean_text---END-OF-FILE---


"co_ai\tools\wikipedia_tool.py"
---START-OF-FILE---
import wikipedia

from co_ai.tools.cos_sim_tool import get_top_k_similar


class WikipediaTool:
    def __init__(self, memory, logger, lang="en", top_k=3):
        self.memory = memory
        self.logger = logger
        wikipedia.set_lang(lang)
        self.top_k = top_k

    def search(self, query: str) -> list[dict]:
        self.logger.log("WikipediaSearchStart", {"query": query})
        search_results = wikipedia.search(query)
        articles = []

        for title in search_results[:10]:
            try:
                page = wikipedia.page(title)
                summary = page.summary[:2000]
                article = {"title": title, "summary": summary, "url": page.url}
                articles.append(article)
                self.logger.log("WikipediaArticleFetched", {"article": article})
            except wikipedia.exceptions.DisambiguationError as e:
                self.logger.log("WikipediaDisambiguationSkipped", {"title": title})
                continue
            except Exception as e:
                self.logger.log(
                    "WikipediaFetchFailed", {"title": title, "error": str(e)}
                )
                continue

        self.logger.log(
            "WikipediaSearchComplete", {"query": query, "count": len(articles)}
        )
        return articles

    def find_similar(self, query: str) -> list[dict]:
        self.logger.log("WikipediaSimilaritySearchStart", {"query": query})
        raw_articles = self.search(query)
        if not raw_articles:
            self.logger.log("WikipediaNoResults", {"query": query})
            return []

        summaries = [a["summary"] for a in raw_articles]
        scored = get_top_k_similar(query, summaries, self.memory, top_k=self.top_k)
        self.logger.log(
            "WikipediaSimilarityScores",
            {"scores": [{"summary": s, "score": sc} for s, sc in scored]},
        )

        final = []
        for summary, score in scored:
            match = next((a for a in raw_articles if a["summary"] == summary), None)
            if match:
                result = match | {"score": round(score, 4)}
                final.append(result)
                self.logger.log("WikipediaMatchSelected", {"result": result})

        self.logger.log(
            "WikipediaSimilaritySearchComplete", {"query": query, "top_k": len(final)}
        )
        return final
---END-OF-FILE---


"co_ai\utils\__init__.py"
---START-OF-FILE---
"""
Utility classes
- prompt_loader
- report_formatter
"""
from .file_utils import camel_to_snake, get_text_from_file, write_text_to_file
from .goal_classifier import classify_goal_strategy
from .lru_cache import SimpleLRUCache
from .resource_extractor import extract_resources
from .run_utils import generate_run_id, get_log_file_path
---END-OF-FILE---


"co_ai\utils\file_utils.py"
---START-OF-FILE---
import re


def camel_to_snake(name):
    s1 = re.sub("(.)([A-Z][a-z]+)", r"\1_\2", name)
    return re.sub("([a-z0-9])([A-Z])", r"\1_\2", s1).lower()

def get_text_from_file(file_path: str) -> str:
    """Get text from a file"""
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read().strip()

def write_text_to_file(path: str, text: str):
    try:
        with open(path, 'w', encoding='utf-8') as file:
            file.write(text)
        print(f"âœ… Successfully wrote to {path}")
    except Exception as e:
        print(f"âŒ Failed to write to {path}: {e}")
---END-OF-FILE---


"co_ai\utils\goal_classifier.py"
---START-OF-FILE---
def classify_goal_strategy(goal: dict) -> str:
    desc = goal.get("goal_text", "").lower()
    kw = " ".join(goal.get("keywords", [])).lower()

    if "dataset" in desc or "huggingface" in kw:
        return "dataset-first"
    elif "accuracy" in desc or "evaluation" in kw:
        return "evaluation-centric"
    elif "explain" in desc or "interpret" in kw:
        return "interpretability"
    elif "code" in desc or "repo" in kw:
        return "code-centric"
    elif "survey" in desc or "literature" in kw:
        return "deep_literature"
    else:
        return "default"
---END-OF-FILE---


"co_ai\utils\lru_cache.py"
---START-OF-FILE---
from collections import OrderedDict


class SimpleLRUCache:
    def __init__(self, max_size=10000):
        self.cache = OrderedDict()
        self.max_size = max_size

    def get(self, key):
        if key in self.cache:
            # Move to the end to show it was recently used
            self.cache.move_to_end(key)
            return self.cache[key]
        return None

    def set(self, key, value):
        if key in self.cache:
            # Update value and mark as recently used
            self.cache.move_to_end(key)
            self.cache[key] = value
        else:
            # Evict least recently used if full
            if len(self.cache) >= self.max_size:
                self.cache.popitem(last=False)
            self.cache[key] = value
---END-OF-FILE---


"co_ai\utils\query_generator.py"
---START-OF-FILE---
class GoalQueryGenerator:
    def generate_queries(self, goal: dict, strategy: str) -> list[str]:
        desc = goal.get("goal_text", goal.get("description", ""))
        kw = goal.get("keywords", [])

        if strategy == "deep_literature":
            return kw + [desc + " site:arxiv.org"]
        elif strategy == "dataset-first":
            return kw + [f"Hugging Face dataset {desc}"]
        elif strategy == "code-centric":
            return kw + [f"GitHub {desc}"]
        else:
            return kw or [desc]
---END-OF-FILE---


"co_ai\utils\resource_extractor.py"
---START-OF-FILE---
# co_ai/utils/resource_extractor.py
import os
from pathlib import Path
from typing import List

import pkg_resources

RESOURCE_MAP = {
    "configs": "resources/configs",
    "prompts": "resources/prompts",
    "docker": "resources/docker"
}


def extract_resources(target_dir: str = None):
    """
    Extract all bundled configs and prompts into local directories.
    
    Args:
        target_dir: Optional override of where to extract files
    """
    target_dir = Path(target_dir or os.getcwd())
    
    for resource_type, source_path in RESOURCE_MAP.items():
        src = Path(source_path)
        dest = target_dir / resource_type
        
        # Skip if already exists
        if dest.exists():
            print(f"[ResourceExtractor] {resource_type} folder found. Skipping.")
            continue
            
        print(f"[ResourceExtractor] Copying {resource_type}...")
        dest.mkdir(exist_ok=True)

        # Walk through resource tree
        for root, _, files in os.walk(pkg_resources.resource_filename(__name__, src)):
            rel_root = Path(root).relative_to(pkg_resources.resource_filename(__name__, ""))
            for f in files:
                src_file = rel_root / f
                dst_file = dest / rel_root.relative_to(resource_type) / f
                
                dst_file.parent.mkdir(parents=True, exist_ok=True)
                
                with open(src_file, "rb") as fin:
                    content = fin.read().decode("utf-8")

                with open(dst_file, "w", encoding="utf-8") as fout:
                    fout.write(content)

    print("[ResourceExtractor] Done. Resources extracted to:")
    print(f" - Configs: {target_dir / 'configs'}")
    print(f" - Prompts: {target_dir / 'prompts'}")---END-OF-FILE---


"co_ai\utils\run_utils.py"
---START-OF-FILE---
import os
import re
import uuid
from datetime import datetime, timezone

from omegaconf import DictConfig


def generate_run_id(goal: str) -> str:
    # Extract keywords from goal
    keywords = re.findall(r'\b\w{5,}\b', goal.lower())  # words with 5+ letters
    keywords = keywords[:2] if keywords else ['run']

    # Sanitize and slugify
    slug = "_".join(keywords)
    slug = re.sub(r'[^a-z0-9_]+', '', slug)

    # Add timestamp and short UUID
    timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H-%M-%S")
    short_uuid = uuid.uuid4().hex[:6]

    return f"{slug}_{timestamp}_{short_uuid}"

def get_log_file_path(run_id:str, cfg: DictConfig) -> str:
    # Get the path to the log file
    if cfg.logging.logger.get("log_file", None):
        print(f"Log file path: {cfg.logging.logger.log_file}")
        return cfg.logging.logger.log_file
    
    timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H-%M-%S")
    safe_run_id = re.sub(r"[\\W_]+", "_", run_id)  # remove/replace unsafe chars
    log_filename = f"{safe_run_id}_{timestamp}.jsonl"
    os.makedirs(cfg.logging.logger.log_path, exist_ok=True)
    log_file_path = os.path.join(cfg.logging.logger.log_path, log_filename)
    print(f"Log file path: {log_file_path}")
    return log_file_path

---END-OF-FILE---


"co_ai\__init__.py"
---START-OF-FILE---
"""
AI Co-Scientist Package

A modular pipeline for hypothesis generation, evaluation, and summarization using DSPy, pgvector, and Ollama.
"""
__version__ = "0.1.0"
---END-OF-FILE---


"co_ai\cli.py"
---START-OF-FILE---
import logging

import click

from co_ai.utils import extract_resources


@click.group()
def cli():
    """AI Co-Scientist CLI"""
    pass


@cli.command()
def init():
    """Initialize config and prompt files from embedded resources"""
    try:
        extract_resources()
        click.echo("[+] Successfully extracted configs and prompts")
    except Exception as e:
        click.echo(f"[-] Failed to extract resources: {e}")
        raise


if __name__ == "__main__":
    cli()---END-OF-FILE---


"co_ai\constants.py"
---START-OF-FILE---
# co_ai/constants.py

# ==== Context Keys ====
AGENT = "Agent"
API_BASE = "api_base"
API_KEY = "api_key"
BATCH_SIZE = "batch_size"
CONTEXT = "context"
DEFAULT = "default"
DETAILS = "details"
EVOLVED = "evolved"
FEEDBACK = "feedback"
FILE = "file"
GOAL = "goal"
GOAL_TYPE = "goal_type"
GOAL_TEXT = "goal_text"
HYPOTHESES = "hypotheses"
DATABASE_MATCHES = "database_matches"
TEXT = "text"
LITERATURE = "literature"
LOOKAHEAD = "lookahead"
MODEL = "model"
NAME = "name"
PIPELINE = "pipeline"
PROXIMITY = "proximity"
RANKING = "ranking"
REFLECTION = "reflection"
REVIEW = "review"
REVIEWS = "reviews"
RUN_ID = "run_id"
PIPELINE_RUN_ID="pipeline_run_id"
SCORE = "score"
SOURCE = "source"
STAGE = "stage"
STRATEGY = "strategy"
VERSION = "version"

# ==== Config Keys ====
DATABASE = "database"
EVENT = "event"
INPUT_KEY = "input_key"
OUTPUT_KEY = "output_key"
PROMPT_DIR = "prompt_dir"
PROMPT_FILE = "prompt_file"
PROMPT_MATCH_RE = "prompt_match_re"
PROMPT_MODE = "prompt_mode"
PROMPT_PATH = "prompt_path"
SAVE_CONTEXT = "save_context"
SAVE_PROMPT = "save_prompt"
SKIP_IF_COMPLETED = "skip_if_completed"
---END-OF-FILE---


"co_ai\main.py"
---START-OF-FILE---
# main.py
import asyncio
import json
import logging
from datetime import datetime

import hydra
import yaml
from omegaconf import DictConfig, OmegaConf

from co_ai.logs import JSONLogger
from co_ai.memory import MemoryTool
from co_ai.supervisor import Supervisor
from co_ai.utils import generate_run_id, get_log_file_path


@hydra.main(version_base=None, config_path="../config", config_name="config")
def run(cfg: DictConfig):
    async def main():
        print(f"Initial Config:\n{OmegaConf.to_yaml(cfg)}")

        # Setup logger and memory
        run_id = generate_run_id(cfg.goal.goal_text if "goal" in cfg else "batch")
        log_path = get_log_file_path(run_id, cfg)
        logger = JSONLogger(log_path=log_path)
        memory = MemoryTool(cfg=cfg.db, logger=logger)

        supervisor = Supervisor(cfg=cfg, memory=memory, logger=logger)

        # âœ… Batch Mode: input_file provided
        if "input_file" in cfg and cfg.input_file:
            print(f"ðŸ“‚ Batch mode: Loading from file: {cfg.input_file}")
            result = await supervisor.run_pipeline_config({"input_file": cfg.input_file})
            print(f"âœ… Batch run completed for file: {cfg.input_file}: {str(result)[:100]}")
            return

        # âœ… Single goal mode
        print(f"ðŸŸ¢ Running pipeline with run_id={run_id}")
        print(f"ðŸ§  Goal: {cfg.goal}")
        print(f"ðŸ“ Config source: {str(cfg)[:100]}...")

        goal = OmegaConf.to_container(cfg.goal, resolve=True)
        context = {
            "goal": goal,
            "run_id": run_id,
        }

        result = await supervisor.run_pipeline_config(context)

        save_json_result(log_path, result)

        if cfg.report.generate_report:
            supervisor.generate_report(result, run_id=run_id)

    asyncio.run(main())


def save_yaml_result(log_path: str, result: dict):
    report_path = log_path.replace(".jsonl", ".yaml")
    with open(report_path, "w", encoding="utf-8") as f:
        yaml.dump(result, f, allow_unicode=True, sort_keys=False)
    print(f"âœ… Result saved to: {report_path}")

def default_serializer(obj):
    if isinstance(obj, datetime):
        return obj.isoformat()
    raise TypeError(f"Type {type(obj)} not serializable")

def save_json_result(log_path: str, result: dict):
    report_path = log_path.replace(".jsonl", "_report.json")
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2, default=default_serializer)
    print(f"âœ… JSON result saved to: {report_path}")

if __name__ == "__main__":
    # Suppress HTTPX logs
    logging.getLogger("httpx").setLevel(logging.WARNING)
    # Suppress LiteLLM logs
    logging.getLogger("LiteLLM").setLevel(logging.WARNING)

    run()
---END-OF-FILE---


"co_ai\supervisor.py"
---START-OF-FILE---
# co_ai/supervisor.py

import json
import os
from datetime import datetime, timezone
from uuid import uuid4

import hydra
from omegaconf import DictConfig, OmegaConf

from co_ai.constants import (GOAL, NAME, PIPELINE, PIPELINE_RUN_ID, PROMPT_DIR,
                             RUN_ID, SAVE_CONTEXT, SKIP_IF_COMPLETED, STAGE)
from co_ai.logs.json_logger import JSONLogger
from co_ai.memory import MemoryTool
from co_ai.reports import ReportFormatter
from co_ai.rules.symbolic_rule_applier import SymbolicRuleApplier


class PipelineStage:
    def __init__(self, name: str, config: dict, stage_dict: dict):
        self.name = name
        self.cls = config.get("cls", "")
        self.enabled = config.get("enabled", True)
        self.iterations = config.get("iterations", 1)
        self.stage_dict = stage_dict


class Supervisor:
    def __init__(self, cfg, memory=None, logger=None):
        self.cfg = cfg
        self.memory = memory or MemoryTool(cfg=cfg.db, logger=logger)
        self.logger = logger or JSONLogger(log_path=cfg.logger.log_path)
        self.logger.log("SupervisorInit", {"cfg": cfg})
        self.rule_applier = SymbolicRuleApplier(cfg, self.memory, self.logger)
        # Parse pipeline stages from config
        self.pipeline_stages = self._parse_pipeline_stages(cfg.pipeline.stages)

    def _parse_pipeline_stages(
        self, stage_configs: list[dict[str, any]]
    ) -> list[PipelineStage]:
        """Parse and validate pipeline stages from config."""
        stages = []
        for stage_config in stage_configs:
            name = stage_config.name
            if not stage_config.enabled:
                print(f"Skipping disabled stage: {name}")
                continue
            stage_dict = self.cfg.agents[name]
            self.logger.log("StageContext", {"stage_dict": stage_dict})
            stages.append(PipelineStage(name, stage_config, stage_dict))
        return stages

    async def run_pipeline_config(self, input_data: dict) -> dict:
        """
        Run all stages defined in config.
        Each stage loads its class dynamically via hydra.utils.get_class()
        """
        self.logger.log("PipelineStart", input_data)
        input_file = input_data.get("input_file", self.cfg.get("input_file", None))

        if input_file and os.path.exists(input_file):
            self.logger.log("BatchProcessingStart", {"file": input_file})
            with open(input_file, "r", encoding="utf-8") as f:
                for i, line in enumerate(f):
                    goal_dict = json.loads(line)
                    goal_orm = self.memory.goals.get_or_create(goal_dict)

                    run_id = goal_orm.get("id", f"goal_{i}")
                    context = {
                        GOAL: goal_dict,
                        RUN_ID: run_id,
                        "prompt_dir": self.cfg.paths.prompts,
                        PIPELINE: [stage.name for stage in self.pipeline_stages],
                    }
                    try:
                        await self._run_pipeline_stages(context)
                    except Exception as e:
                        self.logger.log(
                            "BatchItemFailed",
                            {"index": i, "run_id": run_id, "error": str(e)},
                        )
            self.logger.log("BatchProcessingComplete", {"file": input_file})
            return {"status": "completed_batch", "input_file": input_file}

        goal_dict = self.get_goal(input_data)
        run_id = str(uuid4())
        pipeline_list = [stage.name for stage in self.pipeline_stages]

        context = input_data.copy()
        context.update(
            {
                RUN_ID: run_id,
                PIPELINE: pipeline_list,
                PROMPT_DIR: self.cfg.paths.prompts,
                "goal": goal_dict,
            }
        )

        # Create and store PipelineRun
        pipeline_run_data = {
            "run_id": run_id,
            "pipeline": pipeline_list,  # Should be list of strings like ["generation", "judge"]
            "strategy": context.get("strategy"),
            "model_name": self.cfg.get("model.name", "unknown"),
            "run_config": OmegaConf.to_container(self.cfg),
            "created_at": datetime.now(timezone.utc).isoformat(),
        }

        # Insert into DB
        run_id = self.memory.pipeline_runs.insert(pipeline_run_data)
        context[PIPELINE_RUN_ID] = run_id

        # Now allow lookahead or other steps to adjust context
        context = await self.maybe_adjust_pipeline(context)
        context = self.rule_applier.apply(context)
        return await self._run_pipeline_stages(context)

    def _parse_pipeline_stages_from_list(
        self, stage_names: list[str]
    ) -> list[PipelineStage]:
        return [
            PipelineStage(name, self.cfg.pipeline.stages[name], self.cfg.agents[name])
            for name in stage_names
            if name in self.cfg.agents
        ]

    async def _run_pipeline_stages(self, context: dict) -> dict:
        for stage in self.pipeline_stages:
            if not stage.enabled:
                self.logger.log(
                    "PipelineStageSkipped",
                    {STAGE: stage.name, "reason": "disabled_in_config"},
                )
                continue

            cls = hydra.utils.get_class(stage.cls)
            stage_dict = OmegaConf.to_container(stage.stage_dict, resolve=True)
            self.rule_applier.apply_to_agent(stage_dict, context)

            saved_context = self.load_context(stage_dict, run_id=context.get(RUN_ID))
            if saved_context:
                self.logger.log(
                    "PipelineStageSkipped",
                    {STAGE: stage.name, "reason": "context_loaded"},
                )
                context = {**context, **saved_context}
                continue

            agent = cls(cfg=stage_dict, memory=self.memory, logger=self.logger)

            self.logger.log("PipelineStageStart", {STAGE: stage.name})

            for i in range(stage.iterations):
                self.logger.log(
                    "PipelineIterationStart", {STAGE: stage.name, "iteration": i + 1}
                )
                context = await agent.run(context)
                self.rule_applier.track_pipeline_stage(stage_dict, context)
                self.logger.log(
                    "PipelineIterationEnd", {STAGE: stage.name, "iteration": i + 1}
                )

            self.save_context(stage_dict, context)
            self.logger.log("PipelineStageEnd", {STAGE: stage.name})
            self.logger.log(
                "ContextAfterStage",
                {STAGE: stage.name, "context_keys": list(context.keys())},
            )

            # After final stage
            if self.cfg.get("post_judgment", {}).get("enabled", False):
                judge_cfg = OmegaConf.to_container(self.cfg.post_judgment, resolve=True)
                stage_dict = OmegaConf.to_container(
                    self.cfg.agents.pipeline_judge, resolve=True
                )
                judge_cls = hydra.utils.get_class(judge_cfg["cls"])
                judge_agent = judge_cls(
                    cfg=stage_dict, memory=self.memory, logger=self.logger
                )
                context = await judge_agent.run(context)
                self.rule_applier.track_pipeline_stage(stage_dict, context)

        return context

    def generate_report(self, context: dict[str, any], run_id: str) -> str:
        """Generate a report based on the pipeline context."""
        formatter = ReportFormatter(self.cfg.report.path)
        report = formatter.format_report(context)
        # self.memory.report.log(
        #     run_id, str(context.get("goal")), report, self.cfg.report.path
        # )
        self.logger.log(
            "ReportGenerated", {RUN_ID: run_id, "report_snippet": report[:100]}
        )
        return report

    def save_context(self, cfg: DictConfig, context: dict):
        if self.memory and cfg.get(SAVE_CONTEXT, False):
            run_id = context.get(RUN_ID)
            name = cfg.get(NAME, "NoAgentNameInConfig")
            self.memory.context.save(run_id, name, context, cfg)
            self.logger.log(
                "ContextSaved",
                {NAME: name, RUN_ID: run_id, "context_keys": list(context.keys())},
            )

    def load_context(self, cfg: DictConfig, run_id: str):
        if self.memory and cfg.get(SKIP_IF_COMPLETED, False):
            name = cfg.get(NAME, None)
            if name and self.memory.context.has_completed(run_id, name):
                saved_context = self.memory.context.load(run_id, name)
                if saved_context:
                    self.logger.log("ContextLoaded", {RUN_ID: run_id, NAME: name})
                    return saved_context
        return None

    async def maybe_adjust_pipeline(self, context: dict) -> dict:
        """
        Optionally run DOTSPlanner and/or LookaheadAgent to revise or select the pipeline.
        """
        goal = context.get("goal", {})

        # === RUN DOTS PLANNER FIRST (STRATEGY PLANNER) ===
        if self.cfg.get("planner", {}).get("enabled", False):
            try:
                planner_cfg = OmegaConf.to_container(self.cfg.planner, resolve=True)
                planner_cls = hydra.utils.get_class(planner_cfg["cls"])
                planner = planner_cls(
                    cfg=planner_cfg, memory=self.memory, logger=self.logger
                )

                context = await planner.run(context)

                if "suggested_pipeline" in context:
                    suggested = context["suggested_pipeline"]
                    self.logger.log(
                        "PipelineUpdatedByDOTSPlanner",
                        {
                            "strategy": context.get("strategy", "unknown"),
                            "suggested": suggested,
                        },
                    )
                    self.pipeline_stages = self._parse_pipeline_stages_from_list(
                        suggested
                    )
            except Exception as e:
                self.logger.log("DOTSPlannerFailed", {"error": str(e)})

        # === RUN LOOKAHEAD SECOND (OPTIONAL REFLECTIVE OVERRIDE) ===
        if not self.cfg.get("dynamic", {}).get("lookahead_enabled", False):
            return context

        try:
            lookahead_cfg = OmegaConf.to_container(self.cfg.dynamic, resolve=True)
            stage_dict = OmegaConf.to_container(self.cfg.agents.lookahead, resolve=True)
            stage_dict = self.rule_applier.apply_to_agent(stage_dict, context)
            agent_cls = hydra.utils.get_class(lookahead_cfg["cls"])
            lookahead_agent = agent_cls(
                cfg=stage_dict, memory=self.memory, logger=self.logger
            )

            self.logger.log("LookaheadStart", {"goal": goal})
            context[PIPELINE] = [stage.name for stage in self.pipeline_stages]
            context["agent_registry"] = OmegaConf.to_container(
                OmegaConf.load("config/agent_registry.yaml")["agents"]
            )
            updated_context = await lookahead_agent.run(context)

            if "suggested_pipeline" in updated_context:
                suggested = updated_context["suggested_pipeline"]
                self.logger.log(
                    "PipelineUpdatedByLookahead",
                    {
                        "original": [stage.name for stage in self.pipeline_stages],
                        "suggested": suggested,
                    },
                )
                self.pipeline_stages = self._parse_pipeline_stages_from_list(suggested)
            return updated_context

        except Exception as e:
            self.logger.log("LookaheadFailed", {"error": str(e)})
            return context

    async def rerun_pipeline(self, run_id: str) -> dict:
        """
        Re-run a previously stored pipeline run by its run_id.
        """
        self.logger.log("PipelineRerunStart", {"run_id": run_id})

        # Step 1: Load pipeline run
        pipeline_run = self.memory.pipeline_runs.get_by_run_id(run_id)
        if not pipeline_run:
            raise ValueError(f"No pipeline run found with run_id={run_id}")

        # Step 2: Load goal object
        goal = self.memory.goals.get_by_id(pipeline_run.goal_id)
        if not goal:
            raise ValueError(f"No goal found with goal_id={pipeline_run.goal_id}")

        # Step 3: Build context
        context = {
            "goal": goal,
            RUN_ID: run_id,
            PIPELINE: pipeline_run.pipeline,
            "strategy": pipeline_run.strategy,
            "model_config": pipeline_run.run_config,
            PROMPT_DIR: self.cfg.paths.prompts,
        }

        # Optional: override pipeline stages to match recorded run
        self.pipeline_stages = self._parse_pipeline_stages_from_list(
            pipeline_run.pipeline
        )

        # Optional: reapply lookahead suggestion or symbolic context (or skip it for pure repeatability)
        # context["lookahead"] = pipeline_run.lookahead_context
        # context["symbolic_suggestion"] = pipeline_run.symbolic_suggestion

        # Step 4: Run
        context = await self._run_pipeline_stages(context)

        # Step 5: Generate report (optional)
        self.generate_report(context, run_id)

        self.logger.log("PipelineRerunComplete", {"run_id": run_id})
        return context

    def analyze_pipeline_deltas(self, goal_id: int):
        from co_ai.analysis.reflection_delta import compare_pipeline_runs

        deltas = compare_pipeline_runs(self.memory, goal_id)
        for delta in deltas:
            self.logger.log("ReflectionDeltaComputed", delta)

    def get_goal(self, input_data: dict) -> dict:
        goal_dict = input_data.get("goal")
        if not goal_dict:
            raise ValueError("Missing 'goal' key in input_data")
        goal_orm = self.memory.goals.get_or_create(goal_dict)

        merged = goal_orm.to_dict()
        for key, value in goal_dict.items():
            if value is not None:
                if key in merged and merged[key] != value:
                    self.logger.log("GoalContextOverride", {
                        "field": key,
                        "original": merged[key],
                        "override": value,
                        "note": "Overriding goal field from context"
                    })
                merged[key] = value
        return merged
---END-OF-FILE---



# configs/agents/adaptive_reasoner.yaml

adaptive_reasoner:
  name: adaptive_reasoner
  enabled: true

  mode: adaptive  # "adaptive", "instruction_guided", "consensus_guided"

  format_list:
    - direct
    - short_cot
    - code
    - long_cot

  format_priority_by_difficulty:
    easy:
      - direct
      - short_cot
      - code
      - long_cot
    medium:
      - short_cot
      - code
      - long_cot
      - direct
    hard:
      - long_cot
      - code
      - short_cot
      - direct
    default:
      - short_cot
      - long_cot
      - code
      - direct

  save_prompt: true
  save_context: false
  skip_if_completed: false

  evaluator: arm         #(mrq or llm)  may not be enough items fo mrq
  evaluator_prompt_file: evaluation.txt
  evaluator_model:
    name: ollama/phi3
    api_base: http://localhost:11434
    api_key: null

  analysis_model:
    name: ollama/llama3.2
    api_base: http://localhost:11434
    api_key: null

  model:
    name: ollama/qwen3
    api_base: http://localhost:11434
    api_key: null

  input_key: hypotheses   # add nodes
  output_key: arm
  prompt_mode: file
  prompt_file: generate_cot.txt
  pattern_prompt_file: cot_pattern.txt
  strategy:

  remove_think: false # we require "thinking" part of the prompt

  device: cpu # How many epochs to wait after no improvement
  limit: 1000 # max training data
  epochs: 20  # how much to train
  patience: 3  # How many epochs to wait after no improvement
  min_delta: 0.0001  # Minimum change in loss to qualify as improvement
  log_results: true # save results to database
  save_improved: true # save improved prompts and hypotheses

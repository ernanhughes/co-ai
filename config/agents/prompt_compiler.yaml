# config/agents/prompt_compiler.yaml

prompt_compiler:
  name: prompt_compiler
  enabled: true
  save_context: false
  skip_if_completed: false

  # Number of sample prompts to use in tuning
  sample_size: 20
  # How many prompts to generate once tuned
  generate_count: 5

  use_memory_for_fast_prompts: false
  use_strategy_mutation: true

  model:
    name: ollama_chat/qwen3
    api_base: http://localhost:11434
    api_key: null

  judge: mrq
  judge_prompt_file: evaluator.txt
  judge_model:
    name: ollama_chat/qwen3
    api_base: http://localhost:11434
    api_key: null

  strategy: dspy_compilation

  # Required keys for the agent ot effectively process the context
  required_keys: ["goal", "hypotheses"]   # add nodes
  # Key that it will iterate over ot generate reflections
  input_key: "prompts"
  # Key that it store the results of those reflections
  output_key: "tuned_prompts"  # change

  save_prompt: true
  prompt_mode: file
  prompt_file: compile_prompt.txt

  # A set of keys that will tune the prompts operation
  preferences:
    - goal_consistency
    - factual
    - reliable_source
    - simplicity
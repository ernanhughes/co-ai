{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"co_ai: Collaborative AI Hypothesis Engine","text":"<p>Welcome to the documentation for co_ai, a modular LLM-powered framework designed to assist in scientific hypothesis generation, evaluation, and refinement. This project is inspired by the SAGE architecture proposed in arXiv:2502.18864 and aims to simulate a collaborative AI research team.</p>"},{"location":"#what-is-co_ai","title":"\ud83d\udd0d What is <code>co_ai</code>?","text":"<p><code>co_ai</code> is an extensible agent-based pipeline framework built around a central Supervisor and a suite of intelligent agents. Each agent performs a distinct role \u2014 such as generating hypotheses, ranking them, reflecting on their quality, or evolving better ones \u2014 all while sharing state through a common memory and logging system.</p> <p>The system is designed to:</p> <ul> <li>Generate high-quality hypotheses using goal-driven prompts</li> <li>Evaluate and refine outputs using ranked feedback and few-shot learning</li> <li>Tune itself over time using embedded prompt evaluations</li> <li>Persist context and decisions for future runs</li> </ul>"},{"location":"#key-features","title":"\ud83e\udde0 Key Features","text":"<ul> <li>\ud83e\udde9 Modular agent architecture (Generation, Ranking, Reflection, Evolution)</li> <li>\ud83e\udde0 Vector memory store powered by PostgreSQL + pgvector</li> <li>\ud83d\udcc2 Context preservation across agents via memory tools</li> <li>\ud83d\udcdc Prompt tuning via DSPy or Ollama-based evaluations</li> <li>\u2699\ufe0f Hydra configuration system for flexible runtime setups</li> <li>\ud83d\udcc8 Logging with structured JSONL + emoji-tagged stages</li> </ul>"},{"location":"#example-use-case","title":"\ud83d\ude80 Example Use Case","text":"<p>You define a research goal (e.g., \"The USA is on the verge of defaulting on its debt\"). <code>co_ai</code> spins up a pipeline to:</p> <ol> <li>Generate multiple hypotheses</li> <li>Reflect on their quality</li> <li>Rank and evolve them using internal feedback</li> <li>Store results, logs, prompts, and evaluations</li> <li>Optionally tune the prompts used in the process for the next iteration</li> </ol> <p>Everything is modular and can be extended with custom agents, tools, and storage plugins.</p>"},{"location":"#project-structure","title":"\ud83d\udce6 Project Structure","text":"<pre><code>co_ai/\n\u251c\u2500\u2500 agents/           # Agent classes (generation, reflection, etc.)\n\u251c\u2500\u2500 memory/           # Memory and store definitions\n\u251c\u2500\u2500 logs/             # Structured logging system\n\u251c\u2500\u2500 tuning/           # Prompt tuning tools\n\u251c\u2500\u2500 tools/            # External API utilities (e.g., web search)\n\u251c\u2500\u2500 main.py           # Entry point\n\u2514\u2500\u2500 supervisor.py     # Pipeline orchestration\nconfig/\nprompts/\n\n\n````\n\n---\n\n## \ud83d\udd17 Resources\n\n* [GitHub Repository](https://github.com/ernanhughes/co-ai)\n* [The SAGE Paper (arXiv)](https://arxiv.org/abs/2502.18864)\n* [Prompt Tuning Overview](prompt_tuning.md)\n* [Configuration Guide](configuration.md)\n\n---\n\n## \ud83d\udc68\u200d\ud83d\udd2c Why Use This?\n\n`co_ai` isn\u2019t just another LLM wrapper \u2014 it\u2019s a framework designed to **amplify human creativity and reasoning** through a configurable, extensible AI assistant team. Whether you're testing theories, validating hypotheses, or generating structured research output, `co_ai` turns prompts into pipelines, and pipelines into progress.\n\n</code></pre>"},{"location":"agents/","title":"\ud83e\udde0 Agents Overview","text":"<p><code>co_ai</code> is a modular, agent-based framework inspired by the SAGE architecture. Each agent is a specialized module that performs a distinct role in the scientific hypothesis lifecycle.</p>"},{"location":"agents/#architecture-overview","title":"\ud83d\uddc2\ufe0f Architecture Overview","text":"<p>Each agent inherits from a <code>BaseAgent</code> class and is configured via Hydra. Agents are loaded dynamically based on your pipeline configuration.</p> <pre><code>pipeline:\n  - cls: co_ai.agents.generation.GenerationAgent\n    ...\n  - cls: co_ai.agents.review.ReviewAgent\n    ...\n````\n\n---\n\n## \ud83d\udd0d Agent Descriptions\n\n### \ud83e\uddea GenerationAgent\n\n**Role:** Produces an initial batch of hypotheses from a given research goal.\n\n**Inputs:**\n\n* `goal`\n\n**Outputs:**\n\n* `hypotheses` (list of hypothesis strings)\n\n**Configurable:**\n\n* Prompt template\n* Number of hypotheses\n* Strategy (`goal-aligned`, `wild`, etc.)\n\n---\n\n### \ud83e\uddd1\u200d\u2696\ufe0f ReviewAgent\n\n**Role:** Reviews and rates hypotheses using criteria like novelty, feasibility, and correctness.\n\n**Inputs:**\n\n* `hypotheses`\n* `preferences` (e.g., `[\"factual\", \"simple\", \"reliable_source\"]`)\n\n**Outputs:**\n\n* `reviews` (dict of hypothesis \u2192 review)\n\n---\n\n### \ud83c\udfc6 RankingAgent\n\n**Role:** Ranks hypotheses using a tournament-style evaluation process.\n\n**Inputs:**\n\n* `reviews`\n* `hypotheses`\n\n**Outputs:**\n\n* `ranked` (list of `(hypothesis, score)`)\n\n---\n\n### \ud83e\udd14 ReflectionAgent\n\n**Role:** Reflects on hypotheses in the context of the goal. Identifies gaps or misalignments.\n\n**Inputs:**\n\n* `hypotheses`\n* `goal`\n* `preferences`\n\n**Outputs:**\n\n* `reflections` (markdown summaries)\n\n---\n\n### \ud83e\uddec EvolutionAgent\n\n**Role:** Refines, combines, or mutates top-ranked hypotheses to improve clarity, novelty, and testability.\n\n**Inputs:**\n\n* `ranked`\n* `goal`\n\n**Outputs:**\n\n* `evolved` (list of new hypotheses)\n\n---\n\n### \ud83d\uddfa\ufe0f ProximityAgent\n\n**Role:** Computes similarity between current hypotheses and past ones to detect clustering or redundancy.\n\n**Inputs:**\n\n* `hypotheses`\n* `goal`\n\n**Outputs:**\n\n* `proximity_graph`, `graft_candidates`, `clusters`\n\n---\n\n### \ud83d\udcc8 MetaReviewAgent *(optional)*\n\n**Role:** Provides a final high-level summary and critique of hypothesis quality and coverage.\n\n---\n\n### \ud83e\uddea PromptTuningAgent *(experimental)*\n\n**Role:** Uses generated hypotheses + reviews to tune and evaluate prompt templates post-run.\n\n**Inputs:**\n\n* `goal`\n* `prompts` (retrieved from DB)\n\n**Outputs:**\n\n* Updated prompt evaluations\n\n---\n\n## \u2699\ufe0f Custom Agents\n\nYou can add your own agent by inheriting from `BaseAgent`:\n\n```python\nclass MyCustomAgent(BaseAgent):\n    async def run(self, context: dict) -&gt; dict:\n        ...\n        return context\n</code></pre> <p>Add your agent to <code>config/pipeline.yaml</code> and you're good to go.</p>"},{"location":"agents/#notes","title":"\ud83d\udcda Notes","text":"<ul> <li>Each agent automatically receives <code>logger</code>, <code>memory</code>, and <code>cfg</code>.</li> <li>Logs are structured per stage and can be filtered or visualized later.</li> <li>Agents can store output to database (hypotheses, reviews, prompt logs, etc.)</li> </ul>"},{"location":"config/","title":"\u2699\ufe0f Configuration","text":"<p><code>co_ai</code> uses Hydra for flexible and hierarchical configuration. Each agent, tool, and subsystem loads settings from a structured YAML config. This allows you to dynamically switch models, pipelines, memory backends, and more.</p>"},{"location":"config/#configuration-structure","title":"\ud83d\uddc2\ufe0f Configuration Structure","text":"<p>The root config lives in <code>config/config.yaml</code> and includes nested paths for:</p> <pre><code>defaults:\n  - db: default\n  - logging: default\n  - report: default\n  - agents:\n      - generation\n      - ranking\n      - review\n      - reflection\n      - evolution\n      - proximity\n      - meta_review\n````\n\n---\n\n## \ud83e\udde0 Memory Configuration (`db.yaml`)\n\nExample for a PostgreSQL database with `pgvector`:\n\n```yaml\nhost: localhost\nport: 5432\ndatabase: co_ai\nuser: postgres\npassword: yourpassword\n\n# Optional store overrides\nextra_stores: []\n</code></pre>"},{"location":"config/#logging-loggingyaml","title":"\ud83d\udcdc Logging (<code>logging.yaml</code>)","text":"<pre><code>logger:\n  log_path: logs/\n  log_file: \"\"\n</code></pre> <ul> <li><code>log_path</code>: Base directory for logs</li> <li><code>log_file</code>: Specific log file; if empty, will auto-generate using timestamp and run ID</li> </ul>"},{"location":"config/#reporting-reportyaml","title":"\ud83d\udcca Reporting (<code>report.yaml</code>)","text":"<pre><code>generate_report: true\nformat: yaml\n</code></pre>"},{"location":"config/#agent-configuration","title":"\ud83e\uddea Agent Configuration","text":"<p>Each agent has its own file under <code>config/agents/</code>. For example:</p>"},{"location":"config/#generation-agent","title":"\u2728 Generation Agent","text":"<pre><code>name: generation\nprompt_type: file\nprompt_file: generation_goal_aligned.txt\ntop_k: 5\n</code></pre>"},{"location":"config/#reflection-agent","title":"\ud83e\udde0 Reflection Agent","text":"<pre><code>name: reflection\nprompt_type: file\nprompt_file: reflect_consistency.txt\npreferences:\n  - goal_consistency\n  - factual\n  - reliable_source\n  - simplicity\n</code></pre>"},{"location":"config/#prompt-tuning-agent","title":"\ud83e\uddec Prompt Tuning Agent","text":"<pre><code>name: prompt_tuning\nmodel: qwen2.5\nnum_prompts: 10\n</code></pre>"},{"location":"config/#prompt-directory-override","title":"\ud83d\udcc1 Prompt Directory Override","text":"<p>You can override the prompt directory from the command line or within the context:</p> <pre><code>PROMPT_DIR: \"prompts\"\n</code></pre>"},{"location":"config/#example-run-with-overrides","title":"\ud83e\uddea Example Run with Overrides","text":"<p>You can override any value from the CLI:</p> <pre><code>python co_ai/main.py goal=\"Can AI assist scientific discovery?\" logging.logger.log_path=run_logs\n</code></pre> <p>Or define a full run config:</p> <pre><code>goal: \"Evaluate LLMs for literature review\"\nrun_id: \"review_run_01\"\n</code></pre>"},{"location":"config/#tips","title":"\ud83d\udcd8 Tips","text":"<ul> <li>Every config is passed to its agent as a flat <code>dict</code></li> <li>Use <code>.get()</code> for optional config keys</li> <li>Log config values using <code>OmegaConf.to_yaml(cfg)</code></li> </ul>"},{"location":"config/#runtime-mutation","title":"\ud83d\udd01 Runtime Mutation","text":"<p>Agents can mutate config or context, but best practice is to treat config as static and context as dynamic.</p>"},{"location":"custom/","title":"Building a Custom AI Pipeline with Minimal Configuration","text":"<p>How to build a custom scientific hypothesis generation pipeline using the <code>co_ai</code> framework with the minimum required configuration. This chapter assumes you\u2019re familiar with Python and basic YAML syntax.</p>"},{"location":"custom/#what-you-will-build","title":"\ud83c\udfaf What You Will Build","text":"<p>By the end of this chapter, you'll have a pipeline that:</p> <ul> <li>Accepts a scientific goal</li> <li>Generates hypotheses using a custom agent</li> <li>Logs and stores the results</li> <li>Is fully driven by a Hydra-based config</li> </ul>"},{"location":"custom/#required-files","title":"\ud83e\uddf1 Required Files","text":"<p>At a minimum, your project must include:</p> <pre><code>\nco\\_ai/\n\u2514\u2500\u2500 main.py                  # Entry point\n\u2514\u2500\u2500 agents/\n\u2514\u2500\u2500 my\\_agent.py        # Your custom agent\nconfig/\n\u2514\u2500\u2500 config.yaml              # Hydra config for the run\n\u2514\u2500\u2500 agent\\_\nprompts/\n\u2514\u2500\u2500 my\\_agent/\n\u2514\u2500\u2500 default.txt        # Prompt template (if used)\n\n````\n\n---\n\n## \ud83d\udd27 1. Minimal `config.yaml`\n\nThis is the root Hydra config. It sets the logging path and the pipeline to use:\n\n```yaml\ndefaults:\n  - pipeline: pipeline\n  - _self_\n\nlogging:\n  logger:\n    log_path: logs/\n\ndb:\n  host: localhost\n  port: 5432\n  user: postgres\n  password: postgres\n  database: co_ai\n\nstages:\n  - name: generate\n    cls: co_ai.agents.my_agent.MyGenerationAgent\n    enabled: true\n    strategy: default\n    prompt_file: default\n    prompt_type: file\n</code></pre> <p>Each stage requires:</p> <ul> <li><code>name</code>: a unique identifier</li> <li><code>cls</code>: the full import path of your agent</li> <li><code>enabled</code>: whether the stage should run</li> <li>any agent-specific config you want to inject (e.g., <code>prompt_type</code>)</li> </ul>"},{"location":"custom/#3-mygenerationagent-your-custom-agent","title":"\ud83e\udde0 3. <code>MyGenerationAgent</code>: Your Custom Agent","text":"<p>Every agent should inherit from <code>BaseAgent</code> and implement a <code>run()</code> method.</p> <pre><code># co_ai/agents/my_generation.py\n\nfrom co_ai.agents.base import BaseAgent\n\nclass MyGenerationAgent(BaseAgent):\n    async def run(self, context: dict) -&gt; dict:\n        prompt = self.prompt_loader.load_prompt(self.cfg, context)\n        result = self.call_llm(prompt)\n        context[\"hello\"] = \"World\"\n        return context\n</code></pre> <p>This agent:</p> <ul> <li>Loads a prompt</li> <li>Calls the language model</li> <li>Extracts a list of hypotheses</li> <li>Stores them in the shared <code>context</code></li> </ul>"},{"location":"custom/#4-prompt-template","title":"\ud83d\udcdc 4. Prompt Template","text":"<p>Place a file at <code>prompts/my_agent/default.txt</code> like:</p> <pre><code>Generate 3 hypotheses for the goal: {{ goal }}\n</code></pre>"},{"location":"custom/#5-run-the-pipeline","title":"\ud83d\ude80 5. Run the Pipeline","text":"<p>Use the <code>main.py</code> launcher provided in the framework:</p> <pre><code>python co_ai/main.py goal=\"The USA may default on its debt\"\n</code></pre> <p>This will:</p> <ul> <li>Create a timestamped log file</li> <li>Initialize memory and logging</li> <li>Execute the pipeline as defined</li> </ul>"},{"location":"custom/#optional-additions","title":"\ud83e\udde9 Optional Additions","text":"<p>You can later add:</p> <ul> <li>More stages (e.g. ranking, reflection)</li> <li>Prompt tuning</li> <li>Web search tools</li> <li>Report generation</li> </ul> <p>Each addition is just a YAML node and a Python class away.</p>"},{"location":"custom/#recap","title":"\u2705 Recap","text":"Component Purpose <code>config.yaml</code> Global settings, DB, logging <code>pipeline.yaml</code> Defines agent stages <code>Agent class</code> Implements agent logic <code>Prompt template</code> Guides model output <code>main.py</code> Launches the configured pipeline <p>By following this chapter, you've built a reusable, testable AI pipeline using <code>co_ai</code> with the bare essentials. From here, you can scale out to review, reflect, evolve, and rank your hypotheses in modular stages.</p>"},{"location":"installation/","title":"\ud83d\udee0\ufe0f Installation Guide","text":"<p>This guide walks you through setting up and running the <code>co_ai</code> framework locally.</p>"},{"location":"installation/#requirements","title":"\ud83d\udce6 Requirements","text":"<ul> <li>Python 3.8+</li> <li>PostgreSQL with pgvector extension</li> <li>Ollama (for local LLM inference)</li> <li>Poetry OR standard <code>pip</code> + <code>venv</code></li> <li>Optional: Docker (for running PostgreSQL locally)</li> </ul>"},{"location":"installation/#1-clone-the-repository","title":"\ud83d\udd27 1. Clone the Repository","text":"<pre><code>git clone https://github.com/ernanhughes/co-ai.git\ncd co-ai\n````\n\n---\n\n## \ud83d\udc0d 2. Create a Virtual Environment\n\n**Using `venv`:**\n\n```bash\npython3 -m venv .venv\nsource .venv/bin/activate   # On Windows: .venv\\Scripts\\activate\n</code></pre> <p>Or with <code>poetry</code>:</p> <pre><code>poetry install\npoetry shell\n</code></pre>"},{"location":"installation/#3-install-dependencies","title":"\ud83d\udcda 3. Install Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre> <p>OR using <code>pyproject.toml</code>:</p> <pre><code>pip install .\n</code></pre>"},{"location":"installation/#4-set-up-postgresql-pgvector","title":"\ud83e\udde0 4. Set Up PostgreSQL + pgvector","text":"<p>Option A: Using Docker</p> <pre><code>docker run --name coai-db -p 5432:5432 \\\n  -e POSTGRES_PASSWORD=postgres \\\n  -e POSTGRES_USER=postgres \\\n  -e POSTGRES_DB=coai \\\n  ankane/pgvector\n</code></pre> <p>Option B: Local Installation</p> <ol> <li> <p>Install PostgreSQL</p> </li> <li> <p>Install <code>pgvector</code>:</p> </li> </ol> <p><code>bash    CREATE EXTENSION vector;</code></p> <ol> <li>Create the database:</li> </ol> <p><code>bash    createdb coai</code></p> <ol> <li>Run the schema:</li> </ol> <p><code>bash    psql -U postgres -d coai -f schema.sql</code></p>"},{"location":"installation/#5-install-run-ollama","title":"\ud83e\udd16 5. Install &amp; Run Ollama","text":"<pre><code>ollama run qwen:latest\n</code></pre> <p>Or for smaller models:</p> <pre><code>ollama run llama2\n</code></pre> <p>Make sure <code>Ollama</code> is running on <code>http://localhost:11434</code>.</p>"},{"location":"installation/#6-run-the-app","title":"\u2699\ufe0f 6. Run the App","text":"<pre><code>python co_ai/main.py goal=\"The USA is on the verge of defaulting on its debt\"\n</code></pre> <p>Or with CLI args:</p> <pre><code>python co_ai/main.py --config-name=config goal=\"My research goal here\"\n</code></pre>"},{"location":"installation/#notes","title":"\ud83d\udcdd Notes","text":"<ul> <li>Logs are stored in <code>logs/</code> as structured JSONL.</li> <li>Prompts are saved in <code>prompts/</code> and tracked in the database.</li> <li>You can inspect all configuration using Hydra or customize each agent via <code>config/</code>.</li> </ul>"},{"location":"installation/#youre-ready","title":"\u2705 You\u2019re Ready!","text":"<p>You now have a full pipeline for running research-style hypothesis generation, evaluation, and prompt tuning.</p>"},{"location":"memory/","title":"\ud83e\udde0 Memory System","text":"<p>The <code>MemoryTool</code> in <code>co_ai</code> serves as the shared memory backbone for the entire pipeline. It manages embeddings, hypotheses, reviews, prompt versions, logs, and more\u2014all stored in a PostgreSQL database with pgvector support.</p>"},{"location":"memory/#core-design","title":"\ud83d\udce6 Core Design","text":"<p>The memory system consists of modular \"stores\", each implementing a specific aspect of persistent storage. These stores all inherit from a <code>BaseStore</code> and are registered into a central <code>MemoryTool</code>.</p>"},{"location":"memory/#memorytool-overview","title":"MemoryTool Overview","text":"<pre><code>memory = MemoryTool(cfg=cfg.db, logger=logger)\n````\n\nIt registers multiple internal stores automatically:\n\n```text\n- embeddings: vector storage and similarity search\n- hypotheses: hypothesis versions, links, metadata\n- context: stores pipeline-level state/context\n- prompts: prompt text, responses, evaluations\n- reports: final YAML or HTML reports\n</code></pre>"},{"location":"memory/#store-breakdown","title":"\ud83d\udcda Store Breakdown","text":""},{"location":"memory/#embeddingstore","title":"\ud83d\udcd0 EmbeddingStore","text":"<p>Stores vector embeddings of hypotheses and enables similarity search via <code>pgvector</code>.</p> <p>Key Methods:</p> <ul> <li><code>get_or_create(text)</code></li> <li><code>similar(text, top_k=5)</code></li> </ul>"},{"location":"memory/#hypothesisstore","title":"\ud83d\udca1 HypothesisStore","text":"<p>Stores hypotheses generated throughout the pipeline, along with evaluations and links to the prompt that created them.</p> <p>Schema includes:</p> <ul> <li><code>text</code></li> <li><code>goal</code></li> <li><code>confidence</code></li> <li><code>features</code> (JSON)</li> <li><code>embedding</code> (vector)</li> <li><code>prompt_id</code> (foreign key)</li> </ul>"},{"location":"memory/#promptstore","title":"\ud83d\udcbe PromptStore","text":"<p>Tracks prompt versions, tuning attempts, evaluations, and strategies used during generation.</p> <p>Key Features:</p> <ul> <li>Save new prompts with <code>save()</code></li> <li>Log evaluation results with <code>store_evaluation()</code></li> <li>Retrieve recent prompts via <code>get_latest_prompts(n)</code></li> </ul>"},{"location":"memory/#contextstore","title":"\ud83d\udce5 ContextStore","text":"<p>Stores and retrieves pipeline context snapshots to allow state recovery or debugging between stages.</p> <p>Stored As:</p> <ul> <li><code>yaml</code></li> <li>Timestamped entries</li> <li>One entry per run ID</li> </ul>"},{"location":"memory/#reportlogger","title":"\ud83d\udcdc ReportLogger","text":"<p>Stores the final pipeline result YAML in the <code>reports</code> table and allows for post-run analysis or export.</p>"},{"location":"memory/#custom-stores","title":"\ud83e\udde9 Custom Stores","text":"<p>You can define and plug in your own store like so:</p> <pre><code>class MyCustomStore(BaseStore):\n    def __init__(self, db, logger=None):\n        self.db = db\n        self.logger = logger\n        self.name = \"my_custom_store\"\n</code></pre> <p>Then register it:</p> <pre><code>memory.register_store(MyCustomStore(db, logger))\n</code></pre>"},{"location":"memory/#database-schema-simplified","title":"\ud83d\uddc4\ufe0f Database Schema (Simplified)","text":"<p>Here are a few tables used:</p> <pre><code>CREATE TABLE embeddings (\n    id SERIAL PRIMARY KEY,\n    text TEXT UNIQUE,\n    embedding VECTOR(1024),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n</code></pre> <pre><code>CREATE TABLE hypotheses (\n    id SERIAL PRIMARY KEY,\n    text TEXT,\n    goal TEXT,\n    confidence FLOAT,\n    prompt_id INT,\n    embedding VECTOR(1024),\n    ...\n);\n</code></pre> <pre><code>CREATE TABLE prompts (\n    id SERIAL PRIMARY KEY,\n    agent_name TEXT,\n    prompt_text TEXT,\n    version INT,\n    is_current BOOLEAN,\n    ...\n);\n</code></pre>"},{"location":"memory/#config-example","title":"\ud83d\udee0 Config Example","text":"<p>Hydra config fragment:</p> <pre><code>db:\n  host: localhost\n  port: 5432\n  database: co_ai\n  user: postgres\n  password: yourpassword\n</code></pre>"},{"location":"memory/#benefits","title":"\u2705 Benefits","text":"<ul> <li>Consistent logging and traceability</li> <li>Unified access to evolving data</li> <li>Easy to extend with new stores</li> <li>Enables historical comparisons and tuning</li> </ul>"},{"location":"paper/","title":"\ud83e\udde0 SAGE Paper vs <code>co_ai</code> Implementation Checkpoint","text":"<p>This page summarizes how the <code>co_ai</code> implementation maps directly to the concepts and structure proposed in the SAGE paper (arXiv:2502.18864).</p>"},{"location":"paper/#goal","title":"\u2705 Goal","text":"<p>Automate and assist scientific hypothesis generation, ranking, refinement, and evaluation using LLMs and a modular agent framework.</p>"},{"location":"paper/#1-pipeline-oriented-architecture","title":"\ud83d\udd39 1. Pipeline-Oriented Architecture","text":"<p>SAGE Paper: Modular pipeline of agents: literature, generation, ranking, reflection, evolution. co_ai:</p> <ul> <li>Modular <code>Supervisor</code> class runs a Hydra-configured pipeline</li> <li>Agents derived from <code>BaseAgent</code></li> <li>Context is passed through pipeline stages</li> <li>Agents dynamically enabled/disabled via config</li> </ul>"},{"location":"paper/#2-literature-driven-hypothesis-generation","title":"\ud83d\udd39 2. Literature-Driven Hypothesis Generation","text":"<p>SAGE Paper: Hypotheses are seeded from retrieved literature. co_ai:</p> <ul> <li><code>LiteratureAgent</code> supports DuckDuckGo or SearxNG (optional)</li> <li>Results are embedded into <code>VectorMemory</code></li> <li>Used to ground hypothesis generation and ranking</li> </ul>"},{"location":"paper/#3-hypothesis-generation","title":"\ud83d\udd39 3. Hypothesis Generation","text":"<p>SAGE Paper: LLMs generate hypotheses based on scientific goals. co_ai:</p> <ul> <li><code>GenerationAgent</code> uses configurable prompts</li> <li>Prompt strategies: static, file, template, tuning</li> <li>Integrates prompt tracking and logging</li> </ul>"},{"location":"paper/#4-ranking-evaluation","title":"\ud83d\udd39 4. Ranking &amp; Evaluation","text":"<p>SAGE Paper: Hypotheses evaluated via pairwise ranking. co_ai:</p> <ul> <li><code>RankingAgent</code> implements tournament-style ranking</li> <li>Stores scores and feedback in memory</li> <li>Supports re-ranking and evaluation review</li> </ul>"},{"location":"paper/#5-reflection-meta-review","title":"\ud83d\udd39 5. Reflection &amp; Meta-Review","text":"<p>SAGE Paper: Reflections assess hypothesis quality and alignment. co_ai:</p> <ul> <li><code>ReflectionAgent</code> uses preferences and goal alignment</li> <li>Outputs markdown-formatted reflections</li> <li>Logs reflections and stores review</li> </ul>"},{"location":"paper/#6-evolution-agent","title":"\ud83d\udd39 6. Evolution Agent","text":"<p>SAGE Paper: Refines hypotheses through transformation. co_ai:</p> <ul> <li><code>EvolutionAgent</code> includes grafting of similar hypotheses</li> <li>Performs simplification and clarification</li> <li>Logs transformation path and stores result</li> </ul>"},{"location":"paper/#7-memory-storage","title":"\ud83d\udd39 7. Memory &amp; Storage","text":"<p>SAGE Paper: Persistent storage of hypotheses and evaluations. co_ai:</p> <ul> <li><code>MemoryTool</code> wraps <code>pgvector</code>-backed PostgreSQL stores</li> <li>Separate stores for hypotheses, prompts, context, evaluations</li> <li>Agents log activity to structured JSONL logs</li> </ul>"},{"location":"paper/#8-prompt-tuning-loop","title":"\ud83d\udd39 8. Prompt Tuning Loop","text":"<p>SAGE Paper: Prompts evolve using feedback from prior runs. co_ai:</p> <ul> <li><code>PromptTuningAgent</code> uses <code>OllamaEvaluator</code></li> <li>Compares modified prompts against originals</li> <li>Saves evaluation results in <code>prompt_evaluations</code> table</li> </ul>"},{"location":"paper/#9-logging-traceability","title":"\ud83d\udd39 9. Logging &amp; Traceability","text":"<p>SAGE Paper: Transparent agent operation and outputs. co_ai:</p> <ul> <li>Emoji-annotated structured logs (JSONL)</li> <li>YAML dumps of context at each stage</li> <li>Unique log files per run for reproducibility</li> </ul>"},{"location":"paper/#10-extensibility-modularity","title":"\ud83d\udd39 10. Extensibility &amp; Modularity","text":"<p>SAGE Paper: Agents and stages should be reusable/extensible. co_ai:</p> <ul> <li>Fully pluggable agent framework</li> <li>Easily extend pipeline with new custom stages</li> <li>Prompts, configs, memory, and evaluators are all replaceable</li> </ul>"},{"location":"paper/#summary","title":"\ud83c\udf89 Summary","text":"<p>The <code>co_ai</code> project faithfully implements the key components of the SAGE framework and enhances it with:</p> <ul> <li>Fine-grained logging</li> <li>Prompt tuning loops</li> <li>Modular memory system</li> <li>CLI-based pipeline control</li> </ul> <p>This makes it suitable for scientific automation, reproducible research workflows, and ongoing improvement through prompt evolution.</p>"},{"location":"tools/","title":"\ud83e\uddf0 Tools","text":"<p><code>co_ai</code> ships with a suite of modular tools that agents can access via injection. These tools wrap shared functionality such as embeddings, prompt loading, logging, search, and evaluation.</p>"},{"location":"tools/#memory-tool","title":"\ud83e\udde0 Memory Tool","text":"<p>The <code>MemoryTool</code> manages pluggable stores, each handling a specific data type:</p>"},{"location":"tools/#built-in-stores","title":"Built-in Stores","text":"Store Description <code>EmbeddingStore</code> Caches and retrieves embeddings (via <code>pgvector</code>) <code>HypothesisStore</code> Stores and retrieves hypotheses per goal <code>ContextStore</code> Persists intermediate context state <code>PromptLogger</code> Logs and versions prompts <code>ReportLogger</code> Saves YAML reports per run <pre><code>memory.hypotheses.store(goal, text, confidence, review, features)\nmemory.context.save(run_id, stage, context_dict)\n````\n\nYou can access any store via:\n\n```python\nmemory.get(\"hypotheses\")\n</code></pre>"},{"location":"tools/#web-search-tool","title":"\ud83d\udd0e Web Search Tool","text":"<p>The <code>WebSearchTool</code> supports simple search using DuckDuckGo or a local instance of SearxNG.</p>"},{"location":"tools/#example","title":"Example","text":"<pre><code>results = await WebSearchTool().search2(\"US debt ceiling history\")\n</code></pre> <p>Each result is returned as a formatted string with title, snippet, and URL.</p> <p>\u26a0\ufe0f DuckDuckGo is rate-limited. SearxNG is recommended for production use.</p>"},{"location":"tools/#prompt-loader","title":"\ud83e\uddea Prompt Loader","text":"<p>The <code>PromptLoader</code> handles loading prompts using four modes:</p> Mode Source <code>file</code> From local prompt templates (<code>.txt</code>) <code>static</code> Hardcoded in YAML <code>tuning</code> Best version from memory tuning <code>template</code> Jinja2 templating with context injection <pre><code>prompt = prompt_loader.load_prompt(cfg, context)\n</code></pre> <p>Prompts are formatted with context values, e.g. <code>{goal}</code>.</p>"},{"location":"tools/#evaluation","title":"\u2696\ufe0f Evaluation","text":"<p>The <code>OllamaEvaluator</code> scores refinements using an LLM (e.g. <code>qwen2.5</code>) running locally:</p> <pre><code>evaluation = evaluator.evaluate(original, proposal)\nprint(evaluation.score, evaluation.reason)\n</code></pre> <p>Used for tuning prompts and comparing generated text quality.</p>"},{"location":"tools/#template-utilities","title":"\ud83d\udcda Template Utilities","text":"<p>Prompt templates live under <code>prompts/&lt;agent&gt;/filename.txt</code>.</p> <p>You can render them using:</p> <pre><code>from jinja2 import Template\n\nTemplate(template_text).render(**context)\n</code></pre>"},{"location":"tools/#embedding-tool","title":"\ud83d\udd01 Embedding Tool","text":"<p>The <code>get_embedding(text, cfg)</code> helper uses the configured embedding model and caches results in the database.</p>"},{"location":"tools/#json-logger","title":"\ud83d\udcdd JSON Logger","text":"<p>Structured logging for every event in the pipeline:</p> <pre><code>logger.log(\"HypothesisStored\", {\"goal\": goal, \"text\": text[:100]})\n</code></pre> <p>Each log is saved as a <code>.jsonl</code> file per run.</p>"},{"location":"tools/#pluggable-stores","title":"\ud83d\udd27 Pluggable Stores","text":"<p>You can add custom stores via config:</p> <pre><code>extra_stores:\n  - co_ai.memory.MyCustomStore\n</code></pre> <p>Register them via:</p> <pre><code>memory.register_store(MyCustomStore(...))\n</code></pre>"},{"location":"tools/#adding-your-own-tools","title":"\ud83e\udde9 Adding Your Own Tools","text":"<p>You can pass any tool to agents by extending the agent constructor and updating the supervisor.</p>"}]}